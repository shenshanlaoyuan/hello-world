<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[如何自己设计一个类似Dubbo的RPC框架？]]></title>
      <url>%2F2020%2F06%2F06%2F%E9%9D%A2%E8%AF%95%2F2020-6-6-%E5%A6%82%E4%BD%95%E8%87%AA%E5%B7%B1%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E7%B1%BB%E4%BC%BCDubbo%E7%9A%84RPC%E6%A1%86%E6%9E%B6%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何自己设计一个类似 Dubbo 的 RPC 框架？ 面试官心理分析说实话，就这问题，其实就跟问你如何自己设计一个 MQ 一样的道理，就考两个： 你有没有对某个 rpc 框架原理有非常深入的理解。 你能不能从整体上来思考一下，如何设计一个 rpc 框架，考考你的系统设计能力。 转载请注明出处：http://shenshanlaoyuan.com/2020/06/06/面试/2020-6-6-如何自己设计一个类似Dubbo的RPC框架？/访问原文「如何自己设计一个类似Dubbo的RPC框架？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析其实问到你这问题，你起码不能认怂，因为是知识的扫盲，那我不可能给你深入讲解什么 kafka 源码剖析，dubbo 源码剖析，何况我就算讲了，你要真的消化理解和吸收，起码个把月以后了。 所以我给大家一个建议，遇到这类问题，起码从你了解的类似框架的原理入手，自己说说参照 dubbo 的原理，你来设计一下，举个例子，dubbo 不是有那么多分层么？而且每个分层是干啥的，你大概是不是知道？那就按照这个思路大致说一下吧，起码你不能懵逼，要比那些上来就懵，啥也说不出来的人要好一些。 举个栗子，我给大家说个最简单的回答思路： 上来你的服务就得去注册中心注册吧，你是不是得有个注册中心，保留各个服务的信息，可以用 zookeeper 来做，对吧。 然后你的消费者需要去注册中心拿对应的服务信息吧，对吧，而且每个服务可能会存在于多台机器上。 接着你就该发起一次请求了，咋发起？当然是基于动态代理了，你面向接口获取到一个动态代理，这个动态代理就是接口在本地的一个代理，然后这个代理会找到服务对应的机器地址。 然后找哪个机器发送请求？那肯定得有个负载均衡算法了，比如最简单的可以随机轮询是不是。 接着找到一台机器，就可以跟它发送请求了，第一个问题咋发送？你可以说用 netty 了，nio 方式；第二个问题发送啥格式数据？你可以说用 hessian 序列化协议了，或者是别的，对吧。然后请求过去了。 服务器那边一样的，需要针对你自己的服务生成一个动态代理，监听某个网络端口了，然后代理你本地的服务代码。接收到请求的时候，就调用对应的服务代码，对吧。 这就是一个最最基本的 rpc 框架的思路，先不说你有多牛逼的技术功底，哪怕这个最简单的思路你先给出来行不行？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式服务接口请求的顺序性如何保证？]]></title>
      <url>%2F2020%2F06%2F05%2F%E9%9D%A2%E8%AF%95%2F2020-6-5-%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E8%AF%B7%E6%B1%82%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%80%A7%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题分布式服务接口请求的顺序性如何保证？ 面试官心理分析其实分布式系统接口的调用顺序，也是个问题，一般来说是不用保证顺序的。但是有时候可能确实是需要严格的顺序保证。给大家举个例子，你服务 A 调用服务 B，先插入再删除。好，结果俩请求过去了，落在不同机器上，可能插入请求因为某些原因执行慢了一些，导致删除请求先执行了，此时因为没数据所以啥效果也没有；结果这个时候插入请求过来了，好，数据插入进去了，那就尴尬了。 本来应该是 “先插入 -&gt; 再删除”，这条数据应该没了，结果现在 “先删除 -&gt; 再插入”，数据还存在，最后你死都想不明白是怎么回事。 所以这都是分布式系统一些很常见的问题。 转载请注明出处：http://shenshanlaoyuan.com/2020/06/05/面试/2020-6-5-分布式服务接口请求的顺序性如何保证？/访问原文「分布式服务接口请求的顺序性如何保证？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析首先，一般来说，个人建议是，你们从业务逻辑上设计的这个系统最好是不需要这种顺序性的保证，因为一旦引入顺序性保障，比如使用分布式锁，会导致系统复杂度上升，而且会带来效率低下，热点数据压力过大等问题。 下面我给个我们用过的方案吧，简单来说，首先你得用 dubbo 的一致性 hash 负载均衡策略，将比如某一个订单 id 对应的请求都给分发到某个机器上去，接着就是在那个机器上，因为可能还是多线程并发执行的，你可能得立即将某个订单 id 对应的请求扔一个内存队列里去，强制排队，这样来确保他们的顺序性。 但是这样引发的后续问题就很多，比如说要是某个订单对应的请求特别多，造成某台机器成热点怎么办？解决这些问题又要开启后续一连串的复杂技术方案……曾经这类问题弄的我们头疼不已，所以，还是建议什么呢？ 最好是比如说刚才那种，一个订单的插入和删除操作，能不能合并成一个操作，就是一个删除，或者是其它什么，避免这种问题的产生。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式服务接口的幂等性如何设计？]]></title>
      <url>%2F2020%2F06%2F04%2F%E9%9D%A2%E8%AF%95%2F2020-6-4-%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%B9%82%E7%AD%89%E6%80%A7%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题分布式服务接口的幂等性如何设计（比如不能重复扣款）？ 面试官心理分析从这个问题开始，面试官就已经进入了实际的生产问题的面试了。 一个分布式系统中的某个接口，该如何保证幂等性？这个事儿其实是你做分布式系统的时候必须要考虑的一个生产环境的技术问题。啥意思呢？ 你看，假如你有个服务提供一些接口供外部调用，这个服务部署在了 5 台机器上，接着有个接口就是付款接口。然后人家用户在前端上操作的时候，不知道为啥，总之就是一个订单不小心发起了两次支付请求，然后这俩请求分散在了这个服务部署的不同的机器上，好了，结果一个订单扣款扣两次。 或者是订单系统调用支付系统进行支付，结果不小心因为网络超时了，然后订单系统走了前面我们看到的那个重试机制，咔嚓给你重试了一把，好，支付系统收到一个支付请求两次，而且因为负载均衡算法落在了不同的机器上，尴尬了。。。 所以你肯定得知道这事儿，否则你做出来的分布式系统恐怕容易埋坑。 转载请注明出处：http://shenshanlaoyuan.com/2020/06/04/面试/2020-6-4-分布式服务接口的幂等性如何设计？/访问原文「分布式服务接口的幂等性如何设计？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析这个不是技术问题，这个没有通用的一个方法，这个应该结合业务来保证幂等性。 所谓幂等性，就是说一个接口，多次发起同一个请求，你这个接口得保证结果是准确的，比如不能多扣款、不能多插入一条数据、不能将统计值多加了 1。这就是幂等性。 其实保证幂等性主要是三点： 对于每个请求必须有一个唯一的标识，举个栗子：订单支付请求，肯定得包含订单 id，一个订单 id 最多支付一次，对吧。 每次处理完请求之后，必须有一个记录标识这个请求处理过了。常见的方案是在 mysql 中记录个状态啥的，比如支付之前记录一条这个订单的支付流水。 每次接收请求需要进行判断，判断之前是否处理过。比如说，如果有一个订单已经支付了，就已经有了一条支付流水，那么如果重复发送这个请求，则此时先插入支付流水，orderId 已经存在了，唯一键约束生效，报错插入不进去的。然后你就不用再扣款了。 实际运作过程中，你要结合自己的业务来，比如说利用 redis，用 orderId 作为唯一键。只有成功插入这个支付流水，才可以执行实际的支付扣款。 要求是支付一个订单，必须插入一条支付流水，order_id 建一个唯一键 unique key。你在支付一个订单之前，先插入一条支付流水，order_id 就已经进去了。你就可以写一个标识到 redis 里面去，set order_id payed，下一次重复请求过来了，先查 redis 的 order_id 对应的 value，如果是 payed 就说明已经支付过了，你就别重复支付了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何基于Dubbo进行服务治理？]]></title>
      <url>%2F2020%2F06%2F03%2F%E9%9D%A2%E8%AF%95%2F2020-6-3-%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8EDubbo%E8%BF%9B%E8%A1%8C%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何基于 dubbo 进行服务治理、服务降级、失败重试以及超时重试？ 面试官心理分析服务治理，这个问题如果问你，其实就是看看你有没有服务治理的思想，因为这个是做过复杂微服务的人肯定会遇到的一个问题。 服务降级，这个是涉及到复杂分布式系统中必备的一个话题，因为分布式系统互相来回调用，任何一个系统故障了，你不降级，直接就全盘崩溃？那就太坑爹了吧。 失败重试，分布式系统中网络请求如此频繁，要是因为网络问题不小心失败了一次，是不是要重试？ 超时重试，跟上面一样，如果不小心网络慢一点，超时了，如何重试？ 转载请注明出处：http://shenshanlaoyuan.com/2020/06/03/面试/2020-6-3-如何基于Dubbo进行服务治理？/访问原文「如何基于Dubbo进行服务治理？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析服务治理1. 调用链路自动生成一个大型的分布式系统，或者说是用现在流行的微服务架构来说吧，分布式系统由大量的服务组成。那么这些服务之间互相是如何调用的？调用链路是啥？说实话，几乎到后面没人搞的清楚了，因为服务实在太多了，可能几百个甚至几千个服务。 那就需要基于 dubbo 做的分布式系统中，对各个服务之间的调用自动记录下来，然后自动将各个服务之间的依赖关系和调用链路生成出来，做成一张图，显示出来，大家才可以看到对吧。 2. 服务访问压力以及时长统计需要自动统计各个接口和服务之间的调用次数以及访问延时，而且要分成两个级别。 一个级别是接口粒度，就是每个服务的每个接口每天被调用多少次，TP50/TP90/TP99，三个档次的请求延时分别是多少； 第二个级别是从源头入口开始，一个完整的请求链路经过几十个服务之后，完成一次请求，每天全链路走多少次，全链路请求延时的 TP50/TP90/TP99，分别是多少。 这些东西都搞定了之后，后面才可以来看当前系统的压力主要在哪里，如何来扩容和优化啊。 3. 其它 服务分层（避免循环依赖） 调用链路失败监控和报警 服务鉴权 每个服务的可用性的监控（接口调用成功率？几个 9？99.99%，99.9%，99%） 服务降级比如说服务 A 调用服务 B，结果服务 B 挂掉了，服务 A 重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。 举个栗子，我们有接口 HelloService。HelloServiceImpl 有该接口的具体实现。 123456789public interface HelloService &#123; void sayHello();&#125;public class HelloServiceImpl implements HelloService &#123; public void sayHello() &#123; System.out.println("hello world......"); &#125;&#125; 123456789101112131415161718192021222324252627&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application name="dubbo-provider" /&gt; &lt;dubbo:registry address="zookeeper://127.0.0.1:2181" /&gt; &lt;dubbo:protocol name="dubbo" port="20880" /&gt; &lt;dubbo:service interface="com.zhss.service.HelloService" ref="helloServiceImpl" timeout="10000" /&gt; &lt;bean id="helloServiceImpl" class="com.zhss.service.HelloServiceImpl" /&gt;&lt;/beans&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application name="dubbo-consumer" /&gt; &lt;dubbo:registry address="zookeeper://127.0.0.1:2181" /&gt; &lt;dubbo:reference id="fooService" interface="com.test.service.FooService" timeout="10000" check="false" mock="return null"&gt; &lt;/dubbo:reference&gt;&lt;/beans&gt; 我们调用接口失败的时候，可以通过 mock 统一返回 null。 mock 的值也可以修改为 true，然后再跟接口同一个路径下实现一个 Mock 类，命名规则是 “接口名称+Mock” 后缀。然后在 Mock 类里实现自己的降级逻辑。 12345public class HelloServiceMock implements HelloService &#123; public void sayHello() &#123; // 降级逻辑 &#125;&#125; 失败重试和超时重试所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。配置如下： 1&lt;dubbo:reference id="xxxx" interface="xx" check="true" async="false" retries="3" timeout="2000"/&gt; 举个栗子。 某个服务的接口，要耗费 5s，你这边不能干等着，你这边配置了 timeout 之后，我等待 2s，还没返回，我直接就撤了，不能干等你。 可以结合你们公司具体的场景来说说你是怎么设置这些参数的： timeout：一般设置为 200ms，我们认为不能超过 200ms 还没返回。 retries：设置 retries，一般是在读请求的时候，比如你要查询个数据，你可以设置个 retries，如果第一次没读到，报错，重试指定的次数，尝试再次读取。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dubbo的spi思想是什么？]]></title>
      <url>%2F2020%2F06%2F02%2F%E9%9D%A2%E8%AF%95%2F2020-6-2-Dubbo%E7%9A%84spi%E6%80%9D%E6%83%B3%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题dubbo 的 spi 思想是什么？ 面试官心理分析继续深入问呗，前面一些基础性的东西问完了，确定你应该都 ok，了解 dubbo 的一些基本东西，那么问个稍微难一点点的问题，就是 spi，先问问你 spi 是啥？然后问问你 dubbo 的 spi 是怎么实现的？ 其实就是看看你对 dubbo 的掌握如何。 转载请注明出处：http://shenshanlaoyuan.com/2020/06/02/面试/2020-6-2-Dubbo的spi思想是什么？/访问原文「Dubbo的spi思想是什么？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析spi 是啥？spi，简单来说，就是 service provider interface，说白了是什么意思呢，比如你有个接口，现在这个接口有 3 个实现类，那么在系统运行的时候对这个接口到底选择哪个实现类呢？这就需要 spi 了，需要根据指定的配置或者是默认的配置，去找到对应的实现类加载进来，然后用这个实现类的实例对象。 举个栗子。 你有一个接口 A。A1/A2/A3 分别是接口A的不同实现。你通过配置 接口 A = 实现 A2，那么在系统实际运行的时候，会加载你的配置，用实现 A2 实例化一个对象来提供服务。 spi 机制一般用在哪儿？插件扩展的场景，比如说你开发了一个给别人使用的开源框架，如果你想让别人自己写个插件，插到你的开源框架里面，从而扩展某个功能，这个时候 spi 思想就用上了。 Java spi 思想的体现spi 经典的思想体现，大家平时都在用，比如说 jdbc。 Java 定义了一套 jdbc 的接口，但是 Java 并没有提供 jdbc 的实现类。 但是实际上项目跑的时候，要使用 jdbc 接口的哪些实现类呢？一般来说，我们要根据自己使用的数据库，比如 mysql，你就将 mysql-jdbc-connector.jar 引入进来；oracle，你就将 oracle-jdbc-connector.jar 引入进来。 在系统跑的时候，碰到你使用 jdbc 的接口，他会在底层使用你引入的那个 jar 中提供的实现类。 dubbo 的 spi 思想dubbo 也用了 spi 思想，不过没有用 jdk 的 spi 机制，是自己实现的一套 spi 机制。1Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); Protocol 接口，在系统运行的时候，，dubbo 会判断一下应该选用这个 Protocol 接口的哪个实现类来实例化对象来使用。 它会去找一个你配置的 Protocol，将你配置的 Protocol 实现类，加载到 jvm 中来，然后实例化对象，就用你的那个 Protocol 实现类就可以了。 上面那行代码就是 dubbo 里大量使用的，就是对很多组件，都是保留一个接口和多个实现，然后在系统运行的时候动态根据配置去找到对应的实现类。如果你没配置，那就走默认的实现好了，没问题。1234567891011121314@SPI("dubbo") public interface Protocol &#123; int getDefaultPort(); @Adaptive &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException; @Adaptive &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException; void destroy(); &#125; 在 dubbo 自己的 jar 里，在/META_INF/dubbo/internal/com.alibaba.dubbo.rpc.Protocol文件中：123dubbo=com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocolhttp=com.alibaba.dubbo.rpc.protocol.http.HttpProtocolhessian=com.alibaba.dubbo.rpc.protocol.hessian.HessianProtocol 所以说，这就看到了 dubbo 的 spi 机制默认是怎么玩儿的了，其实就是 Protocol 接口，@SPI(&quot;dubbo&quot;) 说的是，通过 SPI 机制来提供实现类，实现类是通过 dubbo 作为默认 key 去配置文件里找到的，配置文件名称与接口全限定名一样的，通过 dubbo 作为 key 可以找到默认的实现类就是 com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol。 如果想要动态替换掉默认的实现类，需要使用 @Adaptive 接口，Protocol 接口中，有两个方法加了 @Adaptive 注解，就是说那俩接口会被代理实现。 啥意思呢？ 比如这个 Protocol 接口搞了俩 @Adaptive 注解标注了方法，在运行的时候会针对 Protocol 生成代理类，这个代理类的那俩方法里面会有代理代码，代理代码会在运行的时候动态根据 url 中的 protocol 来获取那个 key，默认是 dubbo，你也可以自己指定，你如果指定了别的 key，那么就会获取别的实现类的实例了。 如何自己扩展 dubbo 中的组件下面来说说怎么来自己扩展 dubbo 中的组件。 自己写个工程，要是那种可以打成 jar 包的，里面的 src/main/resources 目录下，搞一个 META-INF/services，里面放个文件叫：com.alibaba.dubbo.rpc.Protocol，文件里搞一个my=com.bingo.MyProtocol。自己把 jar 弄到 nexus 私服里去。 然后自己搞一个 dubbo provider 工程，在这个工程里面依赖你自己搞的那个 jar，然后在 spring 配置文件里给个配置： 1&lt;dubbo:protocol name=”my” port=”20000” /&gt; provider 启动的时候，就会加载到我们 jar 包里的my=com.bingo.MyProtocol 这行配置里，接着会根据你的配置使用你定义好的 MyProtocol 了，这个就是简单说明一下，你通过上述方式，可以替换掉大量的 dubbo 内部的组件，就是扔个你自己的 jar 包，然后配置一下即可。 dubbo 里面提供了大量的类似上面的扩展点，就是说，你如果要扩展一个东西，只要自己写个 jar，让你的 consumer 或者是 provider 工程，依赖你的那个 jar，在你的 jar 里指定目录下配置好接口名称对应的文件，里面通过 key=实现类。 然后对于对应的组件，类似 &lt;dubbo:protocol&gt; 用你的那个 key 对应的实现类来实现某个接口，你可以自己去扩展 dubbo 的各种功能，提供你自己的实现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dubbo负载均衡策略和集群容错策略？]]></title>
      <url>%2F2020%2F06%2F01%2F%E9%9D%A2%E8%AF%95%2F2020-6-1-Dubbo%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AD%96%E7%95%A5%E5%92%8C%E9%9B%86%E7%BE%A4%E5%AE%B9%E9%94%99%E7%AD%96%E7%95%A5%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？ 面试官心理分析继续深问吧，这些都是用 dubbo 必须知道的一些东西，你得知道基本原理，知道序列化是什么协议，还得知道具体用 dubbo 的时候，如何负载均衡，如何高可用，如何动态代理。 说白了，就是看你对 dubbo 熟悉不熟悉： dubbo 工作原理：服务注册、注册中心、消费者、代理通信、负载均衡； 网络通信、序列化：dubbo 协议、长连接、NIO、hessian 序列化协议； 负载均衡策略、集群容错策略、动态代理策略：dubbo 跑起来的时候一些功能是如何运转的？怎么做负载均衡？怎么做集群容错？怎么生成动态代理？ dubbo SPI 机制：你了解不了解 dubbo 的 SPI 机制？如何基于 SPI 机制对 dubbo 进行扩展？ 转载请注明出处：http://shenshanlaoyuan.com/2020/06/01/面试/2020-6-1-Dubbo负载均衡策略和集群容错策略？/访问原文「Dubbo负载均衡策略和集群容错策略？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析dubbo 负载均衡策略random loadbalance默认情况下，dubbo 是 random load balance ，即随机调用实现负载均衡，可以对 provider 不同实例设置不同的权重，会按照权重来负载均衡，权重越大分配流量越高，一般就用这个默认的就可以了。 roundrobin loadbalance这个的话默认就是均匀地将流量打到各个机器上去，但是如果各个机器的性能不一样，容易导致性能差的机器负载过高。所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。 举个栗子。 跟运维同学申请机器，有的时候，我们运气好，正好公司资源比较充足，刚刚有一批热气腾腾、刚刚做好的虚拟机新鲜出炉，配置都比较高：8 核 + 16G 机器，申请到 2 台。过了一段时间，我们感觉 2 台机器有点不太够，我就去找运维同学说，“哥儿们，你能不能再给我一台机器”，但是这时只剩下一台 4 核 + 8G 的机器。我要还是得要。 这个时候，可以给两台 8 核 16G 的机器设置权重 4，给剩余 1 台 4 核 8G 的机器设置权重 2。 leastactive loadbalance这个就是自动感知一下，如果某个机器性能越差，那么接收的请求越少，越不活跃，此时就会给不活跃的性能差的机器更少的请求。 consistanthash loadbalance一致性 Hash 算法，相同参数的请求一定分发到一个 provider 上去，provider 挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性 Hash 策略。 dubbo 集群容错策略failover cluster 模式失败自动切换，自动重试其他机器，默认就是这个，常见于读操作。（失败重试其它机器） 可以通过以下几种方式配置重试次数： 1&lt;dubbo:service retries="2" /&gt; 或者 1&lt;dubbo:reference retries="2" /&gt; 或者 123&lt;dubbo:reference&gt; &lt;dubbo:method name="findFoo" retries="2" /&gt;&lt;/dubbo:reference&gt; failfast cluster 模式一次调用失败就立即失败，常见于非幂等性的写操作，比如新增一条记录（调用失败就立即失败） failsafe cluster 模式出现异常时忽略掉，常用于不重要的接口调用，比如记录日志。 配置示例如下： 1&lt;dubbo:service cluster="failsafe" /&gt; 或者 1&lt;dubbo:reference cluster="failsafe" /&gt; failback cluster 模式失败了后台自动记录请求，然后定时重发，比较适合于写消息队列这种。 forking cluster 模式并行调用多个 provider，只要一个成功就立即返回。常用于实时性要求比较高的读操作，但是会浪费更多的服务资源，可通过 forks=&quot;2&quot; 来设置最大并行数。 broadcacst cluster逐个调用所有的 provider。任何一个 provider 出错则报错（从2.1.0 版本开始支持）。通常用于通知所有提供者更新缓存或日志等本地资源信息。 dubbo动态代理策略默认使用 javassist 动态字节码生成，创建代理类。但是可以通过 spi 扩展机制配置自己的动态代理策略。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dubbo支持哪些序列化协议？]]></title>
      <url>%2F2020%2F05%2F31%2F%E9%9D%A2%E8%AF%95%2F2020-5-31-Dubbo%E6%94%AF%E6%8C%81%E5%93%AA%E4%BA%9B%E5%BA%8F%E5%88%97%E5%8C%96%E5%8D%8F%E8%AE%AE%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题dubbo 支持哪些通信协议？支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？ 面试官心理分析上一个问题，说说 dubbo 的基本工作原理，那是你必须知道的，至少要知道 dubbo 分成哪些层，然后平时怎么发起 rpc 请求的，注册、发现、调用，这些是基本的。 接着就可以针对底层进行深入的问问了，比如第一步就可以先问问序列化协议这块，就是平时 RPC 的时候怎么走的？ 转载请注明出处：http://shenshanlaoyuan.com/2020/05/31/面试/2020-5-31-Dubbo支持哪些序列化协议？/访问原文「Dubbo支持哪些序列化协议？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析序列化，就是把数据结构或者是一些对象，转换为二进制串的过程，而反序列化是将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。 dubbo 支持不同的通信协议 dubbo 协议 默认就是走 dubbo 协议，单一长连接，进行的是 NIO 异步通信，基于 hessian 作为序列化协议。使用的场景是：传输数据量小（每次请求在 100kb 以内），但是并发量很高。 为了要支持高并发场景，一般是服务提供者就几台机器，但是服务消费者有上百台，可能每天调用量达到上亿次！此时用长连接是最合适的，就是跟每个服务消费者维持一个长连接就可以，可能总共就 100 个连接。然后后面直接基于长连接 NIO 异步通信，可以支撑高并发请求。 长连接，通俗点说，就是建立连接过后可以持续发送请求，无须再建立连接。 而短连接，每次要发送请求之前，需要先重新建立一次连接。 rmi 协议 走 Java 二进制序列化，多个短连接，适合消费者和提供者数量差不多的情况，适用于文件的传输，一般较少用。 hessian 协议 走 hessian 序列化协议，多个短连接，适用于提供者数量比消费者数量还多的情况，适用于文件的传输，一般较少用。 http 协议 走 json 序列化。 webservice 走 SOAP 文本序列化。 dubbo 支持的序列化协议dubbo 支持 hession、Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。但是 hessian 是其默认的序列化协议。 说一下 Hessian 的数据结构Hessian 的对象序列化机制有 8 种原始类型： 原始二进制数据 boolean 64-bit date（64 位毫秒值的日期） 64-bit double 32-bit int 64-bit long null UTF-8 编码的 string 另外还包括 3 种递归类型： list for lists and arrays map for maps and dictionaries object for objects 还有一种特殊的类型： ref：用来表示对共享对象的引用。 为什么 PB 的效率是最高的？可能有一些同学比较习惯于 JSON or XML 数据存储格式，对于 Protocol Buffer 还比较陌生。Protocol Buffer 其实是 Google 出品的一种轻量并且高效的结构化数据存储格式，性能比 JSON、XML 要高很多。 其实 PB 之所以性能如此好，主要得益于两个：第一，它使用 proto 编译器，自动进行序列化和反序列化，速度非常快，应该比 XML 和 JSON 快上了 20~100 倍；第二，它的数据压缩效果好，就是说它序列化后的数据量体积小。因为体积小，传输起来带宽和速度上会有优化。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[说一下 Dubbo 的工作原理？]]></title>
      <url>%2F2020%2F05%2F30%2F%E9%9D%A2%E8%AF%95%2F2020-5-30-%E8%AF%B4%E4%B8%80%E4%B8%8B-Dubbo-%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题说一下的 dubbo 的工作原理？注册中心挂了可以继续通信吗？说说一次 rpc 请求的流程？ 面试官心理分析MQ、ES、Redis、Dubbo，上来先问你一些思考性的问题、原理，比如 kafka 高可用架构原理、es 分布式架构原理、redis 线程模型原理、Dubbo 工作原理；之后就是生产环境里可能会碰到的一些问题，因为每种技术引入之后生产环境都可能会碰到一些问题；再来点综合的，就是系统设计，比如让你设计一个 MQ、设计一个搜索引擎、设计一个缓存、设计一个 rpc 框架等等。 那既然开始聊分布式系统了，自然重点先聊聊 dubbo 了，毕竟 dubbo 是目前事实上大部分公司的分布式系统的 rpc 框架标准，基于 dubbo 也可以构建一整套的微服务架构。但是需要自己大量开发。 当然去年开始 spring cloud 非常火，现在大量的公司开始转向 spring cloud 了，spring cloud 人家毕竟是微服务架构的全家桶式的这么一个东西。但是因为很多公司还在用 dubbo，所以 dubbo 肯定会是目前面试的重点，何况人家 dubbo 现在重启开源社区维护了，捐献给了 apache，未来应该也还是有一定市场和地位的。 既然聊 dubbo，那肯定是先从 dubbo 原理开始聊了，你先说说 dubbo 支撑 rpc 分布式调用的架构啥的，然后说说一次 rpc 请求 dubbo 是怎么给你完成的，对吧。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/30/面试/2020-5-30-说一下-Dubbo-的工作原理？/访问原文「说一下 Dubbo 的工作原理？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析dubbo 工作原理 第一层：service 层，接口层，给服务提供者和消费者来实现的 第二层：config 层，配置层，主要是对 dubbo 进行各种配置的 第三层：proxy 层，服务代理层，无论是 consumer 还是 provider，dubbo 都会给你生成代理，代理之间进行网络通信 第四层：registry 层，服务注册层，负责服务的注册与发现 第五层：cluster 层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务 第六层：monitor 层，监控层，对 rpc 接口的调用次数和调用时间进行监控 第七层：protocal 层，远程调用层，封装 rpc 调用 第八层：exchange 层，信息交换层，封装请求响应模式，同步转异步 第九层：transport 层，网络传输层，抽象 mina 和 netty 为统一接口 第十层：serialize 层，数据序列化层 工作流程 第一步：provider 向注册中心去注册 第二步：consumer 从注册中心订阅服务，注册中心会通知 consumer 注册好的服务 第三步：consumer 调用 provider 第四步：consumer 和 provider 都异步通知监控中心 注册中心挂了可以继续通信吗？可以，因为刚开始初始化的时候，消费者会将提供者的地址等信息拉取到本地缓存，所以注册中心挂了可以继续通信。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[为什么要进行系统拆分？]]></title>
      <url>%2F2020%2F05%2F29%2F%E9%9D%A2%E8%AF%95%2F2020-5-29-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E7%B3%BB%E7%BB%9F%E6%8B%86%E5%88%86%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题为什么要进行系统拆分？如何进行系统拆分？拆分后不用 dubbo 可以吗？ 面试官心理分析从这个问题开始就进行分布式系统环节了，现在出去面试分布式都成标配了，没有哪个公司不问问你分布式的事儿。你要是不会分布式的东西，简直这简历没法看，没人会让你去面试。 其实为啥会这样呢？这就是因为整个大行业技术发展的原因。 早些年，印象中在 2010 年初的时候，整个 IT 行业，很少有人谈分布式，更不用说微服务，虽然很多 BAT 等大型公司，因为系统的复杂性，很早就是分布式架构，大量的服务，只不过微服务大多基于自己搞的一套框架来实现而已。 但是确实，那个年代，大家很重视 ssh2，很多中小型公司几乎大部分都是玩儿 struts2、spring、hibernate，稍晚一些，才进入了 spring mvc、spring、mybatis 的组合。那个时候整个行业的技术水平就是那样，当年 oracle 很火，oracle 管理员很吃香，oracle 性能优化啥的都是 IT 男的大杀招啊。连大数据都没人提，当年 OCP、OCM 等认证培训机构，火的不行。 但是确实随着时代的发展，慢慢的，很多公司开始接受分布式系统架构了，这里面尤为对行业有至关重要影响的，是阿里的 dubbo，某种程度上而言，阿里在这里推动了行业技术的前进。 正是因为有阿里的 dubbo，很多中小型公司才可以基于 dubbo，来把系统拆分成很多的服务，每个人负责一个服务，大家的代码都没有冲突，服务可以自治，自己选用什么技术都可以，每次发布如果就改动一个服务那就上线一个服务好了，不用所有人一起联调，每次发布都是几十万行代码，甚至几百万行代码了。 直到今日，很高兴看到分布式系统都成行业面试标配了，任何一个普通的程序员都该掌握这个东西，其实这是行业的进步，也是所有 IT 码农的技术进步。所以既然分布式都成标配了，那么面试官当然会问了，因为很多公司现在都是分布式、微服务的架构，那面试官当然得考察考察你了。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/29/面试/2020-5-29-为什么要进行系统拆分？/访问原文「为什么要进行系统拆分？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析为什么要将系统进行拆分？网上查查，答案极度零散和复杂，很琐碎，原因一大坨。但是我这里给大家直观的感受： 要是不拆分，一个大系统几十万行代码，20 个人维护一份代码，简直是悲剧啊。代码经常改着改着就冲突了，各种代码冲突和合并要处理，非常耗费时间；经常我改动了我的代码，你调用了我的，导致你的代码也得重新测试，麻烦的要死；然后每次发布都是几十万行代码的系统一起发布，大家得一起提心吊胆准备上线，几十万行代码的上线，可能每次上线都要做很多的检查，很多异常问题的处理，简直是又麻烦又痛苦；而且如果我现在打算把技术升级到最新的 spring 版本，还不行，因为这可能导致你的代码报错，我不敢随意乱改技术。 假设一个系统是 20 万行代码，其中 A 在里面改了 1000 行代码，但是此时发布的时候是这个 20 万行代码的大系统一块儿发布。就意味着 20 万上代码在线上就可能出现各种变化，20 个人，每个人都要紧张地等在电脑面前，上线之后，检查日志，看自己负责的那一块儿有没有什么问题。 A 就检查了自己负责的 1 万行代码对应的功能，确保 ok 就闪人了；结果不巧的是，A 上线的时候不小心修改了线上机器的某个配置，导致另外 B 和 C 负责的 2 万行代码对应的一些功能，出错了。 几十个人负责维护一个几十万行代码的单块应用，每次上线，准备几个礼拜，上线 -&gt; 部署 -&gt; 检查自己负责的功能。 拆分了以后，整个世界清爽了，几十万行代码的系统，拆分成 20 个服务，平均每个服务就 1~2 万行代码，每个服务部署到单独的机器上。20 个工程，20 个 git 代码仓库，20 个开发人员，每个人维护自己的那个服务就可以了，是自己独立的代码，跟别人没关系。再也没有代码冲突了，爽。每次就测试我自己的代码就可以了，爽。每次就发布我自己的一个小服务就可以了，爽。技术上想怎么升级就怎么升级，保持接口不变就可以了，真爽。 所以简单来说，一句话总结，如果是那种代码量多达几十万行的中大型项目，团队里有几十个人，那么如果不拆分系统，开发效率极其低下，问题很多。但是拆分系统之后，每个人就负责自己的一小部分就好了，可以随便玩儿随便弄。分布式系统拆分之后，可以大幅度提升复杂系统大型团队的开发效率。 但是同时，也要提醒的一点是，系统拆分成分布式系统之后，大量的分布式系统面临的问题也是接踵而来，所以后面的问题都是在围绕分布式系统带来的复杂技术挑战在说。 如何进行系统拆分？这个问题说大可以很大，可以扯到领域驱动模型设计上去，说小了也很小，我不太想给大家太过于学术的说法，因为你也不可能背这个答案，过去了直接说吧。还是说的简单一点，大家自己到时候知道怎么回答就行了。 系统拆分为分布式系统，拆成多个服务，拆成微服务的架构，是需要拆很多轮的。并不是说上来一个架构师一次就给拆好了，而以后都不用拆。 第一轮；团队继续扩大，拆好的某个服务，刚开始是 1 个人维护 1 万行代码，后来业务系统越来越复杂，这个服务是 10 万行代码，5 个人；第二轮，1个服务 -&gt; 5个服务，每个服务 2 万行代码，每人负责一个服务。 如果是多人维护一个服务，最理想的情况下，几十个人，1 个人负责 1 个或 2~3 个服务；某个服务工作量变大了，代码量越来越多，某个同学，负责一个服务，代码量变成了 10 万行了，他自己不堪重负，他现在一个人拆开，5 个服务，1 个人顶着，负责 5 个人，接着招人，2 个人，给那个同学带着，3 个人负责 5 个服务，其中 2 个人每个人负责 2 个服务，1 个人负责 1 个服务。 个人建议，一个服务的代码不要太多，1 万行左右，两三万撑死了吧。 大部分的系统，是要进行多轮拆分的，第一次拆分，可能就是将以前的多个模块该拆分开来了，比如说将电商系统拆分成订单系统、商品系统、采购系统、仓储系统、用户系统，等等吧。 但是后面可能每个系统又变得越来越复杂了，比如说采购系统里面又分成了供应商管理系统、采购单管理系统，订单系统又拆分成了购物车系统、价格系统、订单管理系统。 扯深了实在很深，所以这里先给大家举个例子，你自己感受一下，核心意思就是根据情况，先拆分一轮，后面如果系统更复杂了，可以继续分拆。你根据自己负责系统的例子，来考虑一下就好了。 拆分后不用 dubbo 可以吗？当然可以了，大不了最次，就是各个系统之间，直接基于 spring mvc，就纯 http 接口互相通信呗，还能咋样。但是这个肯定是有问题的，因为 http 接口通信维护起来成本很高，你要考虑超时重试、负载均衡等等各种乱七八糟的问题，比如说你的订单系统调用商品系统，商品系统部署了 5 台机器，你怎么把请求均匀地甩给那 5 台机器？这不就是负载均衡？你要是都自己搞那是可以的，但是确实很痛苦。 所以 dubbo 说白了，是一种 rpc 框架，就是说本地就是进行接口调用，但是 dubbo 会代理这个调用请求，跟远程机器网络通信，给你处理掉负载均衡、服务实例上下线自动感知、超时重试等等乱七八糟的问题。那你就不用自己做了，用 dubbo 就可以了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何设计一个高并发系统？]]></title>
      <url>%2F2020%2F05%2F28%2F%E9%9D%A2%E8%AF%95%2F2020-5-28-%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何设计一个高并发系统？ 面试官心理分析说实话，如果面试官问你这个题目，那么你必须要使出全身吃奶劲了。为啥？因为你没看到现在很多公司招聘的 JD 里都是说啥，有高并发就经验者优先。 如果你确实有真才实学，在互联网公司里干过高并发系统，那你确实拿 offer 基本如探囊取物，没啥问题。面试官也绝对不会这样来问你，否则他就是蠢。 假设你在某知名电商公司干过高并发系统，用户上亿，一天流量几十亿，高峰期并发量上万，甚至是十万。那么人家一定会仔细盘问你的系统架构，你们系统啥架构？怎么部署的？部署了多少台机器？缓存咋用的？MQ 咋用的？数据库咋用的？就是深挖你到底是如何扛住高并发的。 因为真正干过高并发的人一定知道，脱离了业务的系统架构都是在纸上谈兵，真正在复杂业务场景而且还高并发的时候，那系统架构一定不是那么简单的，用个 redis，用 mq 就能搞定？当然不是，真实的系统架构搭配上业务之后，会比这种简单的所谓“高并发架构”要复杂很多倍。 如果有面试官问你个问题说，如何设计一个高并发系统？那么不好意思，一定是因为你实际上没干过高并发系统。面试官看你简历就没啥出彩的，感觉就不咋地，所以就会问问你，如何设计一个高并发系统？其实说白了本质就是看看你有没有自己研究过，有没有一定的知识积累。 最好的当然是招聘个真正干过高并发的哥儿们咯，但是这种哥儿们人数稀缺，不好招。所以可能次一点的就是招一个自己研究过的哥儿们，总比招一个啥也不会的哥儿们好吧！ 所以这个时候你必须得做一把个人秀了，秀出你所有关于高并发的知识！ 转载请注明出处：http://shenshanlaoyuan.com/2020/05/28/面试/2020-5-28-如何设计一个高并发系统？/访问原文「如何设计一个高并发系统？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析其实所谓的高并发，如果你要理解这个问题呢，其实就得从高并发的根源出发，为啥会有高并发？为啥高并发就很牛逼？ 我说的浅显一点，很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。所以才有说，很多公司，刚开始干的时候，技术比较 low，结果业务发展太快，有的时候系统扛不住压力就挂了。 当然会挂了，凭什么不挂？你数据库如果瞬间承载每秒 5000/8000，甚至上万的并发，一定会宕机，因为比如 mysql 就压根儿扛不住这么高的并发量。 所以为啥高并发牛逼？就是因为现在用互联网的人越来越多，很多 app、网站、系统承载的都是高并发请求，可能高峰期每秒并发量几千，很正常的。如果是什么双十一之类的，每秒并发几万几十万都有可能。 那么如此之高的并发量，加上原本就如此之复杂的业务，咋玩儿？真正厉害的，一定是在复杂业务系统里玩儿过高并发架构的人，但是你没有，那么我给你说一下你该怎么回答这个问题： 可以分为以下 6 点： 系统拆分 缓存 MQ 分库分表 读写分离 ElasticSearch 系统拆分将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。 缓存缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。 MQMQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 mysql 还得用 mysql 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，后边系统消费后慢慢写，控制在 mysql 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok 的，这个之前还特意说过。 分库分表分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 跑的性能。 读写分离读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。 ElasticSearchElasticsearch，简称 es。es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作，也可以考虑用 es 来承载。 上面的 6 点，基本就是高并发系统肯定要干的一些事儿，大家可以仔细结合之前讲过的知识考虑一下，到时候你可以系统的把这块阐述一下，然后每个部分要注意哪些问题，之前都讲过了，你都可以阐述阐述，表明你对这块是有点积累的。 说句实话，毕竟你真正厉害的一点，不是在于弄明白一些技术，或者大概知道一个高并发系统应该长什么样？其实实际上在真正的复杂的业务系统里，做高并发要远远比上面提到的点要复杂几十倍到上百倍。你需要考虑：哪些需要分库分表，哪些不需要分库分表，单库单表跟分库分表如何 join，哪些数据要放到缓存里去，放哪些数据才可以扛住高并发的请求，你需要完成对一个复杂业务系统的分析之后，然后逐步逐步的加入高并发的系统架构的改造，这个过程是无比复杂的，一旦做过一次，并且做好了，你在这个市场上就会非常的吃香。 其实大部分公司，真正看重的，不是说你掌握高并发相关的一些基本的架构知识，架构中的一些技术，RocketMQ、Kafka、Redis、Elasticsearch，高并发这一块，你了解了，也只能是次一等的人才。对一个有几十万行代码的复杂的分布式系统，一步一步架构、设计以及实践过高并发架构的人，这个经验是难能可贵的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何实现 MySQL 的读写分离？]]></title>
      <url>%2F2020%2F05%2F27%2F%E9%9D%A2%E8%AF%95%2F2020-5-27-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0-MySQL-%E7%9A%84%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？ 面试官心理分析高并发这个阶段，肯定是需要做读写分离的，啥意思？因为实际上大部分的互联网公司，一些网站，或者是 app，其实都是读多写少。所以针对这个情况，就是写一个主库，但是主库挂多个从库，然后从多个从库来读，那不就可以支撑更高的读并发压力了吗？ 转载请注明出处：http://shenshanlaoyuan.com/2020/05/27/面试/2020-5-27-如何实现-MySQL-的读写分离？/访问原文「如何实现 MySQL 的读写分离？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析如何实现 MySQL 的读写分离？其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 MySQL 主从复制原理的是啥？主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。 所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。 所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 MySQL 主从同步延时问题（精华）以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。 是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。 我们通过 MySQL 命令：1show status 查看 Seconds_Behind_Master，可以看到从库复制主库的数据落后了几 ms。 一般来说，如果主从延迟较为严重，有以下解决方案： 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分库分表之后，id 主键如何处理？]]></title>
      <url>%2F2020%2F05%2F26%2F%E9%9D%A2%E8%AF%95%2F2020-5-26-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E4%B9%8B%E5%90%8E%EF%BC%8Cid-%E4%B8%BB%E9%94%AE%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题分库分表之后，id 主键如何处理？ 面试官心理分析其实这是分库分表之后你必然要面对的一个问题，就是 id 咋生成？因为要是分成多个表之后，每个表都是从 1 开始累加，那肯定不对啊，需要一个全局唯一的 id 来支持。所以这都是你实际生产环境中必须考虑的问题。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/26/面试/2020-5-26-分库分表之后，id-主键如何处理？/访问原文「分库分表之后，id 主键如何处理？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析基于数据库的实现方案数据库自增 id这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。 这个方案的好处就是方便简单，谁都会用；缺点就是单库生成自增 id，要是高并发的话，就会有瓶颈的；如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值；但是无论如何都是基于单个数据库。 适合的场景：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你并发不高，但是数据量太大导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。 设置数据库 sequence 或者表自增字段步长可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。 比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。 适合的场景：在用户防止产生的 ID 重复时，这种方案实现起来比较简单，也能达到性能目标。但是服务节点固定，步长也固定，将来如果还要增加服务节点，就不好搞了。 UUID好处就是本地生成，不要基于数据库来了；不好之处就是，UUID 太长了、占用空间大，作为主键性能太差了；更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。 适合的场景：如果你是要随机生成个什么文件名、编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。 1UUID.randomUUID().toString().replace("-", "") -&gt; sfsdf23423rr234sfdaf 获取系统当前时间这个就是获取当前时间即可，但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个是肯定不合适的。基本就不用考虑了。 适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号。 snowflake 算法snowflake 算法是 twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2^41 - 1 个毫秒值，换算成年就是表示69年的时间。 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2^5个机房（32个机房），每个机房里可以代表 2^5 个机器（32台机器）。 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 2^12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。 10 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class IdWorker &#123; private long workerId; private long datacenterId; private long sequence; public IdWorker(long workerId, long datacenterId, long sequence) &#123; // sanity check for workerId // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0 if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException( String.format("worker Id can't be greater than %d or less than 0", maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException( String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId)); &#125; System.out.printf( "worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d", timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId); this.workerId = workerId; this.datacenterId = datacenterId; this.sequence = sequence; &#125; private long twepoch = 1288834974657L; private long workerIdBits = 5L; private long datacenterIdBits = 5L; // 这个是二进制运算，就是 5 bit最多只能有31个数字，也就是说机器id最多只能是32以内 private long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); // 这个是一个意思，就是 5 bit最多只能有31个数字，机房id最多只能是32以内 private long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); private long sequenceBits = 12L; private long workerIdShift = sequenceBits; private long datacenterIdShift = sequenceBits + workerIdBits; private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; private long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); private long lastTimestamp = -1L; public long getWorkerId() &#123; return workerId; &#125; public long getDatacenterId() &#123; return datacenterId; &#125; public long getTimestamp() &#123; return System.currentTimeMillis(); &#125; public synchronized long nextId() &#123; // 这儿就是获取当前时间戳，单位是毫秒 long timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; System.err.printf("clock is moving backwards. Rejecting requests until %d.", lastTimestamp); throw new RuntimeException(String.format( "Clock moved backwards. Refusing to generate id for %d milliseconds", lastTimestamp - timestamp)); &#125; if (lastTimestamp == timestamp) &#123; // 这个意思是说一个毫秒内最多只能有4096个数字 // 无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围 sequence = (sequence + 1) &amp; sequenceMask; if (sequence == 0) &#123; timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123; sequence = 0; &#125; // 这儿记录一下最近一次生成id的时间戳，单位是毫秒 lastTimestamp = timestamp; // 这儿就是将时间戳左移，放到 41 bit那儿； // 将机房 id左移放到 5 bit那儿； // 将机器id左移放到5 bit那儿；将序号放最后12 bit； // 最后拼接起来成一个 64 bit的二进制数字，转换成 10 进制就是个 long 型 return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) | (datacenterId &lt;&lt; datacenterIdShift) | (workerId &lt;&lt; workerIdShift) | sequence; &#125; private long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; private long timeGen() &#123; return System.currentTimeMillis(); &#125; // ---------------测试--------------- public static void main(String[] args) &#123; IdWorker worker = new IdWorker(1, 1, 1); for (int i = 0; i &lt; 30; i++) &#123; System.out.println(worker.nextId()); &#125; &#125;&#125; 怎么说呢，大概这个意思吧，就是说 41 bit 是当前毫秒单位的一个时间戳，就这意思；然后 5 bit 是你传递进来的一个机房 id（但是最大只能是 32 以内），另外 5 bit 是你传递进来的机器 id（但是最大只能是 32 以内），剩下的那个 12 bit序列号，就是如果跟你上次生成 id 的时间还在一个毫秒内，那么会把顺序给你累加，最多在 4096 个序号以内。 所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是 0。然后每次接收到一个请求，说这个机房的这个机器要生成一个 id，你就找到对应的 Worker 生成。 利用这个 snowflake 算法，你可以开发自己公司的服务，甚至对于机房 id 和机器 id，反正给你预留了 5 bit + 5 bit，你换成别的有业务含义的东西也可以的。 这个 snowflake 算法相对来说还是比较靠谱的，所以你要真是搞分布式 id 生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何设计可以动态扩容缩容的分库分表方案？]]></title>
      <url>%2F2020%2F05%2F25%2F%E9%9D%A2%E8%AF%95%2F2020-5-25-%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E5%8F%AF%E4%BB%A5%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9%E7%BC%A9%E5%AE%B9%E7%9A%84%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E6%96%B9%E6%A1%88%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何设计可以动态扩容缩容的分库分表方案？ 面试官心理分析对于分库分表来说，主要是面对以下问题： 选择一个数据库中间件，调研、学习、测试； 设计你的分库分表的一个方案，你要分成多少个库，每个库分成多少个表，比如 3 个库，每个库 4 个表； 基于选择好的数据库中间件，以及在测试环境建立好的分库分表的环境，然后测试一下能否正常进行分库分表的读写； 完成单库单表到分库分表的迁移，双写方案； 线上系统开始基于分库分表对外提供服务； 扩容了，扩容成 6 个库，每个库需要 12 个表，你怎么来增加更多库和表呢？ 这个是你必须面对的一个事儿，就是你已经弄好分库分表方案了，然后一堆库和表都建好了，基于分库分表中间件的代码开发啥的都好了，测试都 ok 了，数据能均匀分布到各个库和各个表里去，而且接着你还通过双写的方案咔嚓一下上了系统，已经直接基于分库分表方案在搞了。 那么现在问题来了，你现在这些库和表又支撑不住了，要继续扩容咋办？这个可能就是说你的每个库的容量又快满了，或者是你的表数据量又太大了，也可能是你每个库的写并发太高了，你得继续扩容。 这都是玩儿分库分表线上必须经历的事儿。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/25/面试/2020-5-25-如何设计可以动态扩容缩容的分库分表方案？/访问原文「如何设计可以动态扩容缩容的分库分表方案？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析停机扩容（不推荐）这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。 从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。那么你写个工具，多弄几台机器并行跑，1小时数据就导完了。这没有问题。 如果 3 个库 + 12 个表，跑了一段时间了，数据量都 1~2 亿了。光是导 2 亿数据，都要导个几个小时，6 点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10 点才可以搞完。所以不能这么搞。 优化后的方案一开始上来就是 32 个库，每个库 32 个表，那么总共是 1024 张表。 我可以告诉各位同学，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题。 每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载 32 1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32 1500 = 48000 的写并发，接近 5 万每秒的写入并发，前面再加一个MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。 有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128 个库，256 个库，512 个库。 1024 张表，假设每个表放 500 万数据，在 MySQL 里可以放 50 亿条数据。 每秒 5 万的写并发，总共 50 亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了。 谈分库分表的扩容，第一次分库分表，就一次性给他分个够，32 个库，1024 张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了。 一个实践是利用 32 * 32 来分库分表，即分为 32 个库，每个库里一个表分为 32 张表。一共就是 1024 张表。根据某个 id 先根据 32 取模路由到库，再根据 32 取模路由到库里的表。 orderId id % 32 (库) id / 32 % 32 (表) 259 3 8 1189 5 5 352 0 11 4593 17 15 刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个 mysql 服务器可能建了 n 个库，比如 32 个库。后面如果要拆分，就是不断在库和 mysql 服务器之间做迁移就可以了。然后系统配合改一下配置即可。 比如说最多可以扩展到 32 个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到 1024 个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是 1024 个表。 这么搞，是不用自己写代码做数据迁移的，都交给 dba 来搞好了，但是 dba 确实是需要做一些库表迁移的工作，但是总比你自己写代码，然后抽数据导数据来的效率高得多吧。 哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。 这里对步骤做一个总结： 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32 库 * 32 表，对于大部分公司来说，可能几年都够了。 路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表 扩容的时候，申请增加更多的数据库服务器，装好 mysql，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器。 由 dba 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的。 我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。 重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分库分表如何平滑过渡？]]></title>
      <url>%2F2020%2F05%2F24%2F%E9%9D%A2%E8%AF%95%2F2020-5-24-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%A6%82%E4%BD%95%E5%B9%B3%E6%BB%91%E8%BF%87%E6%B8%A1%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？ 面试官心理分析你看看，你现在已经明白为啥要分库分表了，你也知道常用的分库分表中间件了，你也设计好你们如何分库分表的方案了（水平拆分、垂直拆分、分表），那问题来了，你接下来该怎么把你那个单库单表的系统给迁移到分库分表上去？ 所以这都是一环扣一环的，就是看你有没有全流程经历过这个过程。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/24/面试/2020-5-24-分库分表如何平滑过渡？/访问原文「分库分表如何平滑过渡？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析这个其实从 low 到高大上有好几种方案，我们都玩儿过，我都给你说一下。 停机迁移方案我先给你说一个最 low 的方案，就是很简单，大家伙儿凌晨 12 点开始运维，网站或者 app 挂个公告，说 0 点到早上 6 点进行运维，无法访问。 接着到 0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一个导数的一次性工具，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。 导数完了之后，就 ok 了，修改系统的数据库连接配置啥的，包括可能代码和 SQL 也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。 验证一下，ok了，完美，大家伸个懒腰，看看看凌晨 4 点钟的北京夜景，打个滴滴回家吧。 但是这个方案比较 low，谁都能干，我们来看看高大上一点的方案。 双写迁移方案这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨 4 点的风景。 简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，除了对老库增删改，都加上对新库的增删改，这就是所谓的双写，同时写俩库，老库和新库。 然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。简单来说，就是不允许用老数据覆盖新数据。 导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。 接着当数据完全一致了，就 ok 了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[为什么要分库分表？]]></title>
      <url>%2F2020%2F05%2F23%2F%E9%9D%A2%E8%AF%95%2F2020-5-23-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题为什么要分库分表（设计高并发系统的时候，数据库层面该如何设计）？用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？你们具体是如何对数据库如何进行垂直拆分或水平拆分的？ 面试官心理分析其实这块肯定是扯到高并发了，因为分库分表一定是为了支撑高并发、数据量大两个问题的。而且现在说实话，尤其是互联网类的公司面试，基本上都会来这么一下，分库分表如此普遍的技术问题，不问实在是不行，而如果你不知道那也实在是说不过去！ 转载请注明出处：http://shenshanlaoyuan.com/2020/05/23/面试/2020-5-23-为什么要分库分表？/访问原文「为什么要分库分表？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？）说白了，分库分表是两回事儿，大家可别搞混了，可能是光分库不分表，也可能是光分表不分库，都有可能。 我先给大家抛出来一个场景。 假如我们现在是一个小创业公司（或者是一个 BAT 公司刚兴起的一个新部门），现在注册用户就 20 万，每天活跃用户就 1 万，每天单表数据量就 1000，然后高峰期每秒钟并发请求最多就 10 个。我的天，就这种系统，随便找一个有几年工作经验的，然后带几个刚培训出来的，随便干干都可以。 结果没想到我们运气居然这么好，碰上个 CEO 带着我们走上了康庄大道，业务发展迅猛，过了几个月，注册用户数达到了 2000 万！每天活跃用户数 100 万！每天单表数据量 10 万条！高峰期每秒最大请求达到 1000！同时公司还顺带着融资了两轮，进账了几个亿人民币啊！公司估值达到了惊人的几亿美金！这是小独角兽的节奏！ 好吧，没事，现在大家感觉压力已经有点大了，为啥呢？因为每天多 10 万条数据，一个月就多 300 万条数据，现在咱们单表已经几百万数据了，马上就破千万了。但是勉强还能撑着。高峰期请求现在是 1000，咱们线上部署了几台机器，负载均衡搞了一下，数据库撑 1000QPS 也还凑合。但是大家现在开始感觉有点担心了，接下来咋整呢…… 再接下来几个月，我的天，CEO 太牛逼了，公司用户数已经达到 1 亿，公司继续融资几十亿人民币啊！公司估值达到了惊人的几十亿美金，成为了国内今年最牛逼的明星创业公司！天，我们太幸运了。 但是我们同时也是不幸的，因为此时每天活跃用户数上千万，每天单表新增数据多达 50 万，目前一个表总数据量都已经达到了两三千万了！扛不住啊！数据库磁盘容量不断消耗掉！高峰期并发达到惊人的 5000~8000！别开玩笑了，哥。我跟你保证，你的系统支撑不到现在，已经挂掉了！ 好吧，所以你看到这里差不多就理解分库分表是怎么回事儿了，实际上这是跟着你的公司业务发展走的，你公司业务发展越好，用户就越多，数据量越大，请求量越大，那你单个数据库一定扛不住。 分表比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。 分表是啥意思？就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。 分库分库是啥意思？就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 这就是所谓的分库分表，为啥要分库分表？你明白了吧。 # 分库分表前 分库分表后 并发支撑情况 MySQL 单机部署，扛不住高并发 MySQL从单机到多机，能承受的并发增加了多倍 磁盘使用情况 MySQL 单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低 SQL 执行性能 单表数据量太大，SQL 越跑越慢 单表数据量减少，SQL 执行效率明显提升 用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？这个其实就是看看你了解哪些分库分表的中间件，各个中间件的优缺点是啥？然后你用过哪些分库分表的中间件。 比较常见的包括： Cobar TDDL Atlas Sharding-jdbc Mycat Cobar阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 Cobar 集群，Cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。 TDDL淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。 Atlas360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。 Sharding-jdbc当当开源的，属于 client 层方案，是ShardingSphere的 client 层方案，ShardingSphere还提供 proxy 层的方案 Sharding-Proxy。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且截至 2019.4，已经推出到了 4.0.0-RC1 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 Mycat基于 Cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 Sharding jdbc 来说，年轻一些，经历的锤炼少一些。 总结综上，现在其实建议考量的，就是 Sharding-jdbc 和 Mycat，这两个都可以去考虑使用。 Sharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 Sharding-jdbc 的依赖； Mycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。 通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 Sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 Mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 Mycat，然后大量项目直接透明使用即可。 你们具体是如何对数据库如何进行垂直拆分或水平拆分的？水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来扛更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。 还有表层面的拆分，就是分表，将一个表变成 N 个表，就是让每个表的数据量控制在一定范围内，保证 SQL 的性能。否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多，但是也得看具体你怎么操作，也可能是 500 万，或者是 100 万。你的SQL越复杂，就最好让单表行数越少。 好了，无论分库还是分表，上面说的那些数据库中间件都是可以支持的。就是基本上那些中间件可以做到你分库分表之后，中间件可以根据你指定的某个字段值，比如说 userid，自动路由到对应的库上去，然后再自动路由到对应的表里去。 你就得考虑一下，你的项目里该如何分库分表？一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都 ok 了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。 而且这儿还有两种分库分表的方式： 一种是按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。 或者是按照某个字段 hash 一下均匀分散，这个较为常用。 range 来分，好处在于说，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。 hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[生产环境中的Redis是怎么部署的？]]></title>
      <url>%2F2020%2F05%2F22%2F%E9%9D%A2%E8%AF%95%2F2020-5-22-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84Redis%E6%98%AF%E6%80%8E%E4%B9%88%E9%83%A8%E7%BD%B2%E7%9A%84%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题生产环境中的 redis 是怎么部署的？ 面试官心理分析看看你了解不了解你们公司的 redis 生产集群的部署架构，如果你不了解，那么确实你就很失职了，你的 redis 是主从架构？集群架构？用了哪种集群方案？有没有做高可用保证？有没有开启持久化机制确保可以进行数据恢复？线上 redis 给几个 G 的内存？设置了哪些参数？压测后你们 redis 集群承载多少 QPS？ 兄弟，这些你必须是门儿清的，否则你确实是没好好思考过。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/22/面试/2020-5-22-生产环境中的Redis是怎么部署的？/访问原文「生产环境中的Redis是怎么部署的？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。 机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。 5 台机器对外提供读写，一共有 50g 内存。 因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。 你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。 其实大型的公司，会有基础架构的 team 负责缓存集群的运维。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何解决Redis的并发竞争问题？]]></title>
      <url>%2F2020%2F05%2F21%2F%E9%9D%A2%E8%AF%95%2F2020-5-21-%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3Redis%E7%9A%84%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E9%97%AE%E9%A2%98%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题redis 的并发竞争问题是什么？如何解决这个问题？了解 redis 事务的 CAS 方案吗？ 面试官心理分析这个也是线上非常常见的一个问题，就是多客户端同时并发写一个 key，可能本来应该先到的数据后到了，导致数据版本错了；或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了。 而且 redis 自己就有天然解决这个问题的 CAS 类的乐观锁方案。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/21/面试/2020-5-21-如何解决Redis的并发竞争问题？/访问原文「如何解决Redis的并发竞争问题？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过 zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。 你要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。 每次要写之前，先判断一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何保证缓存与数据库双写一致性？]]></title>
      <url>%2F2020%2F05%2F20%2F%E9%9D%A2%E8%AF%95%2F2020-5-20-%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E7%BC%93%E5%AD%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何保证缓存与数据库的双写一致性？ 面试官心理分析你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 面试题剖析一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。 串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/20/面试/2020-5-20-如何保证缓存与数据库双写一致性？/访问原文「如何保证缓存与数据库双写一致性？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() Cache Aside Pattern最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 为什么是删除缓存，而不是更新缓存？ 原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？ 举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。用到缓存才去算缓存。 其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都把里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。 最初级的缓存不一致问题及解决方案问题：先更新数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。 解决思路：先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，所以去读了数据库中的旧数据，然后更新到缓存中。 比较复杂的数据不一致问题分析数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了… 为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。 解决方案如下： 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新执行“读取数据+更新缓存”的操作，根据唯一标识路由之后，也发送到同一个 jvm 内部队列中。 一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。 高并发的场景下，该解决方案要注意的问题： 读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。 该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每个库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致*读请求的长时阻塞。 一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。 如果一个内存队列中可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。 其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。 我们来实际粗略测算一下。 如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。 经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。 读请求并发量过高 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。 但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。 多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器路由到相同的服务实例上。 比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。 热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis的雪崩、穿透和击穿，如何应对？]]></title>
      <url>%2F2020%2F05%2F19%2F%E9%9D%A2%E8%AF%95%2F2020-5-19-redis%E7%9A%84%E9%9B%AA%E5%B4%A9%E3%80%81%E7%A9%BF%E9%80%8F%E5%92%8C%E5%87%BB%E7%A9%BF%EF%BC%8C%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题了解什么是 redis 的雪崩、穿透和击穿？redis 崩溃之后会怎么样？系统该如何应对这种情况？如何处理 redis 的穿透？ 面试官心理分析其实这是问到缓存必问的，因为缓存雪崩和穿透，是缓存最大的两个问题，要么不出现，一旦出现就是致命性的问题，所以面试官一定会问你。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/19/面试/2020-5-19-redis的雪崩、穿透和击穿，如何应对？/访问原文「redis的雪崩、穿透和击穿，如何应对？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析缓存雪崩对于系统 A，假设每天高峰期每秒 5000 个请求，本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。此时，如果没有采用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。 这就是缓存雪崩。 大约在 3 年前，国内比较知名的一个互联网公司，曾因为缓存事故，导致雪崩，后台系统全部崩溃，事故从当天下午持续到晚上凌晨 3~4 点，公司损失了几千万。 缓存雪崩的事前事中事后的解决方案如下： 事前：redis 高可用，主从+哨兵，redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + hystrix 限流&amp;降级，避免 MySQL 被打死。 事后：redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。 用户发送一个请求，系统 A 收到请求后，先查本地 ehcache 缓存，如果没查到再查 redis。如果 ehcache 和 redis 都没有，再查数据库，将数据库中的结果，写入 ehcache 和 redis 中。 限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？走降级！可以返回一些默认的值，或者友情提示，或者空值。 好处： 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。 只要数据库不死，就是说，对用户来说，2/5 的请求都是可以被处理的。 只要有 2/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来了。 缓存穿透对于系统A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。 黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。 举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“视缓存于无物”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。 解决方式很简单，每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 set -999 UNKNOWN。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。 缓存击穿缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。 不同场景下的解决方式可如下： 若缓存的数据是基本不会发生更新的，则可尝试将该热点数据设置为永不过期。 若缓存的数据更新不频繁，且缓存刷新的整个流程耗时较少的情况下，则可以采用基于 redis、zookeeper 等分布式中间件的分布式互斥锁，或者本地互斥锁以保证仅少量的请求能请求数据库并重新构建缓存，其余线程则在锁释放后能访问到新缓存。 若缓存的数据更新频繁或者在缓存刷新的流程耗时较长的情况下，可以利用定时线程在缓存过期前主动地重新构建缓存或者延后缓存的过期时间，以保证所有的请求能一直访问到对应的缓存。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis集群模式的工作原理能说一下么？]]></title>
      <url>%2F2020%2F05%2F18%2F%E9%9D%A2%E8%AF%95%2F2020-5-18-Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E8%83%BD%E8%AF%B4%E4%B8%80%E4%B8%8B%E4%B9%88%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题redis 集群模式的工作原理能说一下么？在集群模式下，redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？ 面试官心理分析在前几年，redis 如果要搞几个节点，每个节点存储一部分的数据，得借助一些中间件来实现，比如说有 codis，或者 twemproxy，都有。有一些 redis 中间件，你读写 redis 中间件，redis 中间件负责将你的数据分布式存储在多台机器上的 redis 实例中。 这两年，redis 不断在发展，redis 也不断有新的版本，现在的 redis 集群模式，可以做到在多台机器上，部署多个 redis 实例，每个实例存储一部分的数据，同时每个 redis 主实例可以挂 redis 从实例，自动确保说，如果 redis 主实例挂了，会自动切换到 redis 从实例上来。 现在 redis 的新版本，大家都是用 redis cluster 的，也就是 redis 原生支持的 redis 集群模式，那么面试官肯定会就 redis cluster 对你来个几连炮。要是你没用过 redis cluster，正常，以前很多人用 codis 之类的客户端来支持集群，但是起码你得研究一下 redis cluster 吧。 如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个 G，单机就足够了，可以使用 replication，一个 master 多个 slaves，要几个 slave 跟你要求的读吞吐量有关，然后自己搭建一个 sentinel 集群去保证 redis 主从架构的高可用性。 redis cluster，主要是针对海量数据+高并发+高可用的场景。redis cluster 支撑 N 个 redis master node，每个 master node 都可以挂载多个 slave node。这样整个 redis 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/18/面试/2020-5-18-Redis集群模式的工作原理能说一下么？/访问原文「Redis集群模式的工作原理能说一下么？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析redis cluster 介绍 自动将数据进行分片，每个 master 上放一部分数据 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。 16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制基本通信原理集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。 集中式是将集群元数据（节点信息、故障等等）几种存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 storm。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。 redis 维护集群元数据采用另一个方式， gossip 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。 集中式的好处在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；不好在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。 gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。 10000 端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其它几个节点接收到 ping 之后返回 pong。 交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。 gossip 协议gossip 协议包含多种消息，包含 ping,pong,meet,fail 等等。 meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。 1redis-trib.rb add-node 其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。 ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。 pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。 fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。 ping 消息深入ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。 每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 cluster_node_timeout 可以调节，如果调得比较大，那么会降低 ping 的频率。 每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 3 个其它节点的信息，最多包含 总节点数减 2 个其它节点的信息。 分布式寻址算法 hash 算法（大量缓存重建） 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） redis cluster 的 hash slot 算法 hash 算法来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。 一致性 hash 算法一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。 来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置。 在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。 燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。 redis cluster 的 hash slot 算法redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。 redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现。 任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。 redis cluster 的高可用与主备切换原理redis cluster 的高可用的原理，几乎跟哨兵是类似的。 判断节点宕机如果一个节点认为另外一个节点宕机，那么就是 pfail，主观宕机。如果多个节点都认为另外一个节点宕机了，那么就是 fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown。 在 cluster-node-timeout 内，某个节点一直没有返回 pong，那么就被认为 pfail。 如果一个节点认为某个节点 pfail 了，那么会在 gossip ping 消息中，ping 给其他节点，如果超过半数的节点都认为 pfail 了，那么就会变成 fail。 从节点过滤对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。 检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成 master。 从节点选举每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。 所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。 从节点执行主备切换，从节点切换为主节点。 与哨兵比较整个流程跟哨兵相比，非常类似，所以说，redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis的持久化有哪几种方式？]]></title>
      <url>%2F2020%2F05%2F16%2F%E9%9D%A2%E8%AF%95%2F2020-5-16-Redis%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题redis 的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？ 面试官心理分析redis 如果仅仅只是将数据缓存在内存里面，如果 redis 宕机了再重启，内存里的数据就全部都弄丢了啊。你必须得用 redis 的持久化机制，将数据写入内存的同时，异步的慢慢的将数据写入磁盘文件里，进行持久化。 如果 redis 宕机重启，自动从磁盘上加载之前持久化的一些数据就可以了，也许会丢失少许数据，但是至少不会将所有数据都弄丢。 这个其实一样，针对的都是 redis 的生产环境可能遇到的一些问题，就是 redis 要是挂了再重启，内存里的数据不就全丢了？能不能重启的时候把数据给恢复了？ 转载请注明出处：http://shenshanlaoyuan.com/2020/05/16/面试/2020-5-16-Redis的持久化有哪几种方式？/访问原文「Redis的持久化有哪几种方式？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析持久化主要是做灾难恢复、数据恢复，也可以归类到高可用的一个环节中去，比如你 redis 整个挂了，然后 redis 就不可用了，你要做的事情就是让 redis 变得可用，尽快变得可用。 重启 redis，尽快让它对外提供服务，如果没做数据备份，这时候 redis 启动了，也不可用啊，数据都没了。 很可能说，大量的请求过来，缓存全部无法命中，在 redis 里根本找不到数据，这个时候就死定了，出现缓存雪崩问题。所有请求没有在 redis 命中，就会去 mysql 数据库这种数据源头中去找，一下子 mysql 承接高并发，然后就挂了… 如果你把 redis 持久化做好，备份和恢复方案做到企业级的程度，那么即使你的 redis 故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务。 redis 持久化的两种方式 RDB：RDB 持久化机制，是对 redis 中的数据执行周期性的持久化。 AOF：AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。 通过 RDB 或 AOF，都可以将 redis 内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云等云服务。 如果 redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 redis，redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务。 如果同时使用 RDB 和 AOF 两种持久化机制，那么在 redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。 RDB 优缺点 RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 redis 中的数据。 RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis 保持高性能，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。 如果想要在 redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 redis 进程宕机，那么会丢失最近 5 分钟的数据。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF 优缺点 AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次fsync操作，最多丢失 1 秒钟的数据。 AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 AOF 日志文件的命令通过可读较强的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。 AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的。（如果实时写入，那么 QPS 会大降，redis 性能会大大降低） 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 RDB 和 AOF 到底该如何选择 不要仅仅使用 RDB，因为那样会导致你丢失很多数据； 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug； redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何保证Redis高并发、高可用？]]></title>
      <url>%2F2020%2F05%2F15%2F%E9%9D%A2%E8%AF%95%2F2020-5-15-%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81Redis%E9%AB%98%E5%B9%B6%E5%8F%91%E3%80%81%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何保证 redis 的高并发和高可用？redis 的主从复制原理能介绍一下么？redis 的哨兵原理能介绍一下么？ 面试官心理分析其实问这个问题，主要是考考你，redis 单机能承载多高并发？如果单机扛不住如何扩容扛更多的并发？redis 会不会挂？既然 redis 会挂那怎么保证 redis 是高可用的？ 其实针对的都是项目中你肯定要考虑的一些问题，如果你没考虑过，那确实你对生产系统中的问题思考太少。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/15/面试/2020-5-15-如何保证Redis高并发、高可用？/访问原文「如何保证Redis高并发、高可用？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析如果你用 redis 缓存技术的话，肯定要考虑如何用 redis 来加多台机器，保证 redis 是高并发的，还有就是如何让 redis 保证自己不是挂掉以后就直接死掉了，即 redis 高可用。 由于此节内容较多，因此，会分为两个小节进行讲解。 redis 主从架构 redis 基于哨兵实现高可用 redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。 如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。 redis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis的过期策略都有哪些？]]></title>
      <url>%2F2020%2F05%2F13%2F%E9%9D%A2%E8%AF%95%2F2020-5-13-Redis%E7%9A%84%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题redis 的过期策略都有哪些？内存淘汰机制都有哪些？手写一下 LRU 代码实现？ 面试官心理分析如果你连这个问题都不知道，上来就懵了，回答不出来，那线上你写代码的时候，想当然的认为写进 redis 的数据就一定会存在，后面导致系统各种 bug，谁来负责？ 常见的有两个问题： 往 redis 写入的数据怎么没了？ 可能有同学会遇到，在生产环境的 redis 经常会丢掉一些数据，写进去了，过一会儿可能就没了。我的天，同学，你问这个问题就说明 redis 你就没用对啊。redis 是缓存，你给当存储了是吧？ 啥叫缓存？用内存当缓存。内存是无限的吗，内存是很宝贵而且是有限的，磁盘是廉价而且是大量的。可能一台机器就几十个 G 的内存，但是可以有几个 T 的硬盘空间。redis 主要是基于内存来进行高性能、高并发的读写操作的。 那既然内存是有限的，比如 redis 就只能用 10G，你要是往里面写了 20G 的数据，会咋办？当然会干掉 10G 的数据，然后就保留 10G 的数据了。那干掉哪些数据？保留哪些数据？当然是干掉不常用的数据，保留常用的数据了。 数据明明过期了，怎么还占用着内存？ 这是由 redis 的过期策略来决定。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/13/面试/2020-5-13-Redis的过期策略都有哪些？/访问原文「Redis的过期策略都有哪些？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析redis 过期策略redis 过期策略是：定期删除+惰性删除。 所谓定期删除，指的是 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。 假设 redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。 但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。 获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。 但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 redis 内存块耗尽了，咋整？ 答案是：走内存淘汰机制。 内存淘汰机制redis 内存淘汰机制有以下几个： noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。 手写一个 LRU 算法你可以现场手写最原始的 LRU 算法，那个代码量太大了，似乎不太现实。 不求自己纯手工从底层开始打造出自己的 LRU，但是起码要知道如何利用已有的 JDK 数据结构实现一个 Java 版的 LRU。 123456789101112131415161718192021222324class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private final int CACHE_SIZE; /** * 传递进来最多能缓存多少数据 * * @param cacheSize 缓存大小 */ public LRUCache(int cacheSize) &#123; // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。 super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); CACHE_SIZE = cacheSize; &#125; /** * 钩子方法，通过put新增键值对的时候，若该方法返回true * 便移除该map中最老的键和值 */ @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。 return size() &gt; CACHE_SIZE; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis都有哪些数据类型以及适用场景？]]></title>
      <url>%2F2020%2F05%2F12%2F%E9%9D%A2%E8%AF%95%2F2020-5-12-Redis%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%BB%A5%E5%8F%8A%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题redis 都有哪些数据类型？分别在哪些场景下使用比较合适？ 面试官心理分析除非是面试官感觉看你简历，是工作 3 年以内的比较初级的同学，可能对技术没有很深入的研究，面试官才会问这类问题。否则，在宝贵的面试时间里，面试官实在不想多问。 其实问这个问题，主要有两个原因： 看看你到底有没有全面的了解 redis 有哪些功能，一般怎么来用，啥场景用什么，就怕你别就会最简单的 KV 操作； 看看你在实际项目里都怎么玩儿过 redis。 要是你回答的不好，没说出几种数据类型，也没说什么场景，你完了，面试官对你印象肯定不好，觉得你平时就是做个简单的 set 和 get。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/12/面试/2020-5-12-Redis都有哪些数据类型以及适用场景？/访问原文「Redis都有哪些数据类型以及适用场景？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析redis 主要有以下几种数据类型： string hash list set sorted set string这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。1set college szu hash这个是类似 map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 redis 里，然后每次读写缓存的时候，可以就操作 hash 里的某个字段。 1234hset person name bingohset person age 20hset person id 1hget person name 12345person = &#123; "name": "bingo", "age": 20, "id": 1&#125; listlist 是有序列表，这个可以玩儿出很多花样。 比如可以通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。 比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 list 实现分页查询，这个是很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。12# 0开始位置，-1结束位置，结束位置为-1时，表示列表的最后一个位置，即查看所有。lrange mylist 0 -1 比如可以搞个简单的消息队列，从 list 头怼进去，从 list 尾巴那里弄出来。123456lpush mylist 1lpush mylist 2lpush mylist 3 4 5# 1rpop mylist setset 是无序集合，自动去重。 直接基于 set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 jvm 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于 redis 进行全局的 set 去重。 可以基于 set 玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁？对吧。 把两个大 V 的粉丝都放在两个 set 中，对两个 set 做交集。1234567891011121314151617181920212223242526272829303132#-------操作一个set-------# 添加元素sadd mySet 1# 查看全部元素smembers mySet# 判断是否包含某个值sismember mySet 3# 删除某个/些元素srem mySet 1srem mySet 2 4# 查看元素个数scard mySet# 随机删除一个元素spop mySet#-------操作多个set-------# 将一个set的元素移动到另外一个setsmove yourSet mySet 2# 求两set的交集sinter yourSet mySet# 求两set的并集sunion yourSet mySet# 求在yourSet中而不在mySet中的元素sdiff yourSet mySet sorted setsorted set 是排序的 set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。12345678910zadd board 85 zhangsanzadd board 72 lisizadd board 96 wangwuzadd board 63 zhaoliu# 获取排名前三的用户（默认是升序，所以需要 rev 改为降序）zrevrange board 0 3# 获取某用户的排名zrank board zhaoliu]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis和Memcached有什么区别？]]></title>
      <url>%2F2020%2F05%2F11%2F%E9%9D%A2%E8%AF%95%2F2020-5-11-Redis%E5%92%8CMemcached%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题redis 和 memcached 有什么区别？redis 的线程模型是什么？为什么 redis 单线程却能支撑高并发？ 面试官心理分析这个是问 redis 的时候，最基本的问题吧，redis 最基本的一个内部原理和特点，就是 redis 实际上是个单线程工作模型，你要是这个都不知道，那后面玩儿 redis 的时候，出了问题岂不是什么都不知道？ 还有可能面试官会问问你 redis 和 memcached 的区别，但是 memcached 是早些年各大互联网公司常用的缓存方案，但是现在近几年基本都是 redis，没什么公司用 memcached 了。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/11/面试/2020-5-11-Redis和Memcached有什么区别？/访问原文「Redis和Memcached有什么区别？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析redis 和 memcached 有啥区别？redis 支持复杂的数据结构redis 相比 memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， redis 会是不错的选择。 redis 原生支持集群模式在 redis3.x 版本中，便能支持 cluster 模式，而 memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比由于 redis 只使用单核，而 memcached 可以使用多核，所以平均每一个核上 redis 在存储小数据时比 memcached 性能更高。而在 100k 以上的数据中，memcached 性能要高于 redis。虽然 redis 最近也在存储大数据的性能上进行优化，但是比起 memcached，还是稍有逊色。 redis 的线程模型redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。 来看客户端与 redis 的一次通信过程： 要明白，通信是通过 socket 来完成的，不懂的同学可以先去看一看 socket 网络编程。 首先，redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。 客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。 假设此时客户端发送了一个 set key value 请求，此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。 如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。 这样便完成了一次通信。关于 Redis 的一次通信过程，推荐读者阅读《Redis 设计与实现——黄健宏》进行系统学习。 为啥 redis 单线程模型也能效率这么高？ 纯内存操作。 核心是基于非阻塞的 IO 多路复用机制。 C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。 单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在项目中缓存是如何使用的？]]></title>
      <url>%2F2020%2F05%2F10%2F%E9%9D%A2%E8%AF%95%2F2020-5-10-%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%BC%93%E5%AD%98%E6%98%AF%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E7%9A%84%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题项目中缓存是如何使用的？为什么要用缓存？缓存使用不当会造成什么后果？ 面试官心理分析这个问题，互联网公司必问，要是一个人连缓存都不太清楚，那确实比较尴尬。 只要问到缓存，上来第一个问题，肯定是先问问你项目哪里用了缓存？为啥要用？不用行不行？如果用了以后可能会有什么不良的后果？ 这就是看看你对缓存这个东西背后有没有思考，如果你就是傻乎乎的瞎用，没法给面试官一个合理的解答，那面试官对你印象肯定不太好，觉得你平时思考太少，就知道干活儿。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/10/面试/2020-5-10-在项目中缓存是如何使用的？/访问原文「在项目中缓存是如何使用的？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析项目中缓存是如何使用的？这个，需要结合自己项目的业务来。 为什么要用缓存？用缓存，主要有两个用途：高性能、高并发。 高性能假设这么个场景，你有个操作，一个请求过来，吭哧吭哧你各种乱七八糟操作 mysql，半天查出来一个结果，耗时 600ms。但是这个结果可能接下来几个小时都不会变了，或者变了也可以不用立即反馈给用户。那么此时咋办？ 缓存啊，折腾 600ms 查出来的结果，扔缓存里，一个 key 对应一个 value，下次再有人查，别走 mysql 折腾 600ms 了，直接从缓存里，通过一个 key 查出来一个 value，2ms 搞定。性能提升 300 倍。 就是说对于一些需要复杂操作耗时查出来的结果，且确定后面不怎么变化，但是有很多读请求，那么直接将查询出来的结果放在缓存中，后面直接读缓存就好。 高并发mysql 这么重的数据库，压根儿设计不是让你玩儿高并发的，虽然也可以玩儿，但是天然支持不好。mysql 单机支撑到 2000QPS 也开始容易报警了。 所以要是你有个系统，高峰期一秒钟过来的请求有 1万，那一个 mysql 单机绝对会死掉。你这个时候就只能上缓存，把很多数据放缓存，别放 mysql。缓存功能简单，说白了就是 key-value 式操作，单机支撑的并发量轻松一秒几万十几万，支撑高并发 so easy。单机承载并发量是 mysql 单机的几十倍。 缓存是走内存的，内存天然就支撑高并发。 用了缓存之后会有什么不良后果？常见的缓存问题有以下几个： 缓存与数据库双写不一致 缓存雪崩、缓存穿透、缓存击穿 缓存并发竞争 点击超链接，可直接查看缓存相关问题及解决方案。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[es生产集群的部署架构是什么？]]></title>
      <url>%2F2020%2F05%2F09%2F%E9%9D%A2%E8%AF%95%2F2020-5-9-es%E7%94%9F%E4%BA%A7%E9%9B%86%E7%BE%A4%E7%9A%84%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题es 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？ 面试官心理分析这个问题，包括后面的 redis 什么的，谈到 es、redis、mysql 分库分表等等技术，面试必问！就是你生产环境咋部署的？说白了，这个问题没啥技术含量，就是看你有没有在真正的生产环境里干过这事儿！ 有些同学可能是没在生产环境中干过的，没实际去拿线上机器部署过 es 集群，也没实际玩儿过，也没往 es 集群里面导入过几千万甚至是几亿的数据量，可能你就不太清楚这里面的一些生产项目中的细节。 如果你是自己就玩儿过 demo，没碰过真实的 es 集群，那你可能此时会懵。别懵，你一定要云淡风轻的回答出来这个问题，表示你确实干过这事儿。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/09/面试/2020-5-9-es生产集群的部署架构是什么？/访问原文「es生产集群的部署架构是什么？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析其实这个问题没啥，如果你确实干过 es，那你肯定了解你们生产 es 集群的实际情况，部署了几台机器？有多少个索引？每个索引有多大数据量？每个索引给了多少个分片？你肯定知道！ 但是如果你确实没干过，也别虚，我给你说一个基本的版本，你到时候就简单说一下就好了。 es 生产集群我们部署了 5 台机器，每台机器是 6 核 64G 的，集群总内存是 320G。 我们 es 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500MB，每月增量数据大概是 6 亿，15G。目前系统已经运行了几个月，现在 es 集群里数据总量大概是 100G 左右。 目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 es 的），每个索引的数据量大概是 20G，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个 shard。 大概就这么说一下就行了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[es在数十亿级别数量下如何提高查询效率？]]></title>
      <url>%2F2020%2F05%2F08%2F%E9%9D%A2%E8%AF%95%2F2020-5-8-es%E5%9C%A8%E6%95%B0%E5%8D%81%E4%BA%BF%E7%BA%A7%E5%88%AB%E6%95%B0%E9%87%8F%E4%B8%8B%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E6%9F%A5%E8%AF%A2%E6%95%88%E7%8E%87%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题es 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？ 面试官心理分析这个问题是肯定要问的，说白了，就是看你有没有实际干过 es，因为啥？其实 es 性能并没有你想象中那么好的。很多时候数据量大了，特别是有几亿条数据的时候，可能你会懵逼的发现，跑个搜索怎么一下 5~10s，坑爹了。第一次搜索的时候，是 5~10s，后面反而就快了，可能就几百毫秒。 你就很懵，每个用户第一次访问都会比较慢，比较卡么？所以你要是没玩儿过 es，或者就是自己玩玩儿 demo，被问到这个问题容易懵逼，显示出你对 es 确实玩儿的不怎么样？ 转载请注明出处：http://shenshanlaoyuan.com/2020/05/08/面试/2020-5-8-es在数十亿级别数量下如何提高查询效率？/访问原文「es在数十亿级别数量下如何提高查询效率？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析说实话，es 性能优化是没有什么银弹的，啥意思呢？就是不要期待着随手调一个参数，就可以万能的应对所有的性能慢的场景。也许有的场景是你换个参数，或者调整一下语法，就可以搞定，但是绝对不是所有场景都可以这样。 性能优化的杀手锏——filesystem cache你往 es 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去。 es 的搜索引擎严重依赖于底层的 filesystem cache，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file 索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。 性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1秒、5秒、10秒。但如果是走 filesystem cache，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。 这里有个真实的案例。某个公司 es 节点有 3 台机器，每台机器看起来内存很多，64G，总内存就是 64 * 3 = 192G。每台机器给 es jvm heap 是 32G，那么剩下来留给 filesystem cache 的就是每台机器才 32G，总共集群里给 filesystem cache 的就是 32 * 3 = 96G 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，es 数据量是 1T，那么每台机器的数据量是 300G。这样性能好吗？ filesystem cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。 归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。 根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。 比如说你现在有一行数据。id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 es + hbase 这么一个架构。 hbase 的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。 写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。 数据预热假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60G 数据，结果 filesystem cache 就 30G，还是有 30G 数据留在了磁盘上。 其实可以做数据预热。 举个例子，拿微博来说，你可以把一些大V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。 或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。 对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。 冷热分离es 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。 你看，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。 document 模型设计对于 MySQL，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。 最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。 document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。 分页性能优化es 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。 分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你必须得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 es 做分页的时候，你会发现越翻到后面，就越是慢。 我们之前也是遇到过这个问题，用 es 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。 有什么解决方案吗？ 不允许深度分页（默认深度分页性能很差）跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。 类似于 app 里的推荐商品不断下拉出来一页一页的类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 scroll api，关于如何使用，自行上网搜索。 scroll 会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。 但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻。 初始化时必须指定 scroll 参数，告诉 es 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。 除了用 scroll api，你也可以用 search_after 来做，search_after 的思想是使用前一页的结果来帮助检索下一页的数据，显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 sort 字段。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[es写入数据的工作原理是什么？]]></title>
      <url>%2F2020%2F05%2F07%2F%E9%9D%A2%E8%AF%95%2F2020-5-7-es%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题es 写入数据的工作原理是什么啊？es 查询数据的工作原理是什么啊？底层的 lucene 介绍一下呗？倒排索引了解吗？ 面试官心理分析问这个，其实面试官就是要看看你了解不了解 es 的一些基本原理，因为用 es 无非就是写入数据，搜索数据。你要是不明白你发起一个写入和搜索请求的时候，es 在干什么，那你真的是…… 对 es 基本就是个黑盒，你还能干啥？你唯一能干的就是用 es 的 api 读写数据了。要是出点什么问题，你啥都不知道，那还能指望你什么呢？ 转载请注明出处：http://shenshanlaoyuan.com/2020/05/07/面试/2020-5-7-es写入数据的工作原理是什么？/访问原文「es写入数据的工作原理是什么？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析es 写数据过程 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。 coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。 coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。 es 读数据过程可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。 客户端发送请求到任意一个 node，成为 coordinate node。 coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。 接收请求的 node 返回 document 给 coordinate node。 coordinate node 返回 document 给客户端。 es 搜索数据过程es 最强大的是做全文检索，就是比如你有三条数据：123java真好玩儿啊java好难学啊j2ee特别牛 你根据 java 关键词来搜索，将包含 java的 document 给搜索出来。es 就会给你返回：java真好玩儿啊，java好难学啊。 客户端发送请求到一个 coordinate node。 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard，都可以。 query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。 写数据底层原理 先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。 如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh。 每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file，每秒钟会产生一个新的磁盘文件 segment file，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。 但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。 操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache中，这个数据就可以被搜索到了。 为什么叫 es 是准实时的？ NRT，全称 near real-time。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。 重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。 commit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。 这个 commit 操作叫做 flush。默认 30 分钟自动执行一次 flush，但如果 translog 过大，也会触发 flush。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。 translog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。 translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。 实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。 总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。 数据写入 segment file 之后，同时就建立好了倒排索引。 删除/更新数据底层原理如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。 如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。 buffer 每 refresh 一次，就会产生一个 segment file，所以默认情况下是 1 秒钟一个 segment file，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point，标识所有新的 segment file，然后打开 segment file 供搜索使用，同时删除旧的 segment file。 底层 lucene简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。 通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。 倒排索引在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。 那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。 举个栗子。 有以下文档： DocId Doc 1 谷歌地图之父跳槽 Facebook 2 谷歌地图之父加盟 Facebook 3 谷歌地图创始人拉斯离开谷歌加盟 Facebook 4 谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关 5 谷歌地图之父拉斯加盟社交网站 Facebook 对文档进行分词之后，得到以下倒排索引。 WordId Word DocIds 1 谷歌 1,2,3,4,5 2 地图 1,2,3,4,5 3 之父 1,2,4,5 4 跳槽 1,4 5 Facebook 1,2,3,4,5 6 加盟 2,3,5 7 创始人 3 8 拉斯 3,5 9 离开 3 10 与 4 .. .. .. 另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。 那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。 要注意倒排索引的两个重要细节： 倒排索引中的所有词项对应一个或多个文档； 倒排索引中的词项根据字典顺序升序排列 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[es的分布式架构原理是什么？]]></title>
      <url>%2F2020%2F05%2F06%2F%E9%9D%A2%E8%AF%95%2F2020-5-6-es%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题es 的分布式架构原理能说一下么（es 是如何实现分布式的啊）？ 面试官心理分析在搜索这块，lucene 是最流行的搜索库。几年前业内一般都问，你了解 lucene 吗？你知道倒排索引的原理吗？现在早已经 out 了，因为现在很多项目都是直接用基于 lucene 的分布式搜索引擎—— ElasticSearch，简称为 es。 而现在分布式搜索基本已经成为大部分互联网行业的 Java 系统的标配，其中尤为流行的就是 es，前几年 es 没火的时候，大家一般用 solr。但是这两年基本大部分企业和项目都开始转向 es 了。 所以互联网面试，肯定会跟你聊聊分布式搜索引擎，也就一定会聊聊 es，如果你确实不知道，那你真的就 out 了。 如果面试官问你第一个问题，确实一般都会问你 es 的分布式架构设计能介绍一下么？就看看你对分布式搜索引擎架构的一个基本理解。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/06/面试/2020-5-6-es的分布式架构原理是什么？/访问原文「es的分布式架构原理是什么？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析ElasticSearch 设计的理念就是分布式搜索引擎，底层其实还是基于 lucene 的。核心思想就是在多台机器上启动多个 es 进程实例，组成了一个 es 集群。 es 中存储数据的基本单位是索引，比如说你现在要在 es 中存储一些订单数据，你就应该在 es 中创建一个索引 order_idx，所有的订单数据就都写到这个索引里面去，一个索引差不多就是相当于是 mysql 里的一张表。 1index -&gt; type -&gt; mapping -&gt; document -&gt; field。 这样吧，为了做个更直白的介绍，我在这里做个类比。但是切记，不要划等号，类比只是为了便于理解。 index 相当于 mysql 里的一张表。而 type 没法跟 mysql 里去对比，一个 index 里可以有多个 type，每个 type 的字段都是差不多的，但是有一些略微的差别。假设有一个 index，是订单 index，里面专门是放订单数据的。就好比说你在 mysql 中建表，有些订单是实物商品的订单，比如一件衣服、一双鞋子；有些订单是虚拟商品的订单，比如游戏点卡，话费充值。就两种订单大部分字段是一样的，但是少部分字段可能有略微的一些差别。 所以就会在订单 index 里，建两个 type，一个是实物商品订单 type，一个是虚拟商品订单 type，这两个 type 大部分字段是一样的，少部分字段是不一样的。 很多情况下，一个 index 里可能就一个 type，但是确实如果说是一个 index 里有多个 type 的情况（注意，mapping types 这个概念在 ElasticSearch 7.X 已被完全移除，详细说明可以参考官方文档），你可以认为 index 是一个类别的表，具体的每个 type 代表了 mysql 中的一个表。每个 type 有一个 mapping，如果你认为一个 type 是具体的一个表，index 就代表多个 type 同属于的一个类型，而 mapping 就是这个 type 的表结构定义，你在 mysql 中创建一个表，肯定是要定义表结构的，里面有哪些字段，每个字段是什么类型。实际上你往 index 里的一个 type 里面写的一条数据，叫做一条 document，一条 document 就代表了 mysql 中某个表里的一行，每个 document 有多个 field，每个 field 就代表了这个 document 中的一个字段的值。 你搞一个索引，这个索引可以拆分成多个 shard，每个 shard 存储部分数据。拆分多个 shard 是有好处的，一是支持横向扩展，比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去；二是提高性能，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能。 接着就是这个 shard 的数据实际是有多个备份，就是说每个 shard 都有一个 primary shard，负责写入数据，但是还有几个 replica shard。primary shard 写入数据之后，会将数据同步到其他几个 replica shard 上去。 通过这个 replica 的方案，每个 shard 的数据都有多个备份，如果某个机器宕机了，没关系啊，还有别的数据副本在别的机器上呢。高可用了吧。 es 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点其实就是干一些管理的工作的，比如维护索引元数据、负责切换 primary shard 和 replica shard 身份等。要是 master 节点宕机了，那么会重新选举一个节点为 master 节点。 如果是非 master节点宕机了，那么会由 master 节点，让那个宕机节点上的 primary shard 的身份转移到其他机器上的 replica shard。接着你要是修复了那个宕机机器，重启了之后，master 节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据之类的，让集群恢复正常。 说得更简单一点，就是说如果某个非 master 节点宕机了。那么此节点上的 primary shard 不就没了。那好，master 会让 primary shard 对应的 replica shard（在其他机器上）切换为 primary shard。如果宕机的机器修复了，修复后的节点也不再是 primary shard，而是 replica shard。 其实上述就是 ElasticSearch 作为分布式搜索引擎最基本的一个架构设计。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何设计一个消息队列？]]></title>
      <url>%2F2020%2F05%2F05%2F%E9%9D%A2%E8%AF%95%2F2020-5-5-%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如果让你写一个消息队列，该如何进行架构设计？说一下你的思路。 面试官心理分析其实聊到这个问题，一般面试官要考察两块： 你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个消息队列的架构原理。 看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来。 说实话，问类似问题的时候，大部分人基本都会蒙，因为平时从来没有思考过类似的问题，大多数人就是平时埋头用，从来不去思考背后的一些东西。类似的问题，比如，如果让你来设计一个 Spring 框架你会怎么做？如果让你来设计一个 Dubbo 框架你会怎么做？如果让你来设计一个 MyBatis 框架你会怎么做？ 转载请注明出处：http://shenshanlaoyuan.com/2020/05/05/面试/2020-5-5-如何设计一个消息队列？/访问原文「如何设计一个消息队列？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析其实回答这类问题，说白了，不求你看过那技术的源码，起码你要大概知道那个技术的基本原理、核心组成部分、基本架构构成，然后参照一些开源的技术把一个系统设计出来的思路说一下就好。 比如说这个消息队列系统，我们从以下几个角度来考虑一下： 首先这个 mq 得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下 kafka 的设计理念，broker -&gt; topic -&gt; partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？ 其次你得考虑一下这个 mq 的数据要不要落地磁盘吧？那肯定要了，落磁盘才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。 其次你考虑一下你的 mq 的可用性啊？这个事儿，具体参考之前可用性那个环节讲解的 kafka 的高可用保障机制。多副本 -&gt; leader &amp; follower -&gt; broker 挂了重新选举 leader 即可对外服务。 能不能支持数据 0 丢失啊？可以的，参考我们之前说的那个 kafka 数据零丢失方案。 mq 肯定是很复杂的，面试官问你这个问题，其实是个开放题，他就是看看你有没有从架构角度整体构思和设计的思维以及能力。确实这个问题可以刷掉一大批人，因为大部分人平时不思考这些东西。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何解决消息队列的延时以及过期失效问题？]]></title>
      <url>%2F2020%2F05%2F04%2F%E9%9D%A2%E8%AF%95%2F2020-5-4-%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E5%BB%B6%E6%97%B6%E4%BB%A5%E5%8F%8A%E8%BF%87%E6%9C%9F%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？ 面试官心理分析你看这问法，其实本质针对的场景，都是说，可能你的消费端出了问题，不消费了；或者消费的速度极其慢。接着就坑爹了，可能你的消息队列集群的磁盘都快写满了，都没人消费，这个时候怎么办？或者是这整个就积压了几个小时，你这个时候怎么办？或者是你积压的时间太长了，导致比如 RabbitMQ 设置了消息过期时间后就没了怎么办？ 所以就这事儿，其实线上挺常见的，一般不出，一出就是大 case。一般常见于，举个例子，消费端每次消费之后要写 mysql，结果 mysql 挂了，消费端 hang 那儿了，不动了；或者是消费端出了个什么岔子，导致消费速度极其慢。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/04/面试/2020-5-4-如何解决消息队列的延时以及过期失效问题？/访问原文「如何解决消息队列的延时以及过期失效问题？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析关于这个事儿，我们一个一个来梳理吧，先假设一个场景，我们现在消费端出故障了，然后大量消息在 mq 里积压，现在出事故了，慌了。 大量消息在 mq 里积压了几个小时了还没解决几千万条数据在 MQ 里积压了七八个小时，从下午 4 点多，积压到了晚上 11 点多。这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复 consumer 的问题，让它恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。 一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。 一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下： 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。 等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。 mq 中的消息过期失效了假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是大量的数据会直接搞丢。 这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。 假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。 mq 都快写满了如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何保证消息的顺序性？]]></title>
      <url>%2F2020%2F05%2F03%2F%E9%9D%A2%E8%AF%95%2F2020-5-3-%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%80%A7%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何保证消息的顺序性？ 面试官心理分析其实这个也是用 MQ 的时候必问的话题，第一看看你了不了解顺序这个事儿？第二看看你有没有办法保证消息是有顺序的？这是生产系统中常见的问题。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/03/面试/2020-5-3-如何保证消息的顺序性？/访问原文「如何保证消息的顺序性？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析我举个例子，我们以前做过一个 mysql binlog 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -&gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你愣是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景： RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。 Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案RabbitMQ拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 Kafka 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何保证消息的可靠性传输？]]></title>
      <url>%2F2020%2F05%2F02%2F%E9%9D%A2%E8%AF%95%2F2020-5-2-%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BC%A0%E8%BE%93%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？ 面试官心理分析这个是肯定的，用 MQ 有个基本原则，就是数据不能多一条，也不能少一条，不能多，就是前面说的重复消费和幂等性问题。不能少，就是说这数据别搞丢了。那这个问题你必须得考虑一下。 如果说你这个是用 MQ 来传递非常核心的消息，比如说计费、扣费的一些消息，那必须确保这个 MQ 传递过程中绝对不会把计费消息给弄丢。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/02/面试/2020-5-2-如何保证消息的可靠性传输？/访问原文「如何保证消息的可靠性传输？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析数据的丢失问题，可能出现在生产者、MQ、消费者中，咱们从 RabbitMQ 和 Kafka 分别来分析一下吧。 RabbitMQ 生产者弄丢了数据生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。 此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务channel.txSelect，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务channel.txRollback，然后重试发送消息；如果收到了消息，那么可以提交事务channel.txCommit。123456789101112// 开启事务channel.txSelecttry &#123; // 这里发送消息&#125; catch (Exception e) &#123; channel.txRollback // 这里再次重发这条消息&#125;// 提交事务channel.txCommit 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。 所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。 RabbitMQ 弄丢了数据就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。 设置持久化有两个步骤： 创建 queue 的时候将其设置为持久化这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。 第二个是发送消息的时候将消息的 deliveryMode 设置为 2就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。 必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。 注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。 所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack，你也是可以自己重发的。 消费端弄丢了数据RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。 这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。 Kafka消费端弄丢了数据唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。 Kafka 弄丢了数据这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。 生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。 所以此时一般是要求起码设置如下 4 个参数： 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。 我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。 生产者会不会弄丢数据？如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何保证消息不被重复消费？]]></title>
      <url>%2F2020%2F05%2F01%2F%E9%9D%A2%E8%AF%95%2F2020-5-1-%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E8%A2%AB%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？ 面试官心理分析其实这是很常见的一个问题，这俩问题基本可以连起来问。既然是消费消息，那肯定要考虑会不会重复消费？能不能避免重复消费？或者重复消费了也别造成系统异常可以吗？这个是 MQ 领域的基本问题，其实本质上还是问你使用消息队列如何保证幂等性，这个是你架构里要考虑的一个问题。 转载请注明出处：http://shenshanlaoyuan.com/2020/05/01/面试/2020-5-1-如何保证消息不被重复消费？/访问原文「如何保证消息不被重复消费？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析回答这个问题，首先你别听到重复消息这个事儿，就一无所知吧，你先大概说一说可能会有哪些重复消费的问题。 首先，比如 RabbitMQ、RocketMQ、Kafka，都有可能会出现消息重复消费的问题，正常。因为这问题通常不是 MQ 自己保证的，是由我们开发来保证的。挑一个 Kafka 来举个例子，说说怎么重复消费吧。 Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。 举个栗子。 有这么个场景。数据 1/2/3 依次进入 kafka，kafka 会给这三条数据每条分配一个 offset，代表这条数据的序号，我们就假设分配的 offset 依次是 152/153/154。消费者从 kafka 去消费的时候，也是按照这个顺序去消费。假如当消费者消费了 offset=153 的这条数据，刚准备去提交 offset 到 zookeeper，此时消费者进程被重启了。那么此时消费过的数据 1/2 的 offset 并没有提交，kafka 也就不知道你已经消费了 offset=153 这条数据。那么重启之后，消费者会找 kafka 说，嘿，哥儿们，你给我接着把上次我消费到的那个地方后面的数据继续给我传递过来。由于之前的 offset 没有提交成功，那么数据 1/2 会再次传过来，如果此时消费者没有去重的话，那么就会导致重复消费。 如果消费者干的事儿是拿一条数据就往数据库里写一条，会导致说，你可能就把数据 1/2 在数据库里插入了 2 次，那么数据就错啦。 其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。 举个例子吧。假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性。 一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性。 幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。 所以第二个问题来了，怎么保证消息队列消费的幂等性？ 其实还是得结合业务来思考，我这里给几个思路： 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。 当然，如何保证 MQ 的消费是幂等性的，需要结合具体的业务来看。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何保证消息队列的高可用？]]></title>
      <url>%2F2020%2F04%2F30%2F%E9%9D%A2%E8%AF%95%2F2020-4-30-%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题如何保证消息队列的高可用？ 面试官心理分析如果有人问到你 MQ 的知识，高可用是必问的。上一讲提到，MQ 会导致系统可用性降低。所以只要你用了 MQ，接下来问的一些要点肯定就是围绕着 MQ 的那些缺点怎么来解决了。 要是你傻乎乎的就干用了一个 MQ，各种问题从来没考虑过，那你就杯具了，面试官对你的感觉就是，只会简单使用一些技术，没任何思考，马上对你的印象就不太好了。这样的同学招进来要是做个 20k 薪资以内的普通小弟还凑合，要是做薪资 20k+ 的高工，那就惨了，让你设计个系统，里面肯定一堆坑，出了事故公司受损失，团队一起背锅。 转载请注明出处：http://shenshanlaoyuan.com/2020/04/30/面试/2020-4-30-如何保证消息队列的高可用？/访问原文「如何保证消息队列的高可用？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析这个问题这么问是很好的，因为不能问你 Kafka 的高可用性怎么保证？ActiveMQ 的高可用性怎么保证？一个面试官要是这么问就显得很没水平，人家可能用的就是 RabbitMQ，没用过 Kafka，你上来问人家 Kafka 干什么？这不是摆明了刁难人么。 所以有水平的面试官，问的是 MQ 的高可用性怎么保证？这样就是你用过哪个 MQ，你就说说你对那个 MQ 的高可用性的理解。 RabbitMQ 的高可用性RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。 RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。 单机模式单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的😄，没人生产用单机模式。 普通集群模式（无高可用性）普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。 这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。 而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让 RabbitMQ 落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。 所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。 镜像集群模式（高可用性）这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。 那么如何开启这个镜像集群模式呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。 这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？ Kafka 的高可用性Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。 这就是天然的分布式消息队列，就是说一个 topic 的数据，是分散放在多个机器上的，每个机器就放一部分数据。 实际上 RabbitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。 Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。 比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。 Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。 这么搞，就有所谓的高可用性了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中重新选举一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。 写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为） 消费的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。 看到这里，相信你大致明白了 Kafka 是如何保证高可用机制的了，对吧？不至于一无所知，现场还能给面试官画画图。要是遇上面试官确实是 Kafka 高手，深挖了问，那你只能说不好意思，太深入的你没研究过。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[为什么使用消息队列？]]></title>
      <url>%2F2020%2F04%2F29%2F%E9%9D%A2%E8%AF%95%2F2020-4-29-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[面试题 为什么使用消息队列？ 消息队列有什么优点和缺点？ Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别，以及适合哪些场景？ 面试官心理分析其实面试官主要是想看看： 第一，你知不知道你们系统里为什么要用消息队列这个东西？ 不少候选人，说自己项目里用了 Redis、MQ，但是其实他并不知道自己为什么要用这个东西。其实说白了，就是为了用而用，或者是别人设计的架构，他从头到尾都没思考过。 没有对自己的架构问过为什么的人，一定是平时没有思考的人，面试官对这类候选人印象通常很不好。因为面试官担心你进了团队之后只会木头木脑的干呆活儿，不会自己思考。 第二，你既然用了消息队列这个东西，你知不知道用了有什么好处&amp;坏处？ 你要是没考虑过这个，那你盲目弄个 MQ 进系统里，后面出了问题你是不是就自己溜了给公司留坑？你要是没考虑过引入一个技术可能存在的弊端和风险，面试官把这类候选人招进来了，基本可能就是挖坑型选手。就怕你干 1 年挖一堆坑，自己跳槽了，给公司留下无穷后患。 第三，既然你用了 MQ，可能是某一种 MQ，那么你当时做没做过调研？ 你别傻乎乎的自己拍脑袋看个人喜好就瞎用了一个 MQ，比如 Kafka，甚至都从没调研过业界流行的 MQ 到底有哪几种。每一个 MQ 的优点和缺点是什么。每一个 MQ 没有绝对的好坏，但是就是看用在哪个场景可以扬长避短，利用其优势，规避其劣势。 如果是一个不考虑技术选型的候选人招进了团队，leader 交给他一个任务，去设计个什么系统，他在里面用一些技术，可能都没考虑过选型，最后选的技术可能并不一定合适，一样是留坑。 转载请注明出处：http://shenshanlaoyuan.com/2020/04/29/面试/2020-4-29-为什么使用消息队列？/访问原文「为什么使用消息队列？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试题剖析为什么使用消息队列其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么？ 面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用 MQ 可能会很麻烦，但是你现在用了 MQ 之后带给了你很多的好处。 先说一下消息队列常见的使用场景吧，其实场景有很多，但是比较核心的有 3 个：解耦、异步、削峰。 解耦看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃…… 在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！ 如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。 总结：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。 面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。 异步再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。 一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。 如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！ 削峰每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。 一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。 但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。 如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。 这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。 消息队列有什么优缺点优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。 缺点有以下几个： 系统可用性降低 系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，ABCD 四个系统还好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整？MQ 一挂，整套系统崩溃，你不就完了？如何保证消息队列的高可用，可以点击这里查看。 系统复杂度提高 硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 一致性问题 A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。 所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。 Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？ 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[面试遇到连环炮该如何接招？]]></title>
      <url>%2F2020%2F04%2F28%2F%E9%9D%A2%E8%AF%95%2F%E9%9D%A2%E8%AF%95%E9%81%87%E5%88%B0%E8%BF%9E%E7%8E%AF%E7%82%AE%E8%AF%A5%E5%A6%82%E4%BD%95%E6%8E%A5%E6%8B%9B%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[消息队列面试场景面试官：你好。 候选人：你好。 （面试官在你的简历上面看到了，呦，有个亮点，你在项目里用过 MQ，比如说你用过 ActiveMQ） 面试官：你在系统里用过消息队列吗？（面试官在随和的语气中展开了面试） 候选人：用过的（此时感觉没啥） 面试官：那你说一下你们在项目里是怎么用消息队列的？ 候选人：巴拉巴拉，“我们啥啥系统发送个啥啥消息到队列，别的系统来消费啥啥的。比如我们有个订单系统，订单系统每次下一个新的订单的时候，就会发送一条消息到 ActiveMQ 里面去，后台有个库存系统负责获取消息然后更新库存。” （部分同学在这里会进入一个误区，就是你仅仅就是知道以及回答你们是怎么用这个消息队列的，用这个消息队列来干了个什么事情？） 面试官：那你们为什么使用消息队列啊？你的订单系统不发送消息到 MQ，直接订单系统调用库存系统一个接口，咔嚓一下，直接就调用成功，库存不也就更新了。 候选人：额。。。（楞了一下，为什么？我没怎么仔细想过啊，老大让用就用了），硬着头皮胡言乱语了几句。 （面试官此时听你楞了一下，然后听你胡言乱语了几句，开始心里觉得有点儿那什么了，怀疑你之前就压根儿没思考过这问题） 转载请注明出处：http://shenshanlaoyuan.com/2020/04/28/面试/面试遇到连环炮该如何接招？/访问原文「面试遇到连环炮该如何接招？」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 面试官：那你说说用消息队列都有什么优点和缺点？ （面试官此时心里想的是，你的 MQ 在项目里为啥要用，你没怎么考虑过，那我稍微简单点儿，我问问你消息队列你之前有没有考虑过如果用的话，优点和缺点分别是啥？） 候选人：这个。。。（确实平时没怎么考虑过这个问题啊。。。胡言乱语了） （面试官此时心里已经更觉得你这哥儿们不行，平时都没什么思考） 面试官：Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别？ （面试官问你这个问题，就是说，绕过比较虚的话题，直接看看你对各种 MQ 中间件是否了解，是否做过功课，是否做过调研） 候选人：我们就用过 ActiveMQ，所以别的没用过。。。区别，也不太清楚。。。 （面试官此时更是觉得你这哥儿们平时就是瞎用，根本就没什么思考，觉得不行） 面试官：那你们是如何保证消息队列的高可用啊？ 候选人：这个。。。我平时就是简单走 API 调用一下，不太清楚消息队列怎么部署的。。。 面试官：如何保证消息不被重复消费啊？如何保证消费的时候是幂等的啊？ 候选人：啥？（MQ 不就是写入&amp;消费就可以了，哪来这么多问题） 面试官：如何保证消息的可靠性传输啊？要是消息丢失了怎么办啊？ 候选人：我们没怎么丢过消息啊。。。 面试官：那如何保证消息的顺序性？ 候选人：顺序性？什么意思？我为什么要保证消息的顺序性？它不是本来就有顺序吗？ 面试官：如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？ 候选人：不是，我这平时没遇到过这些问题啊，就是简单用用，知道 MQ 的一些功能。 面试官：如果让你写一个消息队列，该如何进行架构设计啊？说一下你的思路。 候选人：。。。。。我还是走吧。。。。 这其实是面试官的一种面试风格，就是说面试官的问题不是发散的，而是从一个小点慢慢铺开。比如说面试官可能会跟你聊聊高并发话题，就这个话题里面跟你聊聊缓存、MQ 等等东西，由浅入深，一步步深挖。 其实上面是一个非常典型的关于消息队列的技术考察过程，好的面试官一定是从你做过的某一个点切入，然后层层展开深入考察，一个接一个问，直到把这个技术点刨根问底，问到最底层。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[慕课网高并发课程笔记（三）-项目准备]]></title>
      <url>%2F2018%2F05%2F05%2FJava%2Fbingfa3%2F</url>
      <content type="text"><![CDATA[线程安全与线程不安全 线程安全：代码所在的进程有多个线程在同时运行，这些线程可能会同时运行同一段代码，如果每次运行的结果和单线程运行的结果一样的，而且其他的变量的值也和预期是一样的，我们就认为这是线程安全的。简单的说，就是并发环境下得到我们期望的结果。 线程不安全：就是不提供数据访问保护，有可能出现多个线程先后更改数据，所得到的数据是脏数据，也可能是在计算中出现错误。 转载请注明出处：http://shenshanlaoyuan.com/2018/05/05/Java/bingfa3/访问原文「慕课网高并发课程笔记（三）-项目准备」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 环境搭建与准备Spring Boot构建项目Https://start.spring.io/ 具体搭建过程参考视频。 注解的准备 线程安全的注解 1234567891011/** * 对于线程安全的类，加入一个@ThreadSafe注解的标示 * @Target(ElementType.TYPE) 说明作用于类上 * @Retention(RetentionPolicy.SOURCE) 指定注解作用的范围，在编译的时候就会被忽略掉 * @author gaowenfeng */@Target(ElementType.TYPE)@Retention(RetentionPolicy.SOURCE)public @interface ThreadSafe &#123; String value() default "";&#125; 线程不安全的注解 1234567891011/** * 用来标示[线程不安全的类] * @Target(ElementType.TYPE) 说明作用于类上 * @Retention(RetentionPolicy.SOURCE) 指定注解作用的范围，在编译的时候就会被忽略掉 * @author gaowenfeng */@Target(ElementType.TYPE)@Retention(RetentionPolicy.SOURCE)public @interface NotThreadSafe &#123; String value() default "";&#125; 推荐写法的注解 12345678/** * 用来标记[推荐]的类或者写法 */@Target(ElementType.TYPE)@Retention(RetentionPolicy.SOURCE)public @interface Recommend &#123; String value() default "";&#125; 不推荐写法的注解 12345678/** * 用来标记[不推荐]的类或者写法 */@Target(ElementType.TYPE)@Retention(RetentionPolicy.SOURCE)public @interface NotRecommend &#123; String value() default "";&#125; ElementType 详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public enum ElementType &#123; /** Class, interface (including annotation type), or enum declaration */ /** 声明注解作用在类，借口，枚举上*/ TYPE, /** Field declaration (includes enum constants) */ /** 声明注解作用在属性上*/ FIELD, /** Method declaration */ /** 声明注解作用在方法上*/ METHOD, /** Formal parameter declaration */ /** 声明注解作用在参数上*/ PARAMETER, /** Constructor declaration */ /** 声明注解作用在构造函数上*/ CONSTRUCTOR, /** Local variable declaration */ /** 声明注解作用在本地变量上*/ LOCAL_VARIABLE, /** Annotation type declaration */ /** 声明注解作用在注解上*/ ANNOTATION_TYPE, /** Package declaration */ /** 声明注解作用在包上*/ PACKAGE, /** * Type parameter declaration * * @since 1.8 */ /** 声明注解可以应用在TYPE声明上*/ TYPE_PARAMETER, /** * Use of a type * Type.TYPE_USE 表示这个 Annotation 可以用在所有使用 Type 的地方（如：泛型，类型转换等） * @since 1.8 */ TYPE_USE&#125; RetentionPolicy详解1234567891011121314151617181920212223public enum RetentionPolicy &#123; /** * Annotations are to be discarded by the compiler. * 在编译的时候会被取消，只用于声明，理解，或者测试 */ SOURCE, /** * Annotations are to be recorded in the class file by the compiler * but need not be retained by the VM at run time. This is the default * behavior. * 注解将被编译器记录在类文件中，但在运行时不需要由VM保留，（默认的选项） */ CLASS, /** * Annotations are to be recorded in the class file by the compiler and * retained by the VM at run time, so they may be read reflectively. * 注解将被编译器记录在类文件中，但在运行时由VM保留，这样他们可以被反射获取（当你需要获取注解中字段的属性值的时候，需要用这个，比如AOP） * @see java.lang.reflect.AnnotatedElement */ RUNTIME&#125; 并发模拟工具 并发模拟代码]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[慕课网高并发课程笔记（二）-并发基础]]></title>
      <url>%2F2018%2F05%2F04%2FJava%2Fbingfa2%2F</url>
      <content type="text"><![CDATA[CPU多级缓存 图左侧为最简单的高速缓存的配置，数据的读取和存储都经过高速缓存，CPU核心与高速缓存有一条特殊的快速通道；主存与高速缓存都连在系统总线上（BUS）这条总线还用于其他组件的通信。 在高速缓存出现后不久，系统变得越来越复杂，高速缓存与主存之间的速度差异被拉大，直到加入了另一级缓存，新加入的这级缓存比第一缓存更大，并且更慢，由于加大一级缓存从经济上考虑是行不通的，所以有了二级缓存，甚至是三级缓存。 转载请注明出处：http://shenshanlaoyuan.com/2018/05/04/Java/bingfa2/访问原文「慕课网高并发课程笔记（二）-并发基础」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 为什么需要CPU缓存？CPU的频率太快了，快到主存跟不上，这样在处理器时钟周期内，CPU常常需要等待主存，浪费资源，所以cache的出现，是为了缓解CPU和内存之间速度的不匹配问题（结构：cpu-&gt;cache-&gt;memort）。 CPU缓存有什么意义？ 时间局部性：如果某个数据被访问，那么在不久的将来他很可能被再次访问 空间局部性：如果某个数据被访问，那么与他相邻的数据很快也可能被访问 CPU多级缓存-缓存一致性（MESI） M: Modified 修改，指的是该缓存行只被缓存在该CPU的缓存中，并且是被修改过的，因此他与主存中的数据是不一致的，该缓存行中的数据需要在未来的某个时间点（允许其他CPU读取主存相应中的内容之前）写回主存，然后状态变成E（独享） E：Exclusive 独享 ，缓存行只被缓存在该CPU的缓存中，是未被修改过的，与主存的数据是一致的，可以在任何时刻当有其他CPU读取该内存时，变成S（共享）状态，当CPU修改该缓存行的内容时，变成M（被修改）的状态 S：Share 共享，意味着该缓存行可能会被多个CPU进行缓存，并且该缓存中的数据与主存数据是一致的，当有一个CPU修改该缓存行时，其他CPU是可以被作废的，变成I(无效的) I：Invalid 无效的，代表这个缓存是无效的，可能是有其他CPU修改了该缓存行 用于保证多个CPU cache之间缓存共享数据的一致 local read：读本地缓存的数据 local write：将数据写到本地缓存里面 remote read：将内（主）存中的数据读取到缓存中来 remote write：将缓存中的数据写会到主存里面 在一个典型的多核系统中，每一个核都会有自己的缓存来共享主存总线，每一个CPU会发出读写（I/O）请求，而缓存的目的是为了减少CPU读写共享主存的次数；一个缓存除了在Invaild状态，都可以满足CPU 的读请求 一个写请求只有在M状态，或者E状态的时候才能给被执行，如果是处在S状态的时候，他必须先将该缓存行变成I状态，这个操作通常作用于广播的方式来完成，这个时候他既不允许不同的CPU同时修改同一个缓存行，即使是修改同一个缓存行中不同端的数据也是不可以的，这里主要解决的是缓存一致性的问题，一个M状态的缓存行必须时刻监听所有试图读该缓存行相对主存的操作，这种操作必须在缓存该缓存行被写会到主存，并将状态变成S状态之前，被延迟执行 一个处于S状态的缓存行，也必须监听其他缓存使该缓存行无效，或者独享该缓存行的请求，并将缓存行变成无效 一个处于E状态的缓存行，他要监听其他缓存读缓存行的操作，一旦有，那么他讲变成S状态 因此对于M和E状态，他们的数据总是一致的与缓存行的真正状态总是保持一致的，但是S状态可能是非一致的，如果一个缓存将处于S状态的 缓存行作废了，另一个缓存可能已经独享了该缓存行，但是该缓存缺不会讲该缓存行升迁为E状态，这是因为其他缓存不会广播他们已经作废掉该缓存行的通知，同样由于缓存并没有保存该缓存行被COPY的数量，因此没有办法确定是否独享了改缓存行，这是一种投机性的优化，因为如果一个CPU想修改一个处于S状态的缓存行，总线需要将所有使用该缓存行的COPY的值变成Invaild状态才可以，而修改E状态的缓存 却不需要这样做 CPU多级缓存-乱序执行优化 处理器多核和缓存导致的一个问题，如果我们不做任何处理，在多核的情况下，的实际结果可能和逻辑运行结果大不相同，如果在一个核上执行数据写入操作，并在最后执行一个操作来标记数据已经写入好了，而在另外一个核上通过该标记位判定数据是否已经写入，这时候就可能出现不一致，标记位先被写入，但是实际的操作缺并未完成，这个未完成既有可能是没有计算完成，也有可能是缓存没有被及时刷新到主存之中，使得其他核读到了错误的数据 Java内存模型（Java Memory Model，JMM）JAVA内存模型规范规定了一个线程如何和何时可以看到其他线程修改过后的共享变量的值 以及何时同步的访问共享变量。 Java内存模型 Heap(堆)：java里的堆是一个运行时的数据区，堆是由垃圾回收来负责的，堆的优势是可以动态的分配内存大小，生存期也不必事先告诉编译器，因为他是在运行时动态分配内存的，java的垃圾回收器会定时收走不用的数据， 缺点是由于要在运行时动态分配，所有存取速度可能会慢一些 Stack(栈)：栈的优势是存取速度比堆要快，仅次于计算机里的寄存器，栈的数据是可以共享的，缺点是存在栈中的数据的大小与生存期必须是确定的，缺乏一些灵活性。栈中主要存放一些基本类型的变量，比如int，short，long，byte，double，float，boolean，char，对象句柄， java内存模型要求调用栈和本地内存变量存放在线程栈（Thread Stack）上，对象存放在堆上。一个本地变量可能存放一个对象的引用，这时引用变量存放在本地栈上，但是对象本身存放在堆上 成员变量跟随着对象存放在堆上，而不管是原始类型还是引用类型，静态成员变量跟随着类的定义一起存在在堆上 存在堆上的对象，可以被持有这个对象的引用的线程访问 如果两个线程同时访问同一个对象的私有变量，这时他们获得的是这个对象的私有拷贝 计算机硬件架构 CPU：一个计算机一般有多个CPU，一个CPU还会有多核 CPU Registers（寄存器）：每个CPU都包含一系列的寄存器，他们是CPU内存的基础，CPU在寄存器上执行的速度远大于在主存上执行的速度。 CPU Cache（高速缓存）：由于计算机的存储设备与处理器的处理设备有着几个数量级的差距，所以现代计算机都会加入一层读写速度与处理器处理速度接近想通的高级缓存来作为内存与处理器之间的缓冲，将运算使用到的数据复制到缓存中，让运算能够快速的执行，当运算结束后，再从缓存同步到内存之中，这样，CPU就不需要等待缓慢的内存读写了 主（内）存：一个计算机包含一个主存，所有的CPU都可以访问主存，主存比缓存容量大的多 运作原理：通常情况下，当一个CPU要读取主存的时候，他会将主存中的数据读取到CPU缓存中，甚至将缓存中的内容读到内部寄存器里面，然后再寄存器执行操作，当运行结束后，会将寄存器中的值刷新回缓存中，并在某个时间点刷新回主存 内存模型与硬件架构之间的关联 所有线程栈和堆会被保存在缓存里面，部分可能会出现在CPU缓存中和CPU内部的寄存器里面 线程和主内存之间的抽象关系 每个线程之间共享变量都存放在主内存里面，每个线程都有一个私有的本地内存本地内存是java内存模型中抽象的概念，并不是真实存在的（他涵盖了缓存写缓冲区。寄存器，以及其他硬件的优化），本地内存中存储了以读或者写共享变量的拷贝的一个副本，从一个更低的层次来说，线程本地内存，他是cpu缓存，寄存器的一个抽象描述，而JVM的静态内存存储模型，他只是一种对内存模型的物理划分而已，只局限在内存，而且只局限在JVM的内存 如果线程A和线程B要通信，必须经历两个过程：1、A将本地内存变量刷新到主内存2、B从主内存中读取变量 Java内存模型-同步八种操作及规则 lock（锁定）：作用于主内存的变量，把一个变量标识变为一条线程独占状态 unlock（解锁）：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定 read（读取）：作用于主内存的变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接受到的值赋值给工作内存的变量 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中 同步规则： 如果要把一个变量从主内存中赋值到工作内存，就需要按顺序得执行read和load操作，如果把变量从工作内存中同步回主内存中，就要按顺序得执行store和write操作，但java内存模型只要求上述操作必须按顺序执行，没有保证必须是连续执行 不允许read和load、store和write操作之一单独出现 不允许一个线程丢弃他的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步到主内存中 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了load和assign操作 一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以同时被一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会解锁，lock和unlock必须成对出现 如果一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎中使用这个变量前需要重新执行load或assign操作初始化变量的值 如果一个变量事先没有被lock操作锁定，则不允许他执行unlock操作，也不允许去unlock一个被其他线程锁定的变量 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作） 并发的优势和风险]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[慕课网高并发课程笔记（一）-并发和高并发基本概念]]></title>
      <url>%2F2018%2F05%2F01%2FJava%2Fbingfa1%2F</url>
      <content type="text"><![CDATA[课程介绍 课程结合大量图示及代码演示，让你更容易， 更系统的掌握多线程并发编程（线程安全，线程调度，线程封闭，同步容器等）与高并发处理思路与手段（扩容，缓存，队列，拆分等）相关知识和经验。帮助你构建完整的并发与高并发知识体系，胜任实际开发中并发与高并发问题的处理，倍增高薪面试成功率！ 转载请注明出处：http://shenshanlaoyuan.com/2018/05/01/Java/bingfa1/访问原文「慕课网高并发课程笔记（一）-并发和高并发基本概念」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 课程地址慕课网 百度网盘密码arkw （资源来源于网络，如有侵权联系删除！） 基本概念并发同时拥有两个或者多个线程，如果程序在单核处理器上运行，多个线程交替得换入或者换出内存，这些线程是同时“存在”的，每个线程都处于执行过程中的某个状态，如果运行在多核处理器上，此时，程序中的每个线程都将分配到一个处理器核上，因此可以同时运行。 高并发高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，他通常是指，通过设计保证系统能够同时并行处理很多请求。 并发和高并发对比 谈并发时：多个线程操作相同的资源，保证线程安全，合理利用资源 谈高并发时：服务能同时处理很多请求（如12306的抢票，天猫双十一的秒杀活动，这会导致系统在短时间内执行大量的操作，如对资源的请求，数据库的访问），提高程序性能（如果高并发处理不好，不光会导致用户体验不好，还可能会使服务器宕机，出现OOM等） 课程内容]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python入门]]></title>
      <url>%2F2018%2F04%2F07%2FPython%2Fpython%E5%85%A5%E9%97%A8%2F</url>
      <content type="text"><![CDATA[Python入门系列电子书，搭配廖雪峰的Python教程学习效果更佳。 转载请注明出处：http://shenshanlaoyuan.com/2018/04/07/Python/python入门/访问原文「Python入门」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() Python基础Linux操作系统基础 Python语法基础 Python核心编程Python高级 系统编程 网络编程 web服务器案例 正则表达式 数据结构与算法数据结构与算法 数据库MySQL mongo redis 前端前端 DjangoDjango 爬虫爬虫 TornadoTornado shell和自动化运维shell nginx 自动化部署]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux(CentOS) 下安装 ZooKeeper]]></title>
      <url>%2F2017%2F05%2F07%2F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2FCentOS-install-ZooKeeper%2F</url>
      <content type="text"><![CDATA[1.单机模式(Standalone mode)下载解压官网下载或使用 wget 命令下载12# wget http://archive.apache.org/dist/zookeeper/stable/zookeeper-3.4.9.tar.gz# tar zxf zookeeper-3.4.9.tar.gz 如果 wget 命令找不到，先安装 wget 的 RPM 包1# yum -y install wget 转载请注明出处：http://shenshanlaoyuan.com/2017/05/07/环境搭建/CentOS-install-ZooKeeper/访问原文「Linux(CentOS) 下安装 ZooKeeper」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 启动 ZooKeeper进入 zookeeper 解压后的目录12# cd zookeeper-3.4.9# ll 在目录下面创建 data 目录/root/zookeeper-3.4.9/data1# mkdir data 修改 conf 目录下面的 zoo_sample.cfg 配置文件名字 1# mv conf/zoo_sample.cfg conf/zoo.cfg 再修改配置文件内容 1# vim conf/zoo.cfg 如果没找到vim 命令先安装下1# yum -y install vim* 把 dataDir 后面改成刚才创建的 data 目录全路径，如下图 进入bin目录开启 ZooKeeper12# cd bin# ./zkServer.sh start 查看是否开启开启成功1# ./zkServer.sh status 出现如下内容开启成功123ZooKeeper JMX enabled by defaultUsing config: /root/zookeeper-3.4.9/bin/../conf/zoo.cfgMode: standalone 关闭 ZooKeeper进入 bin 目录输入命令 1# ./zkServer.sh stop 2.集群模式(Replicated mode)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[解决 CentOS 安装完成后 ifconfig 命令不能用问题]]></title>
      <url>%2F2017%2F05%2F07%2F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F%E8%A7%A3%E5%86%B3CentOS7%E5%AE%89%E8%A3%85%E5%AE%8C%E6%88%90%E5%90%8Eifconfig%E5%91%BD%E4%BB%A4%E4%B8%8D%E8%83%BD%E7%94%A8%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[本来想查看一下本机的 ip 地址，ifconfig 命令报错12# ifconfig-bash: ifconfig: command not found 转载请注明出处：http://shenshanlaoyuan.com/2017/05/07/环境搭建/解决CentOS7安装完成后ifconfig命令不能用问题/访问原文「解决 CentOS 安装完成后 ifconfig 命令不能用问题」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 解决思路1.查看 ifconfig 命令是否存在查看 /sbin/ifconfig 是否存在 2.如果 ifconfig 命令不存在执行下面两个命令安装12# yum upgrade# yum install net-tools 3.如果 ifconfig 命令存在，设置环境变量 临时修改环境变量： 1# export PATH = $PATH:/sbin 永久修改环境变量只要修改 /etc/profile 文件即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mac OS X下Maven的安装与配置]]></title>
      <url>%2F2017%2F05%2F06%2F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2FMacOS-Maven%2F</url>
      <content type="text"><![CDATA[Maven 下载官网下载压缩包,选择 Binary zip archive 转载请注明出处：http://shenshanlaoyuan.com/2017/05/06/环境搭建/MacOS-Maven/访问原文「Mac OS X下Maven的安装与配置」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 打开终端，输入unzip和拖动这个压缩文件到终端，解压1$ unzip /Users/shenshanlaoyuan/Downloads/apache-maven-3.5.0-bin.zip 输入ls,会看到apache-maven-3.5.0这个文件夹 cd进入到这个文件夹1$ cd apache-maven-3.5.0 再输入pwd命令得到当前文件夹全路径，选中复制1$ pwd 配置环境变量打开终端，输入以下命令，编辑bash_profile文件1$ vi ~/.bash_profile 添加以下代码在最后,M2_HOME=后面改成刚才复制的文件夹全路径123# mavenexport M2_HOME=/Users/shenshanlaoyuan/apache-maven-3.5.0export PATH=$PATH:$M2_HOME/bin 编辑完成按ESC键跳到命令模式，输入:wq就能保存并退出 vi。 最后输入如下命令以使修改的bash_profile文件生效1$ source ~/.bash_profile 查看是否安装成功1$ mvn -v 如果输出以下信息，说明 maven 安装成功了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring 框架之bean标签属性]]></title>
      <url>%2F2017%2F03%2F04%2FJava%2Fspring-bean-tag%2F</url>
      <content type="text"><![CDATA[主要有id、class、scope、init-method、destroy-method等。 转载请注明出处：http://shenshanlaoyuan.com/2017/03/04/Java/spring-bean-tag/访问原文「Spring 框架之bean标签属性」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() id属性和name属性的区别idBean起个名字，在约束中采用ID的约束，唯一 取值要求：必须以字母开始，可以使用字母、数字、连字符、下划线、句话、冒号 id:不能出现特殊字符 nameBean起个名字，没有采用ID的约束 取值要求：name:出现特殊字符.如果没有id的话 , name可以当做id使用 Spring框架在整合Struts1的框架的时候，Struts1的框架的访问路径是以 / 开头的，例如：/bookAction class属性Bean对象的全路径 scope属性scope属性代表Bean的作用范围 singleton – 单例（默认值） prototype – 多例，在Spring框架整合Struts2框架的时候，Action类也需要交给Spring做管理，配置把Action类配置成多例！！ request – 应用在Web项目中,每次HTTP请求都会创建一个新的Bean session – 应用在Web项目中,同一个HTTP Session 共享一个Bean globalsession – 应用在Web项目中,多服务器间的session Bean对象的创建和销毁的两个属性配置Spring初始化bean或销毁bean时，有时需要作一些处理工作，因此spring可以在创建和拆卸bean的时候调用bean的两个生命周期方法 init-method当bean被载入到容器的时候调用init-method属性指定的方法 destroy-method当bean从容器中删除的时候调用destroy-method属性指定的方法 想查看destroy-method的效果，有如下条件 scope= singleton有效 web容器中会自动调用，但是main函数或测试用例需要手动调用（需要使用ClassPathXmlApplicationContext的close()方法）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring 框架之Bean工厂]]></title>
      <url>%2F2017%2F03%2F04%2FJava%2Fspring-bean-factory%2F</url>
      <content type="text"><![CDATA[Spring 通过 Bean 工厂创建 Bean 对象。分别是ApplicationContext 和 BeanFactory 。 转载请注明出处：http://shenshanlaoyuan.com/2017/03/04/Java/spring-bean-factory/访问原文「Spring 框架之Bean工厂」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() ApplicationContext接口使用该工厂接口可以获取到具体的Bean对象。该接口下有两个具体的实现类 ClassPathXmlApplicationContext – 加载类路径下的Spring配置文件 FileSystemXmlApplicationContext – 加载本地磁盘下的Spring配置文件 BeanFactory工厂Spring框架早期的创建Bean对象的工厂接口 使用BeanFactory接口也可以获取到Bean对象12345public void run()&#123; BeanFactory factory = new XmlBeanFactory(new ClassPathResource("applicationContext.xml")); UserService us = (UserService) factory.getBean("us"); us.sayHello();&#125; BeanFactory 和 ApplicationContext 的区别 BeanFactory – BeanFactory 采取延迟加载，第一次getBean时才会初始化Bean ApplicationContext – 在加载applicationContext.xml时候就会创建具体的Bean对象的实例，还提供了一些其他的功能 事件传递 Bean自动装配 各种不同应用层的Context实现]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring 框架之IoC]]></title>
      <url>%2F2017%2F03%2F04%2FJava%2Fspring-IoC%2F</url>
      <content type="text"><![CDATA[什么是IoC?Inversion of Control，控制反转,英文缩写为IoC。将对象的创建权交给了Spring。 使用IoC可以解决程序耦合性高的问题。 转载请注明出处：http://shenshanlaoyuan.com/2017/03/04/Java/spring-IoC/访问原文「Spring 框架之IoC」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 简单IoC示例步骤一:下载Spring的开发包官网或者下载地址下载 解压后目录结构: docs :API和开发规范. libs :jar包和源码. schema :约束. 步骤二:创建web项目,引入Spring的开发包 步骤三:创建Spring配置文件在src目录下创建applicationContext.xml文件123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;!-- 使用bean标签 --&gt; &lt;bean id="userService" class="com.shenshanlaoyuan.demo.UserServiceImpl"&gt; &lt;/bean&gt;&lt;/beans&gt; 步骤四:编写相关的类123public interface UserService &#123; public void sayHello();&#125; 12345678public class UserServiceImpl implements UserService &#123; @Override public void sayHello() &#123; System.out.println("hello spring"); &#125;&#125; 步骤五:配置Bean在applicationContext.xml文件beans标签下添加bean12&lt;bean id="userService" class="com.shenshanlaoyuan.demo.UserServiceImpl"&gt;&lt;/bean&gt; 步骤六:编写测试类1234567891011121314151617181920212223/** * 测试类 * @author shenshanlaoyuan * */public class TestService &#123; //原来的方式 @Test public void testSayHello1()&#123; UserServiceImpl usi = new UserServiceImpl(); usi.sayHello(); &#125; //spring方式 @Test public void testSayHello2()&#123; //创建工厂，加载配置文件 ApplicationContext ac = new ClassPathXmlApplicationContext("applicationContext.xml"); //从工厂中获取到对象 UserService us = (UserService) ac.getBean("userService"); //调用对象的方法 us.sayHello(); &#125;&#125; 示例源代码]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring 框架之框架概述]]></title>
      <url>%2F2017%2F03%2F03%2FJava%2Fspring-summary%2F</url>
      <content type="text"><![CDATA[什么是 Spring？ Spring 是一个开源框架。 Spring 是于2003 年兴起的一个轻量级的 Java 开发框架，由 Rod Johnson 在其著作 Expert One-On-One J2EE Development and Design 中阐述的部分理念和原型衍生而来。 它是为了解决企业应用开发的复杂性而创建的。框架的主要优势之一就是其分层架构，分层架构允许使用者选择使用哪一个组件，同时为 J2EE 应用程序开发提供集成的框架。 Spring 使用基本的 JavaBean 来完成以前只可能由EJB完成的事情。然而，Spring 的用途不仅限于服务器端的开发。从简单性、可测试性和松耦合的角度而言，任何 Java 应用都可以从 Spring 中受益。 Spring 的核心是控制反转（IoC）和面向切面（AOP）。简单来说，Spring 是一个分层的 JavaSE/EEfull-stack(一站式) 轻量级开源框架。 转载请注明出处：http://shenshanlaoyuan.com/2017/03/03/Java/spring-summary/访问原文「Spring 框架之框架概述」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 为什么用 Spring？ 方便解耦，简化开发 Spring就是一个大工厂，可以将所有对象创建和依赖关系维护，交给Spring管理AOP编程的支持 AOP编程的支持 Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能 声明式事务的支持 只需要通过配置就可以完成对事务的管理，而无需手动编程 方便程序的测试 Spring对Junit4支持，可以通过注解方便的测试Spring程序 方便集成各种优秀框架 Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz等）的直接支持 降低JavaEE API的使用难度 Spring 对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[引入开源项目的正确姿势]]></title>
      <url>%2F2017%2F02%2F27%2FAndroid%2F%E5%BC%95%E5%85%A5%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF%2F</url>
      <content type="text"><![CDATA[为什么要用开源项目？软件开发领域一直有一个原则，DRY，Don’t repeat yourself，翻译过来就是”不要重复造轮子“。一个项目的开发，我们不可能一切从 0 开始，如果真要那样··· 开源项目的主要目的是共享，其实就为了让大家不要重复造轮子，尤其在互联网这个快速发展的领域，速度就是生命，引入开源项目，可以节省大量的人力和时间。 虽说开源项目为我们节省了大量的人力和时间，但是开源项目并不是完美的，相信使用过开源项目的人都大大小小踩过一些坑，如代码不规范啊，项目有 bug 啊等等，出了问题都会为我们的项目以及公司带来不小的影响，这个时候如何选择开源项目就变得很重要。 转载请注明出处：http://shenshanlaoyuan.com/2017/02/27/Android/引入开源项目的正确姿势/访问原文「引入开源项目的正确姿势」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 如何选择开源项目1.看 Star 数一般来说都会选则按 Star 数来排序，当然 Star 数高不代表是完美的，但起码说明该项目蛮火的，不然也不会那么多人 Star 的。 2.看作者如 JakeWharton 大神、Facebook 团队等。大神和大公司出品的框架质量相对较高，可保证后续的维护和 Bug 修复，不容易烂尾。 3.看最后更新时间、Issues、Fork 等GitHub上有些的项目好几个月甚至一年没更新了，对于一个开源项目来说最怕的是作者不维护了，这就意味着之后再也不会有改进了，而且出了什么问题也很难被迅速解决。 总结对于开源项目的选择，没有哪个最好的，你只有在综合评估的指标下，选择一个相对来说成熟并且适合你自己的就好了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HTML5新增的元素和移除的元素]]></title>
      <url>%2F2016%2F10%2F01%2FWeb%2FHTML5%E6%96%B0%E5%A2%9E%E7%9A%84%E5%85%83%E7%B4%A0%E5%92%8C%E7%A7%BB%E9%99%A4%E7%9A%84%E5%85%83%E7%B4%A0%2F</url>
      <content type="text"><![CDATA[为了更好地处理今天的互联网应用，HTML5添加了很多新元素及功能，比如: 图形的绘制，多媒体内容，更好的页面结构，更好的形式 处理，和几个api拖放元素，定位，包括网页 应用程序缓存，存储，网络工作者，等。 转载请注明出处：http://shenshanlaoyuan.com/2016/10/01/Web/HTML5新增的元素和移除的元素/访问原文「HTML5新增的元素和移除的元素」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 新元素新的语义和结构元素 元素 描述 &lt;article&gt; 定义独立的内容，内容本身必须是有意义的且必须是独立于文档的其余部分。 &lt;aside&gt; 定义页面的侧边栏内容。 &lt;header&gt; 定义了文档的头部区域。 &lt;footer&gt; 定义文档或者文档的一部分区域的页脚。 &lt;section&gt; 定义了文档的某个区域。比如章节、头部、底部或者文档的其他区域。 &lt;time&gt; 定义公历的时间（24 小时制）或日期，时间和时区偏移是可选的。 &lt;details&gt; 规定了用户可见的或者隐藏的需求的补充细节。标签用来供用户开启关闭的交互式控件。任何形式的内容都能被放在 &lt;details&gt; 标签里边。元素的内容对用户是不可见的，除非设置了 open 属性。 &lt;bdi&gt; 标签允许您设置一段文本，使其脱离其父元素的文本方向设置。在发布用户评论或其他您无法完全控制的内容时，该标签很有用。 &lt;summary&gt; 标签为 &lt;details&gt;元素定义一个可见的标题。 当用户点击标题时会显示出详细信息。 &lt;command&gt; 定义命令按钮，比如单选按钮、复选框或按钮。 &lt;dialog&gt; 定义对话框，比如提示框。 &lt;figure&gt; 标签规定独立的流内容（图像、图表、照片、代码等等）。元素的内容应该与主内容相关，同时元素的位置相对于主内容是独立的。如果被删除，则不应对文档流产生影响。 &lt;figcaption&gt; 定义&lt;figure&gt;元素的标题 &lt;mark&gt; 定义带有记号的文本。 &lt;meter&gt; 标签定义度量衡。仅用于已知最大和最小值的度量。比如：磁盘使用情况，查询结果的相关性等。 &lt;nav&gt; 标签定义导航链接的部分。并不是所有的HTML文档都要使用到 &lt;nav&gt; 元素。&lt;nav&gt;元素只是作为标注一个导航链接的区域。 &lt;progress&gt; 标签定义运行中的任务进度（进程）。 &lt;rt&gt; 定义字符（中文注音或字符）的解释或发音。 新表单元素 标签 描述 &lt;datalist&gt; 标签规定了 &lt;input&gt; 元素可能的选项列表。标签被用来在为 &lt;input&gt; 元素提供”自动完成”的特性。用户能看到一个下拉列表，里边的选项是预先定义好的，将作为用户的输入数据。 &lt;keygen&gt; 标签规定用于表单的密钥对生成器字段。当提交表单时，私钥存储在本地，公钥发送到服务器。 &lt;output&gt; 标签作为计算结果输出显示(比如执行脚本的输出)。 新多媒体元素 标签 描述 &lt;audio&gt; 标签定义声音，比如音乐或其他音频流。&lt;audio&gt;元素支持的3种文件格式：MP3、Wav、Ogg。 &lt;video&gt; 标签定义视频，比如电影片段或其他视频流。目前，&lt;video&gt; 元素支持三种视频格式：MP4、WebM、Ogg。 &lt;source&gt; 标签为媒体元素（比如 &lt;video&gt;和 &lt;audio&gt;）定义媒体资源。 &lt;embed&gt; 标签定义了一个容器，用来嵌入外部应用或者互动程序（插件）。 &lt;track&gt; 标签为媒体元素（比如 &lt;audio&gt;and&lt;video&gt;）规定外部文本轨道。这个元素用于规定字幕文件或其他包含文本的文件，当媒体播放时，这些文件是可见的。 &lt;canvas&gt;元素标签通过脚本（通常是 JavaScript）来绘制图形（比如图表和其他图像）。&lt;canvas&gt;标签只是图形容器，必须使用脚本来绘制图形。 移除的元素 &lt;acronym&gt; &lt;applet&gt; &lt;basefont&gt; &lt;big&gt; &lt;center&gt; &lt;dir&gt; &lt;font&gt; &lt;frame&gt; &lt;frameset&gt; &lt;noframes&gt; &lt;strike&gt; &lt;tt&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[解决GitHub Pages自有域名二级目录无法访问问题]]></title>
      <url>%2F2016%2F09%2F26%2FHexo%2F%E8%A7%A3%E5%86%B3GitHub%20Pages%E8%87%AA%E6%9C%89%E5%9F%9F%E5%90%8D%E4%BA%8C%E7%BA%A7%E7%9B%AE%E5%BD%95%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[问题描述通常新建个项目，通过GitHub上创建gh-pages分支可以访问项目主页。然而我访问usename.github.io/repo,或者自定义域名 yoursite.com/repo ，跳转到了 404 页面。 分析自己把博客同时放在 GitHub 和 Coding上，通过 DNS 做了不同线路解析，通过 CNAME 文件绑定了自己的自定义域名。GitHub 可以创建一个个人主页和多个项目主页，一般这样访问是没有问题。我的问题出现在域名解析这，默认线路设置为解析到 Coding 了，显然 Coding 上找不到 GitHub 创建的项目主页，就去博客目录下去找 /repo 目录，肯定找不到啦。 转载请注明出处：http://shenshanlaoyuan.com/2016/09/26/Hexo/解决GitHub Pages自有域名二级目录无法访问问题/访问原文「解决GitHub Pages自有域名二级目录无法访问问题」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 解决方法 我这个问题在域名解析商DNSPAD把解析线路默认改为GitHub就可以了 也可以在 DNS解析增加一条 CNAME 记录，通过 repo.yoursite.com 访问 还可以把项目主页放到博客themes/themes-name/source目录下 有问题欢迎下面留言]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[该重新学习下前端知识了]]></title>
      <url>%2F2016%2F09%2F24%2F%E9%9A%8F%E7%AC%94%2F%E8%AF%A5%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%89%8D%E7%AB%AF%E7%9F%A5%E8%AF%86%E4%BA%86%2F</url>
      <content type="text"><![CDATA[前几天微博朋友圈被微信小程序刷屏了。虽说现在只是内测，但微博Q群里都讨论炸了，可见微信对移动互联网影响多大。 转载请注明出处：http://shenshanlaoyuan.com/2016/09/24/随笔/该重新学习下前端知识了/访问原文「该重新学习下前端知识了」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 什么是微信小程序？微信之父张小龙是这样说的： 小程序会取代原生？微博群里讨论的激烈，无非是担心自己会不会失业。小程序一经推出，各种预言家出来了，说App将被颠覆，原生已死。在我看来，远没到那个地步。就像H5刚出来的时候就有人说会取代原生应用，然后直到现在原生APP还活的好好的，就连目前大火的ReactNative还有很多不完善的地方。取代原生的开发不可能，毕竟有很多的APP在微信小程序还是有很多限制的，比如游戏，一些大型APP。不过也要开始学习下Javascript了，毕竟多学一项技能就会多一份竞争力，总会有好处的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Banner轮播图效果实现]]></title>
      <url>%2F2016%2F09%2F02%2FAndroid%2FBanner%E8%BD%AE%E6%92%AD%E5%9B%BE%E6%95%88%E6%9E%9C%E5%AE%9E%E7%8E%B0%2F</url>
      <content type="text"><![CDATA[项目中常常需要用到如下图的 Banner 广告轮播图的效果,利用 ViewPager 实现。 转载请注明出处：http://shenshanlaoyuan.com/2016/09/02/Android/Banner轮播图效果实现/访问原文「Banner轮播图效果实现」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 布局123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;RelativeLayout xmlns:android="http://schemas.android.com/apk/res/android" xmlns:tools="http://schemas.android.com/tools" android:layout_width="match_parent" android:layout_height="match_parent" tools:context="com.shenshanlaoyuan.viewpagertest.MainActivity" &gt; &lt;RelativeLayout android:layout_width="match_parent" android:layout_height="180dp" &gt; &lt;android.support.v4.view.ViewPager android:id="@+id/pager" android:layout_width="match_parent" android:layout_height="180dp" &gt; &lt;/android.support.v4.view.ViewPager&gt; &lt;LinearLayout android:layout_width="match_parent" android:layout_height="wrap_content" android:layout_alignParentBottom="true" android:background="#33000000" android:orientation="vertical" android:padding="5dp" &gt; &lt;TextView android:id="@+id/tv_title" android:layout_width="wrap_content" android:layout_height="wrap_content" android:layout_gravity="center_horizontal" android:text="图片的title" android:textColor="@android:color/white" /&gt; &lt;!-- 装圆点的容器 --&gt; &lt;LinearLayout android:id="@+id/point_container" android:layout_width="wrap_content" android:layout_height="wrap_content" android:layout_gravity="center_horizontal" android:layout_marginTop="5dp" android:orientation="horizontal" &gt; &lt;/LinearLayout&gt; &lt;/LinearLayout&gt; &lt;/RelativeLayout&gt;&lt;/RelativeLayout&gt; 圆点资源目录 res 下新建 drawable 目录，创建两个圆点 shape 文件 point_normal.xml 文件内容123456&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;shape xmlns:android="http://schemas.android.com/apk/res/android" android:shape="oval" &gt; &lt;corners android:radius="5dp" /&gt; &lt;solid android:color="@android:color/white" /&gt;&lt;/shape&gt; point_selected.xml 文件内容123456&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;shape xmlns:android="http://schemas.android.com/apk/res/android" android:shape="oval" &gt; &lt;corners android:radius="5dip"/&gt; &lt;solid android:color="#ff0000"/&gt;&lt;/shape&gt; 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163package com.shenshanlaoyuan.viewpagertest;import java.util.ArrayList;import java.util.List;import android.app.Activity;import android.os.Bundle;import android.support.v4.view.PagerAdapter;import android.support.v4.view.ViewPager;import android.support.v4.view.ViewPager.OnPageChangeListener;import android.view.View;import android.view.ViewGroup;import android.widget.ImageView;import android.widget.TextView;import android.widget.ImageView.ScaleType;import android.widget.LinearLayout;import android.widget.LinearLayout.LayoutParams;public class MainActivity extends Activity implements OnPageChangeListener &#123; private ViewPager mPager; private LinearLayout mPointContainer; private List&lt;ImageView&gt; mListDatas; private TextView mTitle; //一般从网络获取数据，这里模拟本地获取数据,要在drawable目录添加五张图片 String[] titles = &#123; "第一个页面", "第二个页面", "第三个页面", "第四个页面", "第五个页面" &#125;; int[] imgs = &#123; R.drawable.a, R.drawable.b, R.drawable.c, R.drawable.d, R.drawable.e &#125;; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); mPager = (ViewPager) findViewById(R.id.pager); mPointContainer = (LinearLayout) findViewById(R.id.point_container); mTitle = (TextView) findViewById(R.id.tv_title); // 初始化数据 mListDatas = new ArrayList&lt;ImageView&gt;(); for (int i = 0; i &lt; imgs.length; i++) &#123; // 给集合添加ImageView ImageView iv = new ImageView(this); iv.setImageResource(imgs[i]); //图片拉伸 iv.setScaleType(ScaleType.FIT_XY); mListDatas.add(iv); // 添加圆点 View point = new View(this); point.setBackgroundResource(R.drawable.point_normal); LayoutParams params = new LayoutParams(10, 10); if (i != 0) &#123; params.leftMargin = 10; &#125; else &#123; point.setBackgroundResource(R.drawable.point_selected); mTitle.setText(titles[i]); &#125; //向容器LinearLayout中添加圆点 mPointContainer.addView(point, params); &#125; // 设置适配器 mPager.setAdapter(new MyAdapter()); // 设置监听器 mPager.addOnPageChangeListener(this); // 设置默认选中中间的item，实现循环轮播的效果 int middle = Integer.MAX_VALUE / 2; int extra = middle % mListDatas.size(); int item = middle - extra; mPager.setCurrentItem(item); &#125; class MyAdapter extends PagerAdapter &#123; // 页面的数量 @Override public int getCount() &#123; if (mListDatas != null) &#123; return Integer.MAX_VALUE; &#125; return 0; &#125; // 标记方法，用来判断缓存标记 // view:显示的view // object: 标记 @Override public boolean isViewFromObject(View view, Object object) &#123; return view == object; &#125; // 初始化item @Override public Object instantiateItem(ViewGroup container, int position) &#123; position = position % mListDatas.size(); // position： 要加载的位置 ImageView iv = mListDatas.get(position); // 用来添加要显示的View的 mPager.addView(iv); // 返回记录缓存标记 return iv; &#125; // 销毁item条目 // object:缓存标记 @Override public void destroyItem(ViewGroup container, int position, Object object) &#123; position = position % mListDatas.size(); ImageView iv = mListDatas.get(position); mPager.removeView(iv); &#125; &#125; /************************************* ViewPager监听回调方法 *******************************************/ // 回调方法,当viewpager的滑动状态改变时的回调 // * @see ViewPager#SCROLL_STATE_IDLE : 闲置状态 // * @see ViewPager#SCROLL_STATE_DRAGGING :拖动状态 // * @see ViewPager#SCROLL_STATE_SETTLING: 固定状态 @Override public void onPageScrollStateChanged(int state) &#123; &#125; // 回调方法,当viewpager滚动时的回调 // position: 当前选中的位置 // positionOffset: 滑动的百分比 // positionOffsetPixels: 偏移的距离,滑动的像素 @Override public void onPageScrolled(int position, float positionOffset, int positionOffsetPixels) &#123; &#125; // 回调方法,当viewpager的某个页面选中时的回调 @Override public void onPageSelected(int position) &#123; position = position % mListDatas.size(); // 设置选中的点的样式 int count = mPointContainer.getChildCount(); for (int i = 0; i &lt; count; i++) &#123; View view = mPointContainer.getChildAt(i); view.setBackgroundResource(position == i ? R.drawable.point_selected : R.drawable.point_normal); &#125; // 设置标题 mTitle.setText(titles[position]); &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[dip(独立像素)和px(像素)之间转换]]></title>
      <url>%2F2016%2F06%2F15%2FUtils%2Fdip(%E7%8B%AC%E7%AB%8B%E5%83%8F%E7%B4%A0)%E5%92%8Cpx(%E5%83%8F%E7%B4%A0)%E4%B9%8B%E9%97%B4%E8%BD%AC%E6%8D%A2%2F</url>
      <content type="text"><![CDATA[dip(dp): 与设备无关的像素，与“像素密度”密切相关，推荐使用 px: 普通像素 转载请注明出处：http://shenshanlaoyuan.com/2016/06/15/Utils/dip(独立像素)和px(像素)之间转换/访问原文「dip(独立像素)和px(像素)之间转换」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 1234567891011121314151617public class DensityUtil &#123; /** * 根据手机的分辨率从 dip(独立像素) 的单位 转成为 px(像素) */ public static int dip2px(Context context, float dpValue) &#123; final float scale = context.getResources().getDisplayMetrics().density; return (int) (dpValue * scale + 0.5f); &#125; /** * 根据手机的分辨率从 px(像素) 的单位 转成为 dp(dip) */ public static int px2dip(Context context, float pxValue) &#123; final float scale = context.getResources().getDisplayMetrics().density; return (int) (pxValue / scale + 0.5f); &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The type org.apache.http.HttpResponse cannot be resolved. It is indirectly referenced from required .class files]]></title>
      <url>%2F2016%2F06%2F13%2FAndroid%2FThe%20type%20org.apache.http.HttpResponse%20cannot%20be%20resolved.%20It%20is%20indirectly%20referenced%20from%20required%20.class%20files%2F</url>
      <content type="text"><![CDATA[在 Android 6.0（API 23）中，Google 已经移除了移除了Apache HttpClient 相关的类、HttpResponse 类。缺失jar包使用HttpResponse等会报错：The type org.apache.http.HttpResponse cannot be resolved. It is indirectly referenced from required 转载请注明出处：http://shenshanlaoyuan.com/2016/06/13/Android/The type org.apache.http.HttpResponse cannot be resolved. It is indirectly referenced from required .class files/访问原文「The type org.apache.http.HttpResponse cannot be resolved. It is indirectly referenced from required .class files」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 1234567891011121314151617//获取网络数据HttpUtils httpUtils = new HttpUtils();//发送URL请求 httpUtils.send(HttpMethod.GET, MyConstants.NEWSCENTERURL, new RequestCallBack&lt;String&gt;() &#123; @Override public void onSuccess(ResponseInfo&lt;String&gt; responseInfo) &#123; //访问数据成功 String jsonData = responseInfo.result; System.out.println(jsonData); &#125; @Override public void onFailure(HttpException error, String msg) &#123; //访问数据失败 System.out.println("网络请求数据失败：" + error); &#125;&#125;); 解决方法推荐使用HttpUrlConnection，如果要继续使用需要Apache HttpClient，需要在eclipse下libs里添加org.apache.http.legacy.jar。添加方法如下： Eclipse中 在错误原因上点击ctrl+1，选择Configure build path，或者Project-&gt;Properties-&gt;Java Build Path-&gt;Libraries-&gt;Add Ecternal JARS-&gt;你的SDK目录的 android studio里在相应的module下的build.gradle中加入： 123android &#123;useLibrary 'org.apache.http.legacy'&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MD5加密]]></title>
      <url>%2F2016%2F06%2F10%2FUtils%2FMD5%E5%8A%A0%E5%AF%86%2F</url>
      <content type="text"><![CDATA[为了不让别人看直接看到保存的密码等信息，通常密码先采用MD5加密后再保存。MD5加密不可逆的，网上破解MD5方法原理是，拿MD5加密后的数据去他们亿万级的数据库去匹配。要防止别人破解可以用加密后数据再MD5加密几次，像银行保存的密码信息至少加密十次以上。 转载请注明出处：http://shenshanlaoyuan.com/2016/06/10/Utils/MD5加密/访问原文「MD5加密」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 下图是某破解网站能破解的数据，也只能破解三次加密，而且还是收费，所以担心破解可以多加密几次。 MD5加密工具类：1234567891011121314151617181920212223242526public class Md5Utils &#123; public static String md5(String str)&#123; StringBuilder mess = new StringBuilder(); try &#123; //获取MD5加密器 MessageDigest md = MessageDigest.getInstance("MD5"); byte[] bytes = str.getBytes(); byte[] digest = md.digest(bytes); for (byte b : digest)&#123; //把每个字节转成16进制数 int d = b &amp; 0xff;// 0x000000ff String hexString = Integer.toHexString(d); if (hexString.length() == 1) &#123;//字节的高4位为0 hexString = "0" + hexString; &#125; mess.append(hexString);//把每个字节对应的2位十六进制数当成字符串拼接一起 &#125; &#125; catch (NoSuchAlgorithmException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return mess + ""; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[最爱听民谣]]></title>
      <url>%2F2016%2F06%2F03%2F%E9%9A%8F%E7%AC%94%2F%E6%9C%80%E7%88%B1%E5%90%AC%E6%B0%91%E8%B0%A3%2F</url>
      <content type="text"><![CDATA[转载请注明出处：http://shenshanlaoyuan.com/2016/06/03/随笔/最爱听民谣/访问原文「最爱听民谣」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[前面些文章写的有点水了]]></title>
      <url>%2F2016%2F06%2F02%2F%E9%9A%8F%E7%AC%94%2F%E5%89%8D%E9%9D%A2%E4%BA%9B%E6%96%87%E7%AB%A0%E5%86%99%E7%9A%84%E6%9C%89%E7%82%B9%E6%B0%B4%E4%BA%86%2F</url>
      <content type="text"><![CDATA[因为是第一次写博客，可能前面些文章技术含量很低，后面尽量多写点有质量的文章。 转载请注明出处：http://shenshanlaoyuan.com/2016/06/02/随笔/前面些文章写的有点水了/访问原文「前面些文章写的有点水了」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 以前语文作文很少及格，所以文笔太差了，见谅，呵呵。不过我相信多写写就会好些。 我为什么写博客？一直记得原来的语文老师说的那句，“好记性不如烂笔头”。确实是这样，以前学的东西现在好多都忘记了，多写写文章能加深印象。还有就是，比如现在正在学某些知识，写不出来也证明脑子里面没有记忆，也就是没学进去。所以以后一定要多把工作学习中的知识整理下写出来。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dialog]]></title>
      <url>%2F2016%2F05%2F23%2FAndroid%2FDialog%2F</url>
      <content type="text"><![CDATA[几种常见的对话框使用。 确认取消对话框123456789101112131415 AlertDialog.Builder builder = new AlertDialog.Builder(this);builder.setTitle("约会把...");builder.setMessage("告别单身, 你愿意吗 ?");builder.setPositiveButton("愿意,gogogo", new DialogInterface.OnClickListener() &#123; @Override public void onClick(DialogInterface dialog, int which) &#123; Toast.makeText(MainActivity.this, "我也单身, 说不定 可以 来找我...", 0).show(); &#125;&#125;);builder.setNegativeButton("不愿意", null );builder.show(); 显示效果： 转载请注明出处：http://shenshanlaoyuan.com/2016/05/23/Android/Dialog/访问原文「Dialog」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 单选对话框12345678910111213141516AlertDialog.Builder builder = new AlertDialog.Builder(this);builder.setTitle("单选对话框 ");final String[] items = &#123; "小丽", "小红", "小芳" &#125;;builder.setSingleChoiceItems(items, -1, new DialogInterface.OnClickListener() &#123; @Override public void onClick(DialogInterface dialog, int which) &#123; Toast.makeText(MainActivity.this, " 被点击了 : " + items[which] + ",位置: " + which, 0) .show(); &#125; &#125;);builder.show(); 效果： 多选对话框12345678910111213141516171819AlertDialog.Builder builder = new AlertDialog.Builder(this);builder.setTitle("多选");final String[] items = &#123; "android", "ios", "javaee", "php", "C++" &#125;;boolean[] checkedItems = &#123; true, true, false, false, false &#125;;builder.setMultiChoiceItems(items, checkedItems, new DialogInterface.OnMultiChoiceClickListener() &#123; @Override public void onClick(DialogInterface dialog, int which, boolean isChecked) &#123; Toast.makeText( MainActivity.this, " 被点击了 : " + items[which] + ",位置: " + which + ", 值是: " + isChecked, 0).show(); &#125; &#125;);builder.show(); 显示效果： 进度对话框123456789101112 ProgressDialog pd = new ProgressDialog(this);// 设置对话框的标题pd.setTitle("任务正在执行中");// 设置对话框显示的内容pd.setMessage("任务正在执行中，敬请等待...");// 设置对话框能用“取消”按钮关闭pd.setCancelable(true);// 设置对话框的进度条风格pd.setProgressStyle(ProgressDialog.STYLE_HORIZONTAL);// 设置对话框的进度条是否显示进度pd.setIndeterminate(true);pd.show(); 显示效果： 进度对话框带-进度条的1234567891011121314151617181920212223242526272829final ProgressDialog progress = new ProgressDialog(this);progress.setMessage("正在下载中。。。");progress.setProgressStyle(ProgressDialog.STYLE_HORIZONTAL);// progress.setIndeterminate(true);progress.show();final int totalProgressTime = 100;final Thread t = new Thread() &#123; @Override public void run() &#123; int jumpTime = 0; while (jumpTime &lt; totalProgressTime) &#123; try &#123; sleep(200); jumpTime += 5; progress.setProgress(jumpTime); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125;;t.start(); 显示效果：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Android下的数据存储与访问方式（三）]]></title>
      <url>%2F2016%2F05%2F21%2FAndroid%2FAndroid%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
      <content type="text"><![CDATA[更多方式阅读上两篇文章：Android下的数据存储与访问方式（一）Android下的数据存储与访问方式（二） SQLite 轻量级数据库SQLite，是一款轻型的数据库，是遵守ACID(原子性、一致性、隔离性、持久性)的关联式数据库管理系统，多用于嵌入式开发中。 SQLite的数据类型：Typelessness(无类型), 可以保存任何类型的数据到你所想要保存的任何表的任何列中. 但它又支持常见的类型比如: NULL, VARCHAR, TEXT, INTEGER, BLOB, CLOB…等. 唯一的例外：integer primary key 此字段只能存储64位整数。 转载请注明出处：http://shenshanlaoyuan.com/2016/05/21/Android/Android下的数据存储与访问方式（三）/访问原文「Android下的数据存储与访问方式（三）」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 在Android系统，提供了一个SQLiteOpenHelper抽象类，该类用于对数据库版本进行管理.该类中常用的方法: onCreate 数据库创建时执行(第一次连接获取数据库对象时执行) onUpgrade 数据库更新时执行(版本号改变时执行) onOpen 数据库每次打开时执行(每次打开数据库时调用，在 onCreate，onUpgrade方法之后) 使用SQLiteDatabase操作SQLite数据库Android提供了一个名为SQLiteDatabase的类，该类封装了一些操作数据库的API，使用该类可以完成对数据进行添加(Create)、查询(Retrieve)、更新(Update)和删除(Delete)操作（这些操作简称为CRUD）。 对SQLiteDatabase的学习，我们应该重点掌握execSQL()和rawQuery()方法。 execSQL()方法可以执行insert、delete、update和CREATE TABLE之类有更改行为的SQL语句； rawQuery()方法用于执行select语句。 1.execSQL()方法的使用例子： 1234MySqliteHelper helper = new MySqliteHelper();SQLiteDatabase db = helper.getWritableDatabase();db.execSQL("insert into person(name, age) values('张三', 4)");db.close(); 执行上面SQL语句会往person表中添加进一条记录，在实际应用中， 语句中的“张三”这些参数值会由用户输入界面提供，如果把用户输入的内容原样组拼到上面的insert语句， 当用户输入的内容含有单引号时，组拼出来的SQL语句就会存在语法错误。要解决这个问题需要对单引号进行转义，也就是把单引号转换成两个单引号。有些时候用户往往还会输入像“ &amp; ”这些特殊SQL符号，为保证组拼好的SQL语句语法正确，必须对SQL语句中的这些特殊SQL符号都进行转义，显然，对每条SQL语句都做这样的处理工作是比较烦琐的。 SQLiteDatabase类提供了一个重载后的execSQL(String sql, Object[] bindArgs)方法，使用这个方法可以解决前面提到的问题，因为这个方法支持使用占位符参数(?)。使用例子如下：1234MySqliteHelper helper = new MySqliteHelper();SQLiteDatabase db = helper.getWritableDatabase();db.execSQL("insert into person(name, age) values(?,?)", new Object[]&#123;"张三", 4&#125;); db.close(); execSQL(String sql, Object[] bindArgs)方法的第一个参数为SQL语句，第二个参数为SQL语句中占位符参数的值，参数值在数组中的顺序要和占位符的位置对应。 2.SQLiteDatabase的rawQuery() 用于执行select语句，使用例子如下： 12345678910MySqliteHelper helper = new MySqliteHelper();SQLiteDatabase db = helper.getReadableDatabase();Cursor cursor = db.rawQuery(“select * from person”, null);while (cursor.moveToNext()) &#123; int personid = cursor.getInt(0); //获取第一列的值,第一列的索引从0开始 String name = cursor.getString(1);//获取第二列的值 int age = cursor.getInt(2);//获取第三列的值&#125;cursor.close();db.close(); rawQuery()方法的第一个参数为select语句；第二个参数为select语句中占位符参数的值，如果select语句没有使用占位符，该参数可以设置为null。 带占位符参数的select语句使用例子如下：1Cursor cursor = db.rawQuery("select * from person where name like ? and age=?", new String[]&#123;"%张%", "4"&#125;); Cursor是结果集游标，用于对结果集进行随机访问，如果熟悉jdbc， 其实Cursor与JDBC中的ResultSet作用很相似。使用moveToNext()方法可以将游标从当前行移动到下一行，如果已经移过了结果集的最后一行，返回结果为false，否则为true。另外Cursor 还有常用的moveToPrevious()方法（用于将游标从当前行移动到上一行，如果已经移过了结果集的第一行，返回值为false，否则为true ）、moveToFirst()方法（用于将游标移动到结果集的第一行，如果结果集为空，返回值为false，否则为true ）和moveToLast()方法（用于将游标移动到结果集的最后一行，如果结果集为空，返回值为false，否则为true ） 。 除了execSQL()和rawQuery()方法, SQLiteDatabase还专门提供了对应于添加、删除、更新、查询的操作方法： insert() 增加数据 delete() 删除数据 update() 修改数据 query() 查询数据 这些方法实际上是给那些不太了解SQL语法的开发者使用的，对于熟悉SQL语法的程序员而言，直接使用execSQL()和rawQuery()方法执行SQL语句就能完成数据的添加、删除、更新、查询操作。 使用事务操作SQLite数据库使用SQLiteDatabase的beginTransaction()方法可以开启一个事务，程序执行到endTransaction() 方法时会检查事务的标志是否为成功，如果程序执行到endTransaction()之前调用了setTransactionSuccessful() 方法设置事务的标志为成功则提交事务，如果没有调用setTransactionSuccessful() 方法则回滚事务。使用例子如下：12345678910SQLiteDatabase db = ....;db.beginTransaction();//开始事务try &#123; db.execSQL("insert into person(name, age) values(?,?)", new Object[]&#123;"张三", 4&#125;); db.execSQL("update person set name=? where personid=?", new Object[]&#123;"李四", 1&#125;); db.setTransactionSuccessful();//调用此方法会在执行到endTransaction() 时提交当前事务，如果不调用此方法会回滚事务&#125; finally &#123; db.endTransaction();//由事务的标志决定是提交事务，还是回滚事务&#125; db.close(); 上面两条SQL语句在同一个事务中执行。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[博客添加Fork me on GitHub]]></title>
      <url>%2F2016%2F05%2F20%2FHexo%2F%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0Fork%20me%20on%20GitHub%2F</url>
      <content type="text"><![CDATA[效果如下所示： 转载请注明出处：http://shenshanlaoyuan.com/2016/05/20/Hexo/博客添加Fork me on GitHub/访问原文「博客添加Fork me on GitHub」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 去网址https://github.com/blog/273-github-ribbons挑选自己喜欢的样式，并复制代码。 修改主题文件。以我用的next主题为例,粘贴刚才复制的代码到themes/next/layout/_layout.swig文件中，修改后的的文件内容如下 再把其中的链接地址改为你的github地址。如上图修改红框中地址。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XML文件的生成与解析]]></title>
      <url>%2F2016%2F05%2F16%2FAndroid%2FXML%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%E4%B8%8E%E8%A7%A3%E6%9E%90%2F</url>
      <content type="text"><![CDATA[我们知道SharedPreference背后是用XMl文件进行数据存储的，那么Android下是如何生成XMl文件，如何解析XML文件的呢？ 转载请注明出处：http://shenshanlaoyuan.com/2016/05/16/Android/XML文件的生成与解析/访问原文「XML文件的生成与解析」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 生成XML文件原始拼接的方法假如要生成如下内容的XML文件123456&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;student&gt; &lt;name&gt;张三&lt;/name&gt; &lt;number&gt;123456&lt;/number&gt; &lt;sex&gt;男&lt;/sex&gt;&lt;/student&gt; 那么拼接方式的代码实现如下1234567891011121314151617 File file = new File(getFilesDir(),"张三.xml");StringBuilder sb = new StringBuilder();sb.append("&lt;?xml version='1.0' encoding='utf-8'?&gt;");sb.append("&lt;student&gt;");sb.append("&lt;name&gt;");sb.append("张三");sb.append("&lt;/name&gt;");sb.append("&lt;number&gt;");sb.append("123456");sb.append("&lt;/number&gt;");sb.append("&lt;sex&gt;");sb.append("男");sb.append("&lt;/sex&gt;");sb.append("&lt;/student&gt;");OutputStream out = new FileOutputStream(file);out.write(sb.toString().getBytes());out.close(); 使用序列化器Serializer如果XMl文件内接收的内容含有特殊字符，使用Serializer就不需要手动做判断转义了，自动帮我们完成了转义。 使用Serializer生成XMl代码如下:123456789101112131415161718192021222324252627 File file = new File(getFilesDir(),"张三.xml");OutputStream out = new FileOutputStream(file); // 专门生成xml 文件的 序列化器 XmlSerializer serializer = Xml.newSerializer();serializer.setOutput(out, "UTF-8");// &lt;?xml version="1.0" encoding="utf-8" standalone?&gt;serializer.startDocument("UTF-8", true);serializer.startTag(null, "student");serializer.startTag(null, "name");serializer.text("张三");serializer.endTag(null, "name");serializer.startTag(null, "number");serializer.text("123456");serializer.endTag(null, "number");serializer.startTag(null, "sex");serializer.text("男");serializer.endTag(null, "sex");serializer.endTag(null, "student");serializer.endDocument();out.close(); 解析生成的XML文件Android下使用的是Pull解析器,基于事件的。 假如下面是要解析的XML文件： 文件名称：persons.xml1234567891011&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;persons&gt; &lt;person id=“18"&gt; &lt;name&gt;allen&lt;/name&gt; &lt;age&gt;36&lt;/age&gt; &lt;/person&gt; &lt;person id=“28"&gt; &lt;name&gt;james&lt;/name&gt; &lt;age&gt;25&lt;/age&gt; &lt;/person&gt;&lt;/persons&gt;! 定义了一个java bean用于存放上面解析出来的xml内容， 这个java bean为Person，代码如下123456789101112131415161718192021222324public class Person &#123; private Integer id; private String name; private Short age; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Short getAge() &#123; return age; &#125; public void setAge(Short age) &#123; this.age = age; &#125; &#125; 使用Pull解析器解析，解析出来的内容通过List集合返回1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950private List&lt;Person&gt; parserXmlFromLocal() &#123; try &#123; File path = new File(Environment.getExternalStorageDirectory(), "persons.xml"); //从SD卡中读取xml文件 FileInputStream fis = new FileInputStream(path); // 获得pull解析器对象 XmlPullParser parser = Xml.newPullParser(); // 指定解析的文件和编码格式 parser.setInput(fis, "utf-8"); int eventType = parser.getEventType(); // 获得事件类型 List&lt;Person&gt; personList = null; Person person = null; String id; while(eventType != XmlPullParser.END_DOCUMENT) &#123; String tagName = parser.getName(); // 获得当前节点的名称 switch (eventType) &#123; case XmlPullParser.START_TAG: // 当前等于开始节点 &lt;person&gt; if("persons".equals(tagName)) &#123; // &lt;persons&gt; personList = new ArrayList&lt;Person&gt;(); &#125; else if("person".equals(tagName)) &#123; // &lt;person id="1"&gt; person = new Person(); id = parser.getAttributeValue(null, "id"); person.setId(Integer.valueOf(id)); &#125; else if("name".equals(tagName)) &#123; // &lt;name&gt; person.setName(parser.nextText()); &#125; else if("age".equals(tagName)) &#123; // &lt;age&gt; person.setAge(Integer.parseInt(parser.nextText())); &#125; break; case XmlPullParser.END_TAG: // &lt;/persons&gt; if("person".equals(tagName)) &#123; // 需要把上面设置好值的person对象添加到集合中 personList.add(person); &#125; break; default: break; &#125; eventType = parser.next(); // 获得下一个事件类型 &#125; return personList; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Android下的数据存储与访问方式（二）]]></title>
      <url>%2F2016%2F05%2F14%2FAndroid%2FAndroid%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
      <content type="text"><![CDATA[Android为数据存储提供了如下几种方式：1.文件 2.SharedPreferences 3.SQLite 4.网络 5.ContentProvider 文件方式请阅读上一篇文章Android下的数据存储与访问方式（一） SharedPreferences很多时候我们开发的软件需要向用户提供软件参数设置功能，例如我们常用的QQ，用户可以设置是否允许陌生人添加自己为好友，设置字体的大小等等。对于软件配置参数的保存，如果是window软件通常我们会采用ini文件进行保存，如果是j2se应用，我们会采用properties属性文件或者xml进行保存。 如果是Android应用，我们最适合采用什么方式保存软件配置参数呢？ Android平台给我们提供了一个SharedPreferences类，它是一个轻量级的存储类，特别适合用于保存软件配置参数。 转载请注明出处：http://shenshanlaoyuan.com/2016/05/14/Android/Android下的数据存储与访问方式（二）/访问原文「Android下的数据存储与访问方式（二）」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 使用SharedPreferences保存数据，其背后是用xml文件存放数据，文件存放在/data/data/&lt;package name&gt;/shared_prefs目录下 使用SharedPreferences保存数据12345SharedPreferences sp = getSharedPreferences("config", Context.MODE_PRIVATE);Editor editor = sp.edit();editor.putString("number", number);editor.putString("password", password);editor.commit(); 生成的config.xml文件中内容如下12345&lt;?xml version='1.0' encoding='utf-8' standalone='yes' ?&gt;&lt;map&gt;&lt;string name="number"&gt;1395545452415&lt;/string&gt;&lt;string name="password"&gt;4545872&lt;/string&gt;&lt;/map&gt; 因为SharedPreferences背后是使用xml文件保存数据，getSharedPreferences(name,mode)方法的第一个参数用于指定该文件的名称，名称不用带后缀，后缀会由Android自动加上。方法的第二个参数指定文件的操作模式，共有四种操作模式，这四种模式为 Context.MODE_PRIVATE：为默认操作模式，代表该文件是私有数据，只能被应用本身访问，在该模式下，写入的内容会覆盖原文件的内容，如果想把新写入的内容追加到原文件中。可以使用Context.MODE_APPEND Context.MODE_APPEND：模式会检查文件是否存在，存在就往文件追加内容，否则就创建新文件。Context.MODE_WORLD_READABLE和Context.MODE_WORLD_WRITEABLE用来控制其他应用是否有权限读写该文件。 MODE_WORLD_READABLE：表示当前文件可以被其他应用读取； MODE_WORLD_WRITEABLE：表示当前文件可以被其他应用写入。 如果希望SharedPreferences背后使用的xml文件能被其他应用读和写，可以指定Context.MODE_WORLD_READABLE和Context.MODE_WORLD_WRITEABLE权限。 另外Activity还提供了另一个getPreferences(mode)方法操作SharedPreferences，这个方法默认使用当前类不带包名的类名作为文件的名称。 读取SharedPreferences保存的数据12345 SharedPreferences sp = getSharedPreferences("config", Context.MODE_PRIVATE); // 如果找到了number的值,那么就返回number的值, 否则 就返回这里的默认值 String number= sp.getString("number", "");String password= sp.getString("password", "");]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Android下的数据存储与访问方式（一）]]></title>
      <url>%2F2016%2F05%2F13%2FAndroid%2FAndroid%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
      <content type="text"><![CDATA[很多时候我们的软件需要对处理后的数据进行存储或再次访问。Android为数据存储提供了如下几种方式： 文件 SharedPreferences SQLite 网络 ContentProvider 转载请注明出处：http://shenshanlaoyuan.com/2016/05/13/Android/Android下的数据存储与访问方式（一）/访问原文「Android下的数据存储与访问方式（一）」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 文件的方式将数据文件保存到应用程序中用模拟QQ登录保存帐号和密码做个简单的例子 简单的写了下QQ登录界面布局，如下:1234567891011121314151617181920212223242526272829303132333435363738394041&lt;LinearLayout xmlns:android="http://schemas.android.com/apk/res/android" xmlns:tools="http://schemas.android.com/tools" android:layout_width="match_parent" android:layout_height="match_parent" android:orientation="vertical" android:paddingBottom="@dimen/activity_vertical_margin" android:paddingLeft="@dimen/activity_horizontal_margin" android:paddingRight="@dimen/activity_horizontal_margin" android:paddingTop="@dimen/activity_vertical_margin" tools:context="com.shenshanlaoyuan.qqlogin.MainActivity" &gt; &lt;ImageView android:layout_gravity="center" android:layout_width="100dp" android:layout_height="100dp" android:src="@drawable/qq"/&gt; &lt;EditText android:id="@+id/qqnumber" android:hint="QQ号/手机号/邮箱" android:layout_width="fill_parent" android:layout_height="wrap_content" /&gt; &lt;EditText android:id="@+id/qqpassword" android:hint="密码" android:inputType="textPassword" android:layout_width="fill_parent" android:layout_height="wrap_content" /&gt; &lt;CheckBox android:id="@+id/remember" android:layout_width="fill_parent" android:layout_height="wrap_content" android:text="记住用户名和密码" /&gt; &lt;Button android:onClick="login" android:layout_width="fill_parent" android:layout_height="wrap_content" android:text="登录" android:background="#03a9f4"/&gt;&lt;/LinearLayout&gt; 点击登录按钮就会把帐号和密码保存到应用程序的file文件夹下，代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class MainActivity extends ActionBarActivity &#123; private EditText ed_qqnumber; private EditText ed_qqpassword; private CheckBox cbx; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); //去掉标题栏 requestWindowFeature(Window.FEATURE_NO_TITLE); setContentView(R.layout.activity_main); //初始化控件 ed_qqnumber = (EditText) findViewById(R.id.qqnumber); ed_qqpassword = (EditText) findViewById(R.id.qqpassword); cbx = (CheckBox) findViewById(R.id.remember); //回显数据 File file = new File(this.getFilesDir(),"info.txt"); //判断文件是否存在且有内容 if(file.exists() &amp;&amp; file.length()&gt;0)&#123; try &#123; //读取文件数据，然后回显 BufferedReader bufferedReader = new BufferedReader(new FileReader(file)); String readLine = bufferedReader.readLine(); String number = readLine.split("##")[0]; String password = readLine.split("##")[1]; ed_qqnumber.setText(number); ed_qqpassword.setText(password); &#125; catch (Exception e) &#123; // TODO: handle exception &#125; &#125; &#125; //点击登录后会执行login 方法 public void login (View v)&#123; //获取输入框中写入者值 String number = ed_qqnumber.getText().toString().trim(); String password = ed_qqpassword.getText().toString().trim(); // 判断是否填入了number以及 password的值 if(TextUtils.isEmpty(number)||TextUtils.isEmpty(password))&#123; //给用户提示, 输入账号和密码 Toast.makeText(this, "账号和密码不能为空", 0).show(); return; &#125; //判断是否勾选了checkbox ,如果勾选了checkbox ,那么就将 qq号和密码保存起来 boolean isChecked = cbx.isChecked(); if(isChecked)&#123; //在勾选了checkbox的时候, 将数据保存起来 try &#123; File file = new File(this.getFilesDir(), "info.txt"); OutputStream outputStream = new FileOutputStream(file); String value = number + "##" + password; outputStream.write(value.getBytes()); outputStream.close(); Toast.makeText(this, "保存成功", 0).show(); &#125; catch (Exception e) &#123; // TODO: handle exception Toast.makeText(this, "保存失败", 0).show(); &#125; &#125; &#125;&#125; 保存成功就会在file文件夹下产生保存了帐号密码的文件，如下图 有时候一些临时数据需要保存起来,只需要把new File(this.getFilesDir(),&quot;info.txt&quot;)改成new File(this.getCacheDir(),&quot;info.txt&quot;)，文件就会保存到cache文件夹下，也就是应用程序的缓存目录。 保存数据文件到SD卡中保存数据到sd 卡中需要申请权限 sdcard 做为一个外部的存储设备,有时候sdcard是处于弹出状态,未插入状态, 强制拔出状态. 那么在这些情况下,如果 要向sdcard 中写数据, 为了保证数据确切的存储, 提高用户的感受, 所以在存储到sd 卡中之前, 会去动态判断 sdcard 的状态,只有在sdcard 是处于挂载的状态下,再去 写 sd 卡数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051if(isChecked)&#123; //在勾选了checkbox的时候, 将数据保存起来 try &#123; //细节 一 : // 判断sdcar的 状态 String status = Environment.getExternalStorageState(); // 这里status动态的返回的 sd 卡的状态 // 如果是mouted --- 挂载 , 那么这个时候 可以去 写 数据到 sd 卡中 if(!Environment.MEDIA_MOUNTED.equals(status))&#123; // 表示sd 卡未挂载, 那么 这个时候 就提示用户 检查sd 卡的状态 Toast.makeText(this, "请检查 sd 卡的状态 ", 0).show(); return; &#125; // 细节二 : // 返回可用的 空闲的 空间 大小 ---- in bytes long freeSpace = Environment.getExternalStorageDirectory().getFreeSpace(); //拿到 sd 卡的总的大小, in bytes Environment.getExternalStorageDirectory().getTotalSpace(); //拿到 sd 卡 已经使用的 的大小, in bytes Environment.getExternalStorageDirectory().getUsableSpace(); // 调用这个api 去获得sd卡的可用 控件，这里还做了一个事, 将返回的字节 空间 做了单位的 // 转换 String avalableSize = Formatter.formatFileSize(this, freeSpace); Toast.makeText(this,"可用的 空间 是 : "+ avalableSize, 0).show(); File file = new File(Environment.getExternalStorageDirectory(),"info.txt"); OutputStream out = new FileOutputStream(file); String value = number+"##"+password; out.write(value.getBytes()); out.close(); Toast.makeText(this, "保存成功", 0).show(); &#125; catch (Exception e) &#123; e.printStackTrace(); Toast.makeText(this, "保存失败", 0).show(); &#125; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[为博文添加出处和版权声明]]></title>
      <url>%2F2016%2F04%2F25%2FHexo%2F%E4%B8%BA%E5%8D%9A%E6%96%87%E6%B7%BB%E5%8A%A0%E5%87%BA%E5%A4%84%E5%92%8C%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E%2F</url>
      <content type="text"><![CDATA[自己写博客也有一段时间了，主要是分享自己的学习心得和总结。最近发现有的网站用爬虫抓取我的博文，连作者和出处都没标明。所以自己谷歌找了一个解决办法，如下文。 转载请注明出处：http://shenshanlaoyuan.com/2016/04/25/Hexo/为博文添加出处和版权声明/访问原文「为博文添加出处和版权声明」获取最佳阅读体验并参与讨论 (function() { Element.prototype.remove = function() { this.parentElement.removeChild(this); } NodeList.prototype.remove = HTMLCollection.prototype.remove = function() { for(var i = this.length - 1; i >= 0; i--) { if(this[i] && this[i].parentElement) { this[i].parentElement.removeChild(this[i]); } } } var domain = document.domain; var white_list = ['shenshanlaoyuan.com', 'localhost']; if (white_list.indexOf(domain) >= 0) { var elements = document.getElementsByClassName('source'); elements.remove(); } })() 我这博客用的是hexo搭建的，所以用的是hexo插件的解决办法。 安装插件 在博客目录下打开git bash,输入1npm install hexo-filter-indicate-the-source --save 在 _config.yml文件配置 在站点配置文件_config.yml添加如下配置 123456789indicate_the_source: pattern: indicate-the-source enable: true render_engine: ejs element_class: source domain_white_list: - shenshanlaoyuan.com - localhost template: "&lt;blockquote&gt;&lt;p&gt;转载请注明出处：&lt;%- post.permalink %&gt;&lt;/p&gt;&lt;p&gt;访问原文「&lt;a href='&lt;%- post.permalink %&gt;'&gt;&lt;%- post.title %&gt;&lt;/a&gt;」获取最佳阅读体验并参与讨论&lt;/p&gt;&lt;/blockquote&gt;" 只要把domain_white_list域里面的域名改成你的就行了。 这些作用域的意思请参考《如果你转载文章不注明出处》 文章中写法 只要在文章截断&lt;!-- more --&gt; 的后面追加 &lt;!-- indicate-the-source --&gt;就可以了 显示效果如图红色框里面就是多显示的内容。因为刚才在domain_white_list域填写了你自己的域名，所以在你自己的博客中这些内容不显示的。 添加知识共享协议Creative Commons协议具体内容详见这篇文章《给博客配上CC协议》 只要在博文末尾添加下面段代码就行1&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc-sa/3.0/cn/&quot;&gt;&lt;img alt=&quot;知识共享许可协议&quot; style=&quot;border-width:0&quot; src=&quot;https://i.creativecommons.org/l/by-nc-sa/3.0/cn/80x15.png&quot; /&gt;&lt;/a&gt;&lt;br /&gt;本作品采用&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc-sa/3.0/cn/&quot;&gt;知识共享署名-非商业性使用-相同方式共享 3.0 中国大陆许可协议&lt;/a&gt;进行许可。 显示效果见本文末尾 本作品采用知识共享署名-非商业性使用-相同方式共享 3.0 中国大陆许可协议进行许可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[这两天有些伤感了]]></title>
      <url>%2F2016%2F04%2F17%2F%E9%9A%8F%E7%AC%94%2F%E6%83%85%E4%B8%8D%E7%9F%A5%E6%89%80%E8%B5%B7%EF%BC%8C%E4%B8%80%E5%BE%80%E8%80%8C%E6%B7%B1%2F</url>
      <content type="text"><![CDATA[情不知所起，一往而深！ 只愿得一人心，白首不分离。这清晰的话语，嘲笑孤单的自己! 去年今日此门中，人面桃花相映红。人面不知何处在，桃花依旧笑春风。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[android中点击事件四种响应方式]]></title>
      <url>%2F2016%2F04%2F16%2FAndroid%2Fandroid%E4%B8%AD%E7%82%B9%E5%87%BB%E4%BA%8B%E4%BB%B6%E5%9B%9B%E7%A7%8D%E5%93%8D%E5%BA%94%E6%96%B9%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[点击事件的四种写法： xml文件中指定view的onClick属性，利用反射的方式调用对应Activity中的click()方法 匿名内部类 内部类实现OnClickListener接口 当前类直接实现OnClickListener接口 第一种方式xml文件中指定view的onClick属性，利用反射的方式调用对应Activity中的click()方法xml文件中代码:123456&lt;Button android:id="@+id/btn_button" android:layout_width="wrap_content" android:layout_height="wrap_content" android:onClick="click" android:text="按钮"/&gt; Activity中代码：12345678910111213public class MainActivity extends AppCompatActivity &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super. onCreate(savedInstanceState); setContentView(R.layout.activity_main); &#125; public void click(View v) &#123; Toast.makeText(this, "第一种方式", Toast.LENGTH_SHORT).show(); &#125;&#125; 第二种方式匿名内部类Activity中代码：1234567891011121314151617public class MainActivity extends AppCompatActivity &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); Button button = (Button) findViewById(R.id.btn_button); button.setOnClickListener(new View.OnClickListener() &#123; @Override public void onClick(View v) &#123; Toast.makeText(MainActivity.this, "第二种方式", Toast.LENGTH_SHORT).show(); &#125; &#125;); &#125;&#125; 第三种方式内部类实现OnClickListener接口Activity中代码：12345678910111213141516171819public class MainActivity extends AppCompatActivity &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); Button button = (Button) findViewById(R.id.btn_button); button.setOnClickListener(new MyOnClickListener()); &#125; class MyOnClickListener implements View.OnClickListener&#123; @Override public void onClick(View v) &#123; Toast.makeText(MainActivity.this, "第三种方式", Toast.LENGTH_SHORT).show(); &#125; &#125;&#125; 第四种方式当前类实现OnClickListener接口Activity中代码：12345678910111213141516public class MainActivity extends AppCompatActivity implements View.OnClickListener &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); Button button = (Button) findViewById(R.id.btn_button); button.setOnClickListener(this); &#125; @Override public void onClick(View v) &#123; Toast.makeText(MainActivity.this, "第四种方式", Toast.LENGTH_SHORT).show(); &#125;&#125; 总结第四种用的比较多，第一种方式不推荐，违背了解耦设计思想。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[AndroidStudio快捷键汇总]]></title>
      <url>%2F2016%2F04%2F15%2F%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%2FAndroidStudio%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%B1%87%E6%80%BB%2F</url>
      <content type="text"><![CDATA[方便自己查找，有错误请指出 IDE 按键 说明 F1 帮助 Alt(Option)+F1 查找文件所在目录位置 Alt(Option)+1 快速打开或隐藏工程面板 Ctrl(Command)+Alt(Option)+ 打开设置对话框 Alt(Option)+Home 跳转到导航栏 Esc 光标返回编辑框 Shift+Esc 光标返回编辑框,关闭无用的窗口 Shift+Click 关闭标签页 F12 把焦点从编辑器移到最近使用的工具窗口 Ctrl(Command)+Alt(Option)+Y 同步 Ctrl(Command)+Alt(Option)+S 打开设置对话框 Alt(Option)+Shift+Inert 开启/关闭列选择模式 Ctrl(Command)+Alt(Option)+Shift+S 打开当前项目/模块属性 Alt(Option)+Shift+C 查看文件的变更历史 Ctrl(Command)+Shift+F10 运行 Ctrl(Command)+Shift+F9 debug运行 Ctrl(Command)+Alt(Option)+F12 资源管理器打开文件夹 编辑 按键 说明 Ctrl(Command)+C 复制当前行或选中的内容 Ctrl(Command)+D 粘贴当前行或选中的内容 Ctrl(Command)+X 剪切当前行或选中的内容 Ctrl(Command)+Y 删除行 Ctrl(Command)+Z 倒退 Ctrl(Command)+Shift+Z 向前 Alt(Option)+Enter 自动修正 Ctrl(Command)+Alt(Option)+L 格式化代码 Ctrl(Command)+Alt(Option)+I 将选中的代码进行自动缩进编排 Ctrl(Command)+Alt(Option)+O 优化导入的类和包 Alt(Option)+Insert 得到一些Intention Action，可以生成构造器、Getter、Setter、将 == 改为 equals() 等 Ctrl(Command)+Shift+V 选最近使用的剪贴板内容并插入 Ctrl(Command)+Alt(Option)+Shift+V 简单粘贴 Ctrl(Command)+Shift+Insert 选最近使用的剪贴板内容并插入（同Ctrl(Command)+Shift+V） Ctrl(Command)+Enter 在当前行的上面插入新行，并移动光标到新行（此功能光标在行首时有效） Shift+Enter 在当前行的下面插入新行，并移动光标到新行 Ctrl(Command)+J 自动代码 Ctrl(Command)+Alt(Option)+T 把选中的代码放在 try{} 、if{} 、 else{} 里 Shift+Alt(Option)+Insert 竖编辑模式 Ctrl(Command)+ / 注释 // Ctrl(Command)+Shift+ / 注释 /…/ Ctrl(Command)+Shift+J 合并成一行 F2/Shift+F2 跳转到下/上一个错误语句处 Ctrl(Command)+Shift+Back 跳转到上次编辑的地方 Ctrl(Command)+Alt(Option)+Space 类名自动完成 Shift+Alt(Option)+Up/Down 内容向上/下移动 Ctrl(Command)+Shift+Up/Down 语句向上/下移动 Ctrl(Command)+Shift+U 大小写切换 Tab 代码标签输入完成后，按 Tab，生成代码 Ctrl(Command)+Backspace 按单词删除 Ctrl(Command)+Shift+Enter 语句完成 Ctrl(Command)+Alt(Option)+J 用动态模板环绕 文件 按键 说明 Ctrl(Command)+F12 显示当前文件的结构 Ctrl(Command)+H 显示类继承结构图 Ctrl(Command)+Q 显示注释文档 Ctrl(Command)+P 方法参数提示 Ctrl(Command)+U 打开当前类的父类或者实现的接口 Alt(Option)+Left/Right 切换代码视图 Ctrl(Command)+Alt(Option)+Left/Right 返回上次编辑的位置 Alt(Option)+Up/Down 在方法间快速移动定位 Ctrl(Command)+B 快速打开光标处的类或方法 Ctrl(Command)+W 选中代码，连续按会有其他效果 Ctrl(Command)+Shift+W 取消选择光标所在词 Ctrl(Command)+ - / + 折叠/展开代码 Ctrl(Command)+Shift+ - / + 折叠/展开全部代码 Ctrl(Command)+Shift+. 折叠/展开当前花括号中的代码 Ctrl(Command)+ ] / [ 跳转到代码块结束/开始处 F2 或 Shift+F2 高亮错误或警告快速定位 Ctrl(Command)+Shift+C 复制路径 Ctrl(Command)+Alt(Option)+Shift+C 复制引用，必须选择类名 Alt(Option)+Up/Down 在方法间快速移动定位 Shift+F1 要打开编辑器光标字符处使用的类或者方法 Java 文档的浏览器 Ctrl(Command)+G 定位行 查找 按键 说明 Ctrl(Command)+F 在当前窗口查找文本 Ctrl(Command)+Shift+F 在指定环境下查找文本 F3 向下查找关键字出现位置 Shift+F3 向上一个关键字出现位置 Ctrl(Command)+R 在当前窗口替换文本 Ctrl(Command)+Shift+R 在指定窗口替换文本 Ctrl(Command)+N 查找类 Ctrl(Command)+Shift+N 查找文件 Ctrl(Command)+Shift+Alt(Option)+N 查找项目中的方法或变量 Ctrl(Command)+B 查找变量的来源 Ctrl(Command)+Alt(Option)+B 跳转到类或方法实现处 Ctrl(Command)+Shift+B 快速打开光标处的类或方法 Ctrl(Command)+E 最近打开的文件 Alt(Option)+F3 快速查找，效果和Ctrl(Command)+F相同 F4 跳转至定义变量的位置 Alt(Option)+F7 查询当前元素在工程中的引用 Ctrl(Command)+F7 查询当前元素在当前文件中的引用，然后按 F3 可以选择 Ctrl(Command)+Alt(Option)+F7 选中查询当前元素在工程中的引用 Ctrl(Command)+Shift+F7 高亮显示匹配的字符，按 Esc 高亮消失 Ctrl(Command)+Alt(Option)+F7 查找某个方法的所有调用地方 Ctrl(Command)+Shift+Alt(Option)+N 查找类中的方法或变量 Ctrl(Command)+Shift+O 弹出显示查找内容 Ctrl(Command)+Alt(Option)+Up/Down 快速跳转搜索结果 Ctrl(Command)+Shift+S 高级搜索、搜索结构 重构 按键 说明 F5 复制 F6 移动 Alt(Option)+Delete 安全删除 Ctrl(Command)+U 转到父类 Ctrl(Command)+O 重写父类的方法 Ctrl(Command)+I 实现方法 Ctrl(Command)+Alt(Option)+N 内联 Ctrl(Command)+Alt(Option)+Shift+T 弹出重构菜单 Shift+F6 重构-重命名 Ctrl(Command)+Alt(Option)+M 提取代码组成方法 Ctrl(Command)+Alt(Option)+C 将变量更改为常量 Ctrl(Command)+Alt(Option)+V 定义变量引用当前对象或者方法的返回值 Ctrl(Command)+Alt(Option)+F 将局部变量更改为类的成员变量 Ctrl(Command)+Alt(Option)+P 将变量更改为方法的参数 调试 按键 说明 F8 跳到下一步 Shift+F8 跳出函数、跳到下一个断点 Alt(Option)+Shift+F8 强制跳出函数 F7 进入代码 Shift+F7 智能进入代码 Alt(Option)+Shift+F7 强制进入代码 Alt(Option)+F9 运行至光标处 Ctrl(Command)+Alt(Option)+F9 强制运行至光标处 Ctrl(Command)+F2 停止运行 Alt(Option)+F8 计算变量值 VCS 按键 说明 Alt(Option)+ ~ VCS 操作菜单 Ctrl(Command)+K 提交更改 Ctrl(Command)+T 更新项目 Ctrl(Command)+Alt(Option)+Shift+D 显示变化]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[AndroidStudio中如何使用Git和Github管理项目]]></title>
      <url>%2F2016%2F04%2F13%2F%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%2FAndroidStudio%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Git%E5%92%8CGithub%E7%AE%A1%E7%90%86%E9%A1%B9%E7%9B%AE%2F</url>
      <content type="text"><![CDATA[本文适用于window系统，Mac系统请参考。 要想在AndroidStudio中使用Git，本地必须安装Git,访问https://git-for-windows.github.io下载安装。Git详细安装使用请参考git教程 AndroidStudio中自带Git插件，只需配置下就可以。打开File-&gt;Setting-&gt;Version Control-&gt;Git，设置Git路径（需要定位到bin目录下的git.exe），再点击Test,弹出如下提示框，证明配置成功。 配置Github。打开File-&gt;Setting-&gt;Version Control-&gt;Github,设置Github帐号密码，没有就去官网注册。输入完，点Test,弹出如下提示框，证明配置成功. 初始化git。打开VCS-&gt;Enable Control Integration，再选择Git。菜单栏就会有如下图几个按钮 上传项目到Github前可以选择忽略上传部分文件，比如保存了数据库密码的配置文件，请参考《忽略特殊文件》。在项目根目录下的.gitignore文件配置，AndroidStudio默认帮我们配置好了，所以这步有需要可以配置。 上传项目到Github。打开VCS&gt;Import into Version Control&gt;Share Project on GitHub然后再添加Github仓库名称和描述,上传成功，登录Github就可以看到自己上传的项目了。 在Github看到好的项目需要克隆到AndroidStudio,可以打开VCS-&gt;Checkout from Version Control-&gt;GitHub，输入如上图HTTPS中的URL]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[android知识结构图]]></title>
      <url>%2F2016%2F04%2F03%2FAndroid%2Fandroid%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%BA%BF%E8%B7%AF%E5%9B%BE%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[跟随大神的脚步才能成长为大神-android大神Blog推荐（转）]]></title>
      <url>%2F2016%2F03%2F28%2FAndroid%2F%E5%A4%A7%E7%A5%9EBlog%E6%8E%A8%E8%8D%90%2F</url>
      <content type="text"><![CDATA[发现一些非常给力的CSDNBlog和个人Blog，这些Blog都有一个共同的特点，即内容详实，讲解透彻，也算是给后来的初学者指一条路吧！只要你下定决心跟随强者的脚步，成为人们眼中的大神，只不过是时间问题！下面排名不分先后 CSDN 鸿洋：http://blog.csdn.net/lmj623565791 爱哥：http://blog.csdn.net/aigestudio 老罗：http://blog.csdn.net/luoshengyang 小巫：http://blog.csdn.net/wwj_748 Mr.Simple：http://blog.csdn.net/bboyfeiyu 任玉刚：http://blog.csdn.net/singwhatiwanna 夏安明：http://blog.csdn.net/xiaanming 徐医生：http://blog.csdn.net/eclipsexys 郭神：http://blog.csdn.net/sinyu890807/ 张兴业：http://blog.csdn.net/xyz_lmn/ 咪当系欧巴(非常有天赋的Coder)：http://blog.csdn.net/hellogv 个人Blog 胡凯(android官方开发教程中文译文项目发起者)：http://hukai.me/blog/archives/ 码农明明桑（有审美细胞的开发者）：http://blog.isming.me/ stormzhang（收到FaceBook面试邀请的大神）：http://stormzhang.com/posts.html Coder Robin：http://coderrobin.com/ 郝锡强：http://www.haoxiqiang.info/static/timing.html Trinea：http://www.trinea.cn/ 农民伯伯：http://www.cnblogs.com/over140/ daimajia(北京师范大学在读研究生，SwipeLayout是ZListView产生的源头)：http://blog.daimajia.com/ stay4it：http://www.cnblogs.com/stay/ 张明云：http://zmywly8866.github.io/pages/archive.html 你要时常去看的网站 Android开发者官网(你必须熟练使用翻墙)：http://developer.android.com/index.html GitHub(开源项目就在这里找)：https://github.com/ 开源项目解析：http://www.codekk.com/open-source-project-analysis Android代码查找：https://www.codota.com/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[最火的Android开源项目（转载）]]></title>
      <url>%2F2016%2F03%2F28%2FAndroid%2F%E6%9C%80%E7%81%AB%E7%9A%84Android%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89%2F</url>
      <content type="text"><![CDATA[了解常见的开源项目，可以扩大我们的视野，知道有哪些可以利用的资源，对于我们平常的设计和开发很有好处。当然，如果有更多时间的话，也可以专门学习某开源项目，提高自己的开发水平。以下是几个网址，是从网上看到的，简单介绍了一些Android开源项目，个人觉得了解一下，对于平时开发还是很有帮助的。有时间可以仔细看看。直接拿来用！最火的Android开源项目（一）http://www.csdn.net/article/2013-05-03/2815127-Android-open-source-projects直接拿来用！最火的Android开源项目（二）http://www.csdn.net/article/2013-05-08/2815145-Android-open-source-projects-two直接拿来用！最火的Android开源项目（完结篇）http://www.csdn.net/article/2013-05-21/2815370-Android-open-source-projects-finale]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hexo搭建独立博客，托管到Github和Coding上]]></title>
      <url>%2F2016%2F03%2F28%2FHexo%2FHexo%E6%90%AD%E5%BB%BA%E7%8B%AC%E7%AB%8B%E5%8D%9A%E5%AE%A2%EF%BC%8C%E6%89%98%E7%AE%A1%E5%88%B0Github%E5%92%8CCoding%E6%95%99%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[前言这是一篇很详细的独立博客搭建教程，意在帮助小白们能快速入门，拥有自己的独立博客。作者已在window平台已搭建成功，博客效果请点链接查看。 为什么用Hexo搭建独立博客？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 Github和Coding又是什么？ Github是国外免费的Git代码托管平台。利用Github Page服务可以免费创建一个静态网站。 Coding则是国内Git代码托管平台。国内首个Git代码托管平台GitCafe已被Coding收购。也提供page服务。 为什么用两个代码托管平台？很多人都把hexo托管到github上，因为github大家都用的比较久了。但是，你的博客主要访问者肯定还是国内的用户，国内的用户访问coding比github是要快不少的。还可以利用域名解析实现国内的走coding，海外的走github，分流网站的访问。 步骤安装Git Git是什么？Git是目前世界上最先进的分布式版本控制系统（没有之一）。了解更多，参考git教程 点击下载，然后按默认选项安装即可。 安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，蹦出一个类似命令行窗口的东西，就说明Git安装成功！ 安装完成后，还需要最后一步设置，在命令行输入12$ git config --global user.name &quot;Your Name&quot;$ git config --global user.email &quot;email@example.com&quot; 因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。 注意git config命令的--global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。 安装Node.js 直接进入官网 打开Downloads 再点击Windows Installer下载,默认安装就行安装Hexo打开Git-bash，输入1npm install -g hexo-cli 即可完成Hexo的安装。 本地部署Hexo 在电脑磁盘新建一个存放博客目录的文件夹，例如：blog 进入到blog文件夹,点空白处右击鼠标，打开Git-Bash 输入hexo init 然后 npm install,该文件夹有如下目录，就安装成功！ 12345678.├── _config.yml // 网站的配置信息，你可以在此配置大部分的参数。├── package.json ├── scaffolds // 模板文件夹。当你新建文章时，Hexo会根据scaffold来建立文件。├── source // 存放用户资源的地方| ├── _drafts| └── _posts└── themes // 存放网站的主题。Hexo会根据主题来生成静态页面。 输入hexo s 启动博客 123$ hexo sINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 打开浏览器输入 http://localhost:4000/ 即可访问 将博客托管到Github和Coding上托管到github 注册github帐号访问官网注册,你的username和邮箱十分重要，GitHub上很多通知都是通过邮箱的。比如你的主页上传并构建成功会通过邮箱通知，更重要的是，如果构建失败的话也会在邮件中说明原因。 创建项目仓库注册并登陆Github官网成功后，点击页面右上角的+，选择New repository。在Repository name中填写Github账号名.github.io点击Create repository，完成创建。托管到coding 注册coding帐号访问官网注册并登录 创建仓库点+创建项目填写项目名称描述创建即可,配置SHH配置shh key是让本地git项目与远程的github建立联系 检查是否已经有SSH Key，打开Git Bash，输入 1cd ~/.ssh 如果没有.ssh这个目录，则生成一个新的SSH，输入 1ssh-keygen -t rsa -C &quot;your e-mail&quot; 注意1: 此处的邮箱地址，你可以输入自己的邮箱地址；注意2: 此处的「-C」的是大写的「C」 接下来几步都直接按回车键,然后系统会要你输入密码12Enter passphrase (empty for no passphrase):&lt;输入加密串&gt;Enter same passphrase again:&lt;再次输入加密串&gt; 这个密码会在你提交项目时使用，如果为空的话提交项目时则不用输入。这个设置是防止别人往你的项目里提交内容。注意：输入密码的时候没有*字样的，你直接输入就可以了。 最后看到这样的界面，就成功设置ssh key了添加 SSH Key 到 GitHub和Coding 打开Git Bash，然后输入 1cd ~/.ssh 进入到.shh文件夹中再输入ls，查看是否有id_rsa.pub文件 输入cat命令，打开id_rsa.pub文件1cat id_rsa.pub 再鼠标全选中右击复制 再配置到GitHub和Coding的SSH中进入Github官网，点击+旁边的头像，再按settings进入设置在点击New SSH key创建title输入邮箱，key里面粘贴刚才右击复制的内容,再点Add SSH key同样进入coding,点击账户，在点SSH公钥设置即可测试SSH是否配置成功 打开Git Bash，然后输入 1ssh -T git@github.com 如配置了密码则要输入密码,输完按回车如果显示以下内容，则说明Github中的ssh配置成功。 12Hi username! You&apos;ve successfully authenticated, but GitHub does notprovide shell access. 再输入 1ssh -T git@git.coding.net 如果显示以下则说明coding中的ssh配置成功1Hello username You&apos;ve connected to Coding.net by SSH successfully! 创建Github Pages和Coding Pages 服务 GitHub Pages分两种，一种是你的GitHub用户名建立的username.github.io这样的用户&amp;组织页（站），另一种是依附项目的pages。想建立个人博客是用的第一种，形如cnfeat.github.io这样的可访问的站，每个用户名下面只能建立一个。更多 官网点击代码再点击Coding Pages 服务开启。分支和github分支写一样，填写master将博客网站上传到GitHub和Coding中 打开D:\blog文件夹中的_config.yml文件，找到如下位置，填写1234567# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: github: git@github.com:yourname/yourname.github.io.git,master coding: git@git.coding.net:yourname/yourname.git,master 注： (1) 其中yourname替换成你的Github账户名;(2)注意在yml文件中，:后面都是要带空格的。 在blog文件夹中空白处右击打开Git Bash输入123npm install hexo-deployer-git --savehexo g #生成静态网页hexo d #开始部署 此时，通过访问http://yourname.github.io和[http://yourname.coding.me]()可以看到默认的Hexo首页面（与之前本地测试时一样）。 更换Hexo主题本网站使用的是Next主题。该主题简洁易用，在移动端也表现不错。 下载主题在blog文件夹中空白处右击打开Git Bash输入 1git clone https://github.com/iissnan/hexo-theme-next themes/next 修改网站的主题为Next打开D:\blog下的_config.yml文件，找到theme字段，将其修改为next 1234# Extensions## Plugins: http://hexo.io/plugins/## Themes: http://hexo.io/themes/theme: next 验证主题是否可用输入 1hexo s #启动服务，调试用 再在浏览器输入http://localhost:4000/确认网站主题是否切换为Next. 博客blog根目录下的_config.yml配置网站信息_config.yml配置请点参考注册及绑定自己的域名地址域名注册推荐选择国内的万网或者国外的Goddady进行域名的注册，注册完还需改下绑定DNS服务商域名解析如果你选择的是万网注册的域名，可以使用其自带的域名解析服务。也可以选择免费的DNSPod 域名解析填写如下图 打开blog文件夹下的source文件夹，新建CNAME文件,内容填写自己的域名CNAME文件设置的目的是，通过访问 yourname.github.io 可以跳转到你所注册的域名上。github是直接项目里面加CNAME文件。coding是直接在项目主页设置的，去coding项目主页添加CNAME，绑定域名。总结只要按照上面步骤一步步设置，相信你也可以拥有自己的独立博客。希望此文对还在搭建hexo独立博客的小伙伴有所帮助。主题相关配置查看下面的，hexo和next帮助文档。参考 《Hexo+Github: 搭建属于自己的静态博客》 《hexo你的博客》 《如何使用10个小时搭建出个人域名而又Geek的独立博客？》 《将hexo博客同时托管到github和coding》 《个人域名如何同时绑定 github 和 coding 上的博客》 《如何搭建一个独立博客——简明Github Pages与Hexo教程》 《「搭建Hexo博客」简明教程》 《使用 github Pages 服务建立 ixirong.com 独立博客全过程》帮助文档 《Hexo文档》 《Next使用文档》 《Git教程》 《Github帮助文档》 《Coding帮助文档》 《Markdown 语法说明》更多教程可以来我深山老猿独立博客里面看到转载请注明出处http://shenshanlaoyuan.com/]]></content>
    </entry>

    
  
  
</search>
